@book{mathai1992,
    author = {Arak M. Mathai and Serge B. Provost},
    year = {1992},
    title = {Quadratic forms in random variables: Theory and applications},
    publisher={M. Dekker},
    address={New York},
    volume={126},
}

@inproceedings{Chwialkowski2015,
    author = {Chwialkowski, Kacper and Ramdas, Aaditya and Sejdinovic, Dino and Gretton, Arthur},
    title = {Fast Two-Sample Testing with Analytic Representations of Probability Measures},
    year = {2015},
    publisher = {MIT Press},
    address = {Cambridge, MA, USA},
    booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems},
    pages = {1981--1989},
}

@InProceedings{rainforth2018,
  title = 	 {On Nesting {M}onte {C}arlo Estimators},
  author = 	 {Rainforth, Tom and Cornish, Rob and Yang, Hongseok and Warrington, Andrew and Wood, Frank},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {4267--4276},
  year = 	 {2018},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  publisher = {PMLR},
}

@article{sriperumbudur2010,
  title={Hilbert space embeddings and metrics on probability measures},
  author={Sriperumbudur, Bharath K and Gretton, Arthur and Fukumizu, Kenji and Sch{\"o}lkopf, Bernhard and Lanckriet, Gert RG},
  journal={Journal of Machine Learning Research},
  volume={11},
  number={Apr},
  pages={1517--1561},
  year={2010}
}

@incollection{Widmann2019,
    title = {Calibration tests in multi-class classification: A unifying framework},
    author = {Widmann, David and Lindsten, Fredrik and Zachariah, Dave},
    booktitle = {Proceedings of the 32th International Conference on Neural Information Processing Systems},
    pages = {12236--12246},
    year = {2019},
}

@article{Szabo2018,
  author  = {Zolt{\'a}n Szab{\'o} and Bharath K. Sriperumbudur},
  title   = {Characteristic and Universal Tensor Product Kernels},
  journal = {Journal of Machine Learning Research},
  year    = {2018},
  volume  = {18},
  number  = {233},
  pages   = {1--29},
}

@article{Gelbrich1990,
  doi = {10.1002/mana.19901470121},
  year = {1990},
  publisher = {Wiley},
  volume = {147},
  number = {1},
  pages = {185--203},
  author = {Matthias Gelbrich},
  title = {On a Formula for the $L^2$ {Wasserstein} Metric between Measures on {Euclidean} and {Hilbert} Spaces},
  journal = {Mathematische Nachrichten}
}

@article{Delon2019,
    title={A {Wasserstein}-type distance in the space of {Gaussian} Mixture Models},
    author={Julie Delon and Agnes Desolneux},
    year={2019},
    eprint={1907.05254},
    archivePrefix={arXiv},
    primaryClass={math.OC}
}

@article{Chen2019,
  doi = {10.1109/access.2018.2889838},
  year = {2019},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {7},
  pages = {6269--6278},
  author = {Yongxin Chen and Tryphon T. Georgiou and Allen Tannenbaum},
  title = {Optimal Transport for {Gaussian} Mixture Models},
  journal = {{IEEE} Access}
}

@article{Mueller1997,
  doi = {10.2307/1428011},
  year = {1997},
  month = jun,
  publisher = {Cambridge University Press ({CUP})},
  volume = {29},
  number = {2},
  pages = {429--443},
  author = {Alfred M\"{u}ller},
  title = {Integral Probability Metrics and Their Generating Classes of Functions},
  journal = {Advances in Applied Probability}
}

@article{Sriperumbudur2012,
  doi = {10.1214/12-ejs722},
  year = {2012},
  publisher = {Institute of Mathematical Statistics},
  volume = {6},
  number = {0},
  pages = {1550--1599},
  author = {Bharath K. Sriperumbudur and Kenji Fukumizu and Arthur Gretton and Bernhard Sch\"{o}lkopf and Gert R. G. Lanckriet},
  title = {On the empirical estimation of integral probability metrics},
  journal = {Electronic Journal of Statistics}
}

@misc{Sriperumbudur2009,
    title={On integral probability metrics, $\phi$-divergences and binary classification},
    author={Bharath K. Sriperumbudur and Kenji Fukumizu and Arthur Gretton and Bernhard Schölkopf and Gert R. G. Lanckriet},
    year={2009},
    eprint={0901.2698},
    archivePrefix={arXiv},
    primaryClass={cs.IT}
}

@InProceedings{Guo2017,
  title = {On Calibration of Modern Neural Networks},
  author = {Chuan Guo and Geoff Pleiss and Yu Sun and Kilian Q. Weinberger},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  pages = {1321--1330},
  year = {2017},
  volume = {70},
  series = {Proceedings of Machine Learning Research},
  month = {8},
  publisher = {PMLR},
  pdf = {http://proceedings.mlr.press/v70/guo17a/guo17a.pdf},
  abstract = {Confidence calibration – the problem of predicting probability estimates representative of the true correctness likelihood – is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling – a single-parameter variant of Platt Scaling – is surprisingly effective at calibrating predictions.}
}

@article{Breiman1996,
  doi = {10.1007/bf00058655},
  year = {1996},
  month = {08},
  publisher = {Springer Science and Business Media {LLC}},
  volume = {24},
  number = {2},
  pages = {123--140},
  author = {Leo Breiman},
  title = {Bagging predictors},
  journal = {Machine Learning}
}

@article{Friedman1991,
  issn = {00905364},
  abstract = {A new method is presented for flexible regression modeling of high dimensional data. The model takes the form of an expansion in product spline basis functions, where the number of basis functions as well as the parameters associated with each one (product degree and knot locations) are automatically determined by the data. This procedure is motivated by the recursive partitioning approach to regression and shares its attractive properties. Unlike recursive partitioning, however, this method produces continuous models with continuous derivatives. It has more power and flexibility to model relationships that are nearly additive or involve interactions in at most a few variables. In addition, the model can be represented in a form that separately identifies the additive contributions and those associated with the different multivariable interactions.},
  author = {Jerome H. Friedman},
  journal = {The Annals of Statistics},
  number = {1},
  pages = {1--67},
  publisher = {Institute of Mathematical Statistics},
  title = {Multivariate Adaptive Regression Splines},
  volume = {19},
  year = {1991}
}

@article{Friedman1983,
  doi = {10.1137/0904023},
  year = {1983},
  month = jun,
  publisher = {Society for Industrial {\&} Applied Mathematics ({SIAM})},
  volume = {4},
  number = {2},
  pages = {291--301},
  author = {Jerome H. Friedman and Eric Grosse and Werner Stuetzle},
  title = {Multidimensional Additive Spline Approximation},
  journal = {{SIAM} Journal on Scientific and Statistical Computing}
}

@incollection{Friedman1979,
  doi = {10.1007/bfb0098488},
  year = {1979},
  publisher = {Springer Berlin Heidelberg},
  pages = {5--22},
  author = {Jerome H. Friedman},
  title = {A tree-structured approach to nonparametric multiple regression},
  booktitle = {Lecture Notes in Mathematics}
}

@inproceedings{Kingma2015,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  timestamp = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{Song2019,
  title = {Distribution calibration for regression},
  author = {Song, Hao and Diethe, Tom and Kull, Meelis and Flach, Peter},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  pages = {5897--5906},
  year = {2019},
  volume = {97},
  series = {Proceedings of Machine Learning Research},
  month = {6},
  publisher = {PMLR},
  pdf = {http://proceedings.mlr.press/v97/song19a/song19a.pdf},
  abstract = {We are concerned with obtaining well-calibrated output distributions from regression models. Such distributions allow us to quantify the uncertainty that the model has regarding the predicted target value. We introduce the novel concept of distribution calibration, and demonstrate its advantages over the existing definition of quantile calibration. We further propose a post-hoc approach to improving the predictions from previously trained regression models, using multi-output Gaussian Processes with a novel Beta link function. The proposed method is experimentally verified on a set of common regression models and shows improvements for both distribution-level and quantile-level calibration.}
}

@inbook{Platt2000,
  author = {J. Platt},
  editor = {A. J. {Smola} and P. {Bartlett} and B. {Schölkopf} and D. {Schuurmans}},
  booktitle = {Advances in Large-Margin Classifiers},
  title = {Probabilities for SV Machines},
  year = {2000},
  pages = {61--73},
  publisher = {MIT Press},
}

@incollection{Zadrozny2002,
  title = {Reducing multiclass to binary by coupling probability estimates},
  author = {Bianca Zadrozny},
  booktitle = {Advances in Neural Information Processing Systems 14},
  pages = {1041--1048},
  year = {2002},
  publisher = {MIT Press},
}

@InProceedings{Kull2017,
  title = {Beta calibration: a well-founded and easily implemented improvement on logistic calibration for binary classifiers},
  author = {Meelis Kull and Telmo Silva Filho and Peter Flach},
  booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
  pages = {623--631},
  year = {2017},
  volume = {54},
  series = {Proceedings of Machine Learning Research},
  month = {4},
  publisher = {PMLR},
  pdf = {http://proceedings.mlr.press/v54/kull17a/kull17a.pdf},
  abstract = {For optimal decision making under variable class distributions and misclassification costs a classifier needs to produce well-calibrated estimates of the posterior probability. Isotonic calibration is a powerful non-parametric method that is however prone to overfitting on smaller datasets; hence a parametric method based on the logistic curve is commonly used. While logistic calibration is designed for normally distributed per-class scores, we demonstrate experimentally that many classifiers including Naive Bayes and Adaboost suffer from a particular distortion where these score distributions are heavily skewed. In such cases logistic calibration can easily yield probability estimates that are worse than the original scores. Moreover, the logistic curve family does not include the identity function, and hence logistic calibration can easily uncalibrate a perfectly calibrated classifier.  In this paper we solve all these problems with a richer class of calibration maps based on the beta distribution. We derive the method from first principles and show that fitting it is as easy as fitting a logistic curve. Extensive experiments show that beta calibration is superior to logistic calibration for Naive Bayes and Adaboost.  }
}

@incollection{Kull2019,
  title = {Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with Dirichlet calibration},
  author = {Kull, Meelis and Perello Nieto, Miquel and K\"{a}ngsepp, Markus and Silva Filho, Telmo and Song, Hao and Flach, Peter},
  booktitle = {Advances in Neural Information Processing Systems 32},
  pages = {12316--12326},
  year = {2019},
}

@article{Gretton2012,
  author  = {Arthur Gretton and Karsten M. Borgwardt and Malte J. Rasch and Bernhard Sch{{\"o}}lkopf and Alexander Smola},
  title   = {A Kernel Two-Sample Test},
  journal = {Journal of Machine Learning Research},
  year    = {2012},
  volume  = {13},
  number  = {25},
  pages   = {723--773},
  url     = {http://jmlr.org/papers/v13/gretton12a.html}
}

@InProceedings{Kuleshov2018,
  title = {Accurate Uncertainties for Deep Learning Using Calibrated Regression},
  author = {Kuleshov, Volodymyr and Fenner, Nathan and Ermon, Stefano},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  pages = {2796--2804},
  year = {2018},
  volume = {80},
  series = {Proceedings of Machine Learning Research},
  month = {7},
  publisher = {PMLR},
  pdf = {http://proceedings.mlr.press/v80/kuleshov18a/kuleshov18a.pdf},
}

@InProceedings{Vaicenavicius2019,
  title = {Evaluating model calibration in classification},
  author = {Vaicenavicius, Juozas and Widmann, David and Andersson, Carl and Lindsten, Fredrik and Roll, Jacob and Sch\"{o}n, Thomas},
  booktitle = {Proceedings of Machine Learning Research},
  pages = {3459--3467},
  year = {2019},
  volume = {89},
  series = {Proceedings of Machine Learning Research},
  month = {4},
  publisher = {PMLR},
  pdf = {http://proceedings.mlr.press/v89/vaicenavicius19a/vaicenavicius19a.pdf},
  abstract = {Probabilistic classifiers output a probability distribution on target classes rather than just a class prediction. Besides providing a clear separation of prediction and decision making, the main advantage of probabilistic models is their ability to represent uncertainty about predictions. In safety-critical applications, it is pivotal for a model to possess an adequate sense of uncertainty, which for probabilistic classifiers translates into outputting probability distributions that are consistent with the empirical frequencies observed from realized outcomes. A classifier with such a property is called calibrated. In this work, we develop a general theoretical calibration evaluation framework grounded in probability theory, and point out subtleties present in model calibration evaluation that lead to refined interpretations of existing evaluation techniques. Lastly, we propose new ways to quantify and visualize miscalibration in probabilistic classification, including novel multidimensional reliability diagrams.}
}

@InProceedings{Naeini2015,
  author = {Mahdi Pakdaman Naeini and Gregory Cooper and Milos Hauskrecht},
  title = {Obtaining Well Calibrated Probabilities Using {Bayesian} Binning},
  booktitle = {AAAI Conference on Artificial Intelligence},
  year = {2015},
  keywords = {Bayesian binning; classifier calibration; accurate probability; calibrated probability; Bayesian Scoring},
}

@InProceedings{Kumar2018,
  title = {Trainable Calibration Measures for Neural Networks from Kernel Mean Embeddings},
  author = {Kumar, Aviral and Sarawagi, Sunita and Jain, Ujjwal},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  pages = {2805--2814},
  year = {2018},
  volume = {80},
  series = {Proceedings of Machine Learning Research},
  month = {7},
  publisher = {PMLR},
  pdf = {http://proceedings.mlr.press/v80/kumar18a/kumar18a.pdf},
}

@article{Ho2005,
  doi = {10.1093/biomet/92.1.234},
  year = {2005},
  month = mar,
  publisher = {Oxford University Press ({OUP})},
  volume = {92},
  number = {1},
  pages = {234--241},
  author = {Yvonne H. S. Ho and Stephen M. S. Lee},
  title = {Calibrated interpolated confidence intervals for population quantiles},
  journal = {Biometrika}
}

@article{Fasiolo2020,
  doi = {10.1080/01621459.2020.1725521},
  year = {2020},
  month = mar,
  publisher = {Informa {UK} Limited},
  pages = {1--11},
  author = {Matteo Fasiolo and Simon N. Wood and Margaux Zaffran and Raphaël Nedellec and Yannig Goude},
  title = {Fast Calibrated Additive Quantile Regression},
  journal = {Journal of the American Statistical Association}
}

@article{Rueda2006,
  doi = {10.1007/s00184-006-0116-1},
  year = {2006},
  month = dec,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {66},
  number = {3},
  pages = {355--371},
  author = {M. Rueda and S. Mart{\'{\i}}nez-Puertas and H. Mart{\'{\i}}nez-Puertas and A. Arcos},
  title = {Calibration methods for estimating quantiles},
  journal = {Metrika}
}

@article{Taillardat2016,
  doi = {10.1175/mwr-d-15-0260.1},
  year = {2016},
  month = jun,
  publisher = {American Meteorological Society},
  volume = {144},
  number = {6},
  pages = {2375--2393},
  author = {Maxime Taillardat and Olivier Mestre and Michaël Zamo and Philippe Naveau},
  title = {Calibrated Ensemble Forecasts Using Quantile Regression Forests and Ensemble Model Output Statistics},
  journal = {Monthly Weather Review}
}

@article{Sriperumbudur2011,
  author  = {Bharath K. Sriperumbudur and Kenji Fukumizu and Gert R.G. Lanckriet},
  title   = {Universality, Characteristic Kernels and {RKHS} Embedding of Measures},
  journal = {Journal of Machine Learning Research},
  year    = {2011},
  volume  = {12},
  number  = {70},
  pages   = {2389--2410},
}

@incollection{Fukumizu2008,
  title = {Kernel Measures of Conditional Dependence},
  author = {Fukumizu, Kenji and Gretton, Arthur and Xiaohai Sun and Sch\"{o}lkopf, Bernhard},
  booktitle = {Advances in Neural Information Processing Systems 20},
  editor = {J. C. Platt and D. Koller and Y. Singer and S. T. Roweis},
  pages = {489--496},
  year = {2008},
  publisher = {Curran Associates, Inc.},
}

@article{Fukumizu2004,
  title={Dimensionality reduction for supervised learning with reproducing kernel Hilbert spaces},
  author={Fukumizu, Kenji and Bach, Francis R and Jordan, Michael I},
  journal={Journal of Machine Learning Research},
  volume={5},
  number={Jan},
  pages={73--99},
  year={2004}
}

@incollection{Zaremba2013,
  title = {B-test: A Non-parametric, Low Variance Kernel Two-sample Test},
  author = {Zaremba, Wojciech and Gretton, Arthur and Blaschko, Matthew},
  booktitle = {Advances in Neural Information Processing Systems 26},
  pages = {755--763},
  year = {2013},
}

@article{Hoeffding1948,
  doi = {10.1214/aoms/1177730196},
  year = {1948},
  month = sep,
  publisher = {Institute of Mathematical Statistics},
  volume = {19},
  number = {3},
  pages = {293--325},
  author = {Wassily Hoeffding},
  title = {A Class of Statistics with Asymptotically Normal Distribution},
  journal = {The Annals of Mathematical Statistics}
}

@book{Vaart1998,
  doi = {10.1017/cbo9780511802256},
  year = {1998},
  month = oct,
  publisher = {Cambridge University Press},
  author = {A. W. van der Vaart},
  title = {Asymptotic Statistics}
}

@book{Dudley1989,
  author={Dudley, Richard M.},
  year={1989},
  title={Real analysis and probability},
  publisher={Wadsworth \& Brooks/Cole Pub. Co},
  address={Pacific Grove, Calif},
}

@article{Hotelling1931,
  doi = {10.1214/aoms/1177732979},
  year = {1931},
  month = aug,
  publisher = {Institute of Mathematical Statistics},
  volume = {2},
  number = {3},
  pages = {360--378},
  author = {Harold Hotelling},
  title = {The Generalization of Student's Ratio},
  journal = {The Annals of Mathematical Statistics}
}

@inproceedings{Song2009,
  doi = {10.1145/1553374.1553497},
  year = {2009},
  publisher = {{ACM} Press},
  author = {Le Song and Jonathan Huang and Alex Smola and Kenji Fukumizu},
  title = {Hilbert space embeddings of conditional distributions with applications to dynamical systems},
  booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning - {ICML} 09}
}

@article{Song2013,
  doi = {10.1109/msp.2013.2252713},
  year = {2013},
  month = jul,
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {30},
  number = {4},
  pages = {98--111},
  author = {Le Song and Kenji Fukumizu and Arthur Gretton},
  title = {Kernel Embeddings of Conditional Distributions: A Unified Kernel Framework for Nonparametric Inference in Graphical Models},
  journal = {{IEEE} Signal Processing Magazine}
}

@misc{Park2020,
  title={A Measure-Theoretic Approach to Kernel Conditional Mean Embeddings},
  author={Junhyung Park and Krikamol Muandet},
  year={2020},
  eprint={2002.03689},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@incollection{Ren2016,
  title = {Conditional Generative Moment-Matching Networks},
  author = {Ren, Yong and Zhu, Jun and Li, Jialian and Luo, Yucen},
  booktitle = {Advances in Neural Information Processing Systems 29},
  pages = {2928--2936},
  year = {2016},
}

@article{Fukumizu2013,
  author  = {Kenji Fukumizu and Le Song and Arthur Gretton},
  title   = {Kernel {Bayes'} Rule: {Bayesian} Inference with Positive Definite Kernels},
  journal = {Journal of Machine Learning Research},
  year    = {2013},
  volume  = {14},
  number  = {82},
  pages   = {3753--3783},
  url     = {http://jmlr.org/papers/v14/fukumizu13a.html}
}

@article{Gomez1998,
  doi = {10.1080/03610929808832115},
  year = {1998},
  month = jan,
  publisher = {Informa {UK} Limited},
  volume = {27},
  number = {3},
  pages = {589--600},
  author = {E. G{\'{o}}mez and M.A. Gomez-Viilegas and J.M. Mar{\'{i}}n},
  title = {A multivariate generalization of the power exponential family of distributions},
  journal = {Communications in Statistics - Theory and Methods}
}

@article{GomezSanchezManzano2008,
  doi = {10.1080/03610920701762754},
  year = {2008},
  month = feb,
  publisher = {Informa {UK} Limited},
  volume = {37},
  number = {6},
  pages = {972--985},
  author = {E. G{\'{o}}mez-S{\'{a}}nchez-Manzano and M. A. G{\'{o}}mez-Villegas and J. M. Mar{\'{i}}n},
  title = {Multivariate Exponential Power Distributions as Mixtures of Normal Distributions with {Bayesian} Applications},
  journal = {Communications in Statistics - Theory and Methods}
}

@article{Broecker2009,
  doi = {10.1002/qj.456},
  year = {2009},
  month = jul,
  publisher = {Wiley},
  volume = {135},
  number = {643},
  pages = {1512--1519},
  author = {Jochen Br\"{o}cker},
  title = {Reliability, sufficiency, and the decomposition of proper scores},
  journal = {Quarterly Journal of the Royal Meteorological Society}
}

@article{Gneiting2007,
  doi = {10.1198/016214506000001437},
  year = {2007},
  month = mar,
  publisher = {Informa {UK} Limited},
  volume = {102},
  number = {477},
  pages = {359--378},
  author = {Tilmann Gneiting and Adrian E Raftery},
  title = {Strictly Proper Scoring Rules, Prediction, and Estimation},
  journal = {Journal of the American Statistical Association}
}

@article{Micchelli2005,
  doi = {10.1162/0899766052530802},
  year = {2005},
  month = jan,
  publisher = {{MIT} Press - Journals},
  volume = {17},
  number = {1},
  pages = {177--204},
  author = {Charles A. Micchelli and Massimiliano Pontil},
  title = {On Learning Vector-Valued Functions},
  journal = {Neural Computation}
}

@book{Serfling1980,
  doi = {10.1002/9780470316481},
  year = {1980},
  month = nov,
  publisher = {John Wiley {\&} Sons,  Inc.},
  editor = {Robert J. Serfling},
  title = {Approximation Theorems of Mathematical Statistics}
}

@article{Arcones1992,
  author =       {Arcones, Miguel A and Gin{\'e}, Evarist},
  title =        {On the Bootstrap of {$U$} and {$V$} Statistics},
  journal =      {The Annals of Statistics},
  volume =       20,
  number =       2,
  pages =        {655--674},
  year =         {1992},
  publisher =    {JSTOR},
}

@incollection{Gretton2009,
  title = {A Fast, Consistent Kernel Two-Sample Test},
  author = {Gretton, Arthur and Fukumizu, Kenji and Harchaoui, Za\"{\i}d and Sriperumbudur, Bharath K.},
  booktitle = {Advances in Neural Information Processing Systems 22},
  pages = {673--681},
  year = {2009},
}

@incollection{Gretton2007,
  title = {A Kernel Method for the Two-Sample-Problem},
  author = {Gretton, Arthur and Borgwardt, Karsten and Malte Rasch and Sch\"{o}lkopf, Bernhard and Alex J. Smola},
  booktitle = {Advances in Neural Information Processing Systems 19},
  pages = {513--520},
  year = {2007},
}

@book{Johnson1994,
  author={Johnson,Norman L. and Kotz,Samuel and Balakrishnan,N.},
  year={1994},
  title={Continuous univariate distributions: Vol. 1},
  publisher={Wiley},
  address={New York},
  edition={2nd},
  keywords={Statistisk metod; Matematisk statistik; Mathematical statistics; distribution (probability theory); Probability theory; Distributionsteori; Sannolikhetskalkyl och matematisk statistik; Sannolikhetskalkyl; statistical methods; Fördelningsteori},
  language={English},
}

@InProceedings{Glorot2010,
  title = {Understanding the difficulty of training deep feedforward neural networks},
  author = {Xavier Glorot and Yoshua Bengio},
  booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = {249--256},
  year = {2010},
  volume = {9},
  series = {Proceedings of Machine Learning Research},
  month = {5},
  publisher = {PMLR},
  pdf = {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
  abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}
}

@misc{Innes2018a,
    title={Fashionable Modelling with {Flux}},
    author={Michael Innes and Elliot Saba and Keno Fischer and Dhairya Gandhi and Marco Concetto Rudilosso and Neethu Mariya Joy and Tejan Karmali and Avik Pal and Viral Shah},
    year={2018},
    eprint={1811.01457},
    archivePrefix={arXiv},
    primaryClass={cs.PL}
}

@article{Innes2018b,
  doi = {10.21105/joss.00602},
  year = {2018},
  month = may,
  publisher = {The Open Journal},
  volume = {3},
  number = {25},
  pages = {602},
  author = {Mike Innes},
  title = {{Flux}: Elegant machine learning with {Julia}},
  journal = {Journal of Open Source Software}
}

@incollection{Kendall2017,
  title = {What Uncertainties Do We Need in {Bayesian} Deep Learning for Computer Vision?},
  author = {Kendall, Alex and Gal, Yarin},
  booktitle = {Advances in Neural Information Processing Systems 30},
  pages = {5574--5584},
  year = {2017},
}

@article{DeGroot1983,
  doi = {10.2307/2987588},
  year = {1983},
  month = mar,
  publisher = {{JSTOR}},
  volume = {32},
  number = {1/2},
  pages = {12},
  author = {Morris H. DeGroot and Stephen E. Fienberg},
  title = {The Comparison and Evaluation of Forecasters},
  journal = {The Statistician}
}

@article{Murphy1977,
  doi = {10.2307/2346866},
  year = {1977},
  publisher = {{JSTOR}},
  volume = {26},
  number = {1},
  pages = {41},
  author = {Allan H. Murphy and Robert L. Winkler},
  title = {Reliability of Subjective Probability Forecasts of Precipitation and Temperature},
  journal = {Applied Statistics}
}

@inproceedings{Gustafsson2019,
  title={Evaluating scalable {Bayesian} deep learning methods for robust computer vision},
  author={Gustafsson, Fredrik K and Danelljan, Martin and Sch{\"o}n, Thomas B},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
  year={2020}
}

@book{Villani2009,
  doi = {10.1007/978-3-540-71050-9},
  year = {2009},
  publisher = {Springer Berlin Heidelberg},
  author = {C{\'{e}}dric Villani},
  title = {Optimal Transport}
}

@book{Berg1984,
  doi = {10.1007/978-1-4612-1128-0},
  year = {1984},
  publisher = {Springer New York},
  author = {Christian Berg and Jens Peter Reus Christensen and Paul Ressel},
  title = {Harmonic Analysis on Semigroups}
}

@misc{Peyre2018,
    title={Computational Optimal Transport},
    author={Gabriel Peyré and Marco Cuturi},
    year={2018},
    eprint={1803.00567},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@article{Broecker2007,
  doi = {10.1175/waf993.1},
  year = {2007},
  month = jun,
  publisher = {American Meteorological Society},
  volume = {22},
  number = {3},
  pages = {651--661},
  author = {Jochen Br\"{o}cker and Leonard A. Smith},
  title = {Increasing the Reliability of Reliability Diagrams},
  journal = {Weather and Forecasting}
}

@incollection{Jitkrittum2016,
  title = {Interpretable Distribution Features with Maximum Testing Power},
  author = {Jitkrittum, Wittawat and Szab\'{o}, Zolt\'{a}n and Chwialkowski, Kacper P and Gretton, Arthur},
  booktitle = {Advances in Neural Information Processing Systems 29},
  pages = {181--189},
  year = {2016},
}

@incollection{Genevay2016,
  title = {Stochastic Optimization for Large-scale Optimal Transport},
  author = {Genevay, Aude and Cuturi, Marco and Peyr\'{e}, Gabriel and Bach, Francis},
  booktitle = {Advances in Neural Information Processing Systems 29},
  pages = {3440--3448},
  year = {2016},
}

@article{Deledalle2018,
  doi = {10.1137/18m116890x},
  year = {2018},
  month = jan,
  publisher = {Society for Industrial {\&} Applied Mathematics ({SIAM})},
  volume = {11},
  number = {4},
  pages = {2568--2609},
  author = {Charles-Alban Deledalle and Shibin Parameswaran and Truong Q. Nguyen},
  title = {Image Denoising with Generalized Gaussian Mixture Model Patch Priors},
  journal = {{SIAM} Journal on Imaging Sciences}
}

@article{Yakowitz1968,
  doi = {10.1214/aoms/1177698520},
  year = {1968},
  month = feb,
  publisher = {Institute of Mathematical Statistics},
  volume = {39},
  number = {1},
  pages = {209--214},
  author = {Sidney J. Yakowitz and John D. Spragins},
  title = {On the Identifiability of Finite Mixtures},
  journal = {The Annals of Mathematical Statistics}
}

@article{Chen2020,
  doi = {10.1109/tpami.2019.2908635},
  year = {2020},
  month = sep,
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {42},
  number = {9},
  pages = {2133--2147},
  author = {Yukun Chen and Jianbo Ye and Jia Li},
  title = {Aggregated {Wasserstein} Distance and State Registration for Hidden {Markov} Models},
  journal = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence}
}
