{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Friedman regression problem\n",
    "\n",
    "\n",
    "You are seeing the\n",
    "notebook output generated by\n",
    "[Literate.jl](https://github.com/fredrikekre/Literate.jl) from the\n",
    "[Julia source file](https://github.com/devmotion/Calibration_ICLR2021/blob/main/experiments/src/friedman/script.jl).\n",
    "The corresponding\n",
    "HTML output can be viewed [here](https://devmotion.github.io/Calibration_ICLR2021/dev/generated/friedman/),\n",
    "and the plain script output can be found [here](./friedman.jl).\n",
    "\n",
    "> **Note**\n",
    "> If you want to run the experiments, make sure you have an identical environment.\n",
    "> Please use Julia 1.5.3 and activate and instantiate the environment using\n",
    "> [this Project.toml file](https://github.com/devmotion/Calibration_ICLR2021/blob/main/experiments/src/friedman/Project.toml)\n",
    "> and [this Manifest.toml file](https://github.com/devmotion/Calibration_ICLR2021/blob/main/experiments/src/friedman/Manifest.toml).\n",
    ">\n",
    "> [The Github repository](https://github.com/devmotion/Calibration_ICLR2021/) contains\n",
    "> [more detailed instructions](https://github.com/devmotion/Calibration_ICLR2021/blob/main/experiments/README.md) and a\n",
    "> `nix` project environment with a pinned Julia binary for improved reproducibility."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Packages"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Arrow\n",
    "using CairoMakie\n",
    "using CalibrationErrors\n",
    "using CalibrationErrorsDistributions\n",
    "using CalibrationTests\n",
    "using CSV\n",
    "using DataFrames\n",
    "using Distributions\n",
    "using Flux\n",
    "using ProgressLogging\n",
    "using Query\n",
    "using Roots\n",
    "using Showoff\n",
    "\n",
    "using Random\n",
    "\n",
    "using CairoMakie.AbstractPlotting.ColorSchemes: Dark2_8\n",
    "\n",
    "using Logging: with_logger\n",
    "using TerminalLoggers: TerminalLogger\n",
    "\n",
    "# set random seed\n",
    "Random.seed!(1234)\n",
    "\n",
    "# create path before saving and\n",
    "# use a different line join style to avoid visually irritating and incorrect values\n",
    "function wsavefig(file, fig::Figure=current_figure())\n",
    "    mkpath(dirname(file))\n",
    "    scene = CairoMakie.AbstractPlotting.get_scene(fig)\n",
    "    ext = lowercase(replace(splitext(file)[2], \".\" => \"\"))\n",
    "    screen = CairoMakie.CairoScreen(scene, file, Symbol(ext))\n",
    "    CairoMakie.Cairo.set_line_join(screen.context, CairoMakie.Cairo.CAIRO_LINE_JOIN_BEVEL)\n",
    "    CairoMakie.cairo_draw(screen, scene)\n",
    "    CairoMakie.Cairo.finish(screen.surface)\n",
    "    return screen\n",
    "end\n",
    "\n",
    "# define progress logging frontend\n",
    "const PROGRESSLOGGER = TerminalLogger()\n",
    "\n",
    "# define non-intrusive plotting style\n",
    "set_theme!(\n",
    "    Theme(;\n",
    "        Axis=(\n",
    "            rightspinevisible=false,\n",
    "            topspinevisible=false,\n",
    "            xgridvisible=false,\n",
    "            ygridvisible=false,\n",
    "        ),\n",
    "        Legend=(framevisible=false,),\n",
    "    ),\n",
    ")"
   ],
   "metadata": {},
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Regression problem\n",
    "\n",
    "We study the so-called Friedman 1 regression problem, which was initially described for\n",
    "200 inputs in the six-dimensional unit hypercube and later modified to 100 inputs in the\n",
    "10-dimensional unit hypercube. In this regression problem real-valued target $Y$ depends\n",
    "on input $X$ via\n",
    "$$\n",
    "Y = 10 \\sin{(\\pi X_1 X_2)} + 20 (X_3 − 0.5)^2 + 10 X_4 + 5 X_5 + \\epsilon,\n",
    "$$\n",
    "where noise $\\epsilon$ is typically chosen to be independently standard normally\n",
    "distributed.\n",
    "We generate a training data set of 100 inputs distributed uniformly at random in the\n",
    "10-dimensional unit hypercube and corresponding targets with identically and independently\n",
    "distributed noise following a standard normal distribution."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "friedman1(x) = 10 * sinpi(x[1] * x[2]) + 20 * (x[3] - 1//2)^2 + 10 * x[4] + 5 * x[5]\n",
    "\n",
    "function sample_data(n::Int)\n",
    "    # sample inputs\n",
    "    xs = rand(10, n)\n",
    "\n",
    "    # sample targets\n",
    "    ys = map(eachcol(xs)) do x\n",
    "        return friedman1(x) + randn()\n",
    "    end\n",
    "\n",
    "    return xs, ys\n",
    "end\n",
    "\n",
    "Random.seed!(100)\n",
    "train_data = sample_data(100);"
   ],
   "metadata": {},
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the evaluation of the models we use another data set of 50 samples that is\n",
    "sampled according to the same law."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Random.seed!(200)\n",
    "test_data = sample_data(50);"
   ],
   "metadata": {},
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model\n",
    "\n",
    "We consider models $P^{(\\theta,\\sigma^2)}$ of normal distributions with fixed variance\n",
    "$$\n",
    "P^{(\\theta,\\sigma^2)}(Y | X = x) = \\mathcal{N}(f_{\\theta}(x), \\sigma^2),\n",
    "$$\n",
    "where $f_{\\theta}(x)$, the model of the mean of the distribution $\\mathbb{P}(Y|X = x)$,\n",
    "is given by a fully connected neural network with two hidden layers with 200 and 50\n",
    "hidden units and ReLU activation functions. The parameters of the neural network are\n",
    "denoted by $\\theta$."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "nn_model (generic function with 1 method)"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "cell_type": "code",
   "source": [
    "# `Float64` version of `Flux.glorot_uniform`\n",
    "function glorot_uniform(nout::Int, nin::Int)\n",
    "    return (rand(nout, nin) .- 0.5) .* sqrt(24 / (nout + nin))\n",
    "end\n",
    "\n",
    "# neural network model\n",
    "function nn_model()\n",
    "    # initial parameters\n",
    "    f = Chain(\n",
    "        Dense(10, 200, relu; initW=glorot_uniform, initb=zeros),\n",
    "        Dense(200, 50, relu; initW=glorot_uniform, initb=zeros),\n",
    "        Dense(50, 1; initW=glorot_uniform, initb=zeros),\n",
    "        vec,\n",
    "    )\n",
    "    σ = Random.randexp()\n",
    "\n",
    "    return f, σ\n",
    "end"
   ],
   "metadata": {},
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training\n",
    "\n",
    "We use a maximum likelihood approach and train the parameters $\\theta$ of the model\n",
    "for 5000 iterations by minimizing the mean squared error on the training data set\n",
    "using ADAM. In each iteration, the variance $\\sigma^2$ is set to the maximizer of the\n",
    "likelihood of the training data set.\n",
    "\n",
    "We train 10 models for each combination of regression problem and model\n",
    "category, and compute the predicted distributions on the training and test data sets\n",
    "in each iteration step.\n",
    "\n",
    "The initial values of the weight matrices of the neural networks are sampled from the\n",
    "[uniform Glorot initialization](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)\n",
    "and the offset vectors are initialized with zeros. The model parameters are learnt by\n",
    "iteratively minimizing the negative log-likelihood on the training data set.\n",
    "The parameters of the neural networks are trained by gradient descent with the\n",
    "[Adam optimization algorithm](https://arxiv.org/pdf/1412.6980.pdf) (default\n",
    "settings in [Flux.jl](https://github.com/FluxML/Flux.jl)), and in each iteration\n",
    "step the variance parameter of the predicted Gaussian distributions is set to the optimal\n",
    "value with respect to the negative log-likelihood on the training data set."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌ Info: training NN model: run 1\n",
      "└ @ Main.##281 string:51\n",
      "┌ Info: training NN model: run 2\n",
      "└ @ Main.##281 string:51\n",
      "┌ Info: training NN model: run 3\n",
      "└ @ Main.##281 string:51\n",
      "┌ Info: training NN model: run 4\n",
      "└ @ Main.##281 string:51\n",
      "┌ Info: training NN model: run 5\n",
      "└ @ Main.##281 string:51\n",
      "┌ Info: training NN model: run 6\n",
      "└ @ Main.##281 string:51\n",
      "┌ Info: training NN model: run 7\n",
      "└ @ Main.##281 string:51\n",
      "┌ Info: training NN model: run 8\n",
      "└ @ Main.##281 string:51\n",
      "┌ Info: training NN model: run 9\n",
      "└ @ Main.##281 string:51\n",
      "┌ Info: training NN model: run 10\n",
      "└ @ Main.##281 string:51\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "function train(id, (train_xs, train_ys), (test_xs, _))\n",
    "    # check if file exists\n",
    "    filename = joinpath(\"data\", \"friedman\", \"predictions_id=$(id).arrow\")\n",
    "    isfile(filename) && return nothing\n",
    "\n",
    "    # compute the predictions of the initial neural network\n",
    "    f, σ = nn_model()\n",
    "    train_μs = f(train_xs)\n",
    "    test_μs = f(test_xs)\n",
    "\n",
    "    # save the initial model and its predictions\n",
    "    niters = 5000\n",
    "    train_μss = Vector{typeof(train_μs)}(undef, niters + 1)\n",
    "    test_μss = Vector{typeof(test_μs)}(undef, niters + 1)\n",
    "    σs = Vector{typeof(σ)}(undef, niters + 1)\n",
    "    train_μss[1] = train_μs\n",
    "    test_μss[1] = test_μs\n",
    "    σs[1] = σ\n",
    "\n",
    "    # train with ADAM\n",
    "    params = Flux.Params(Flux.params(f))\n",
    "    opt = ADAM()\n",
    "    @progress name = \"training (id = $id)\" for i in 2:(niters + 1)\n",
    "        # compute gradients\n",
    "        gradients = gradient(params) do\n",
    "            return Flux.Losses.mse(f(train_xs), train_ys)\n",
    "        end\n",
    "\n",
    "        # update the parameters\n",
    "        Flux.Optimise.update!(opt, params, gradients)\n",
    "\n",
    "        # update the variance\n",
    "        yhats = f(train_xs)\n",
    "        σ = sqrt(Flux.Losses.mse(yhats, train_ys))\n",
    "\n",
    "        # save the model and its predictions\n",
    "        train_μss[i] = yhats\n",
    "        test_μss[i] = f(test_xs)\n",
    "        σs[i] = σ\n",
    "    end\n",
    "\n",
    "    # save the predictions\n",
    "    mkpath(dirname(filename))\n",
    "    Arrow.write(filename, (train_μs=train_μss, test_μs=test_μss, σ=σs))\n",
    "\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "Random.seed!(100)\n",
    "for (id, seed) in enumerate(rand(UInt, 10))\n",
    "    @info \"training NN model: run $id\"\n",
    "    Random.seed!(seed)\n",
    "    with_logger(PROGRESSLOGGER) do\n",
    "        train(id, train_data, test_data)\n",
    "    end\n",
    "end"
   ],
   "metadata": {},
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluations\n",
    "\n",
    "We estimate the average negative log-likelihood (NLL) and the mean squared error (MSE).\n",
    "Additionally, we estimate the average pinball loss\n",
    "$$\n",
    "\\frac{1}{n_\\tau} \\sum_{i=1}^{n_\\tau} \\mathbb{E}_{X,Y} L_{\\tau_i}\\big(Y, \\mathrm{quantile}(P_X,\\tau_i)\\big)\n",
    "$$\n",
    "for quantile levels $\\tau_i = 0.05i$ ($n_\\tau = 19$), where\n",
    "$L_\\tau(y, \\tilde{y}) = (1 - \\tau) (\\tilde{y} - y)_{+} + \\tau (y - \\tilde{y})_{+}$\n",
    "for observation $y$ and prediction $\\tilde{y}$, and\n",
    "$\\mathrm{quantile}(P_x, \\tau) = \\inf_y \\{P_x(Y \\leq y) \\geq \\tau\\}$ for quantile level\n",
    "$\\tau \\in [0, 1]$."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "mean_pinball_loss (generic function with 1 method)"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "cell_type": "code",
   "source": [
    "function pinball_loss(level::Real, y::Real, quantile::Real)\n",
    "    δ = quantile - y\n",
    "    return ((δ > 0) - level) * δ\n",
    "end\n",
    "\n",
    "function mean_pinball_loss(prediction, y)\n",
    "    levels = 0.05:0.05:0.95\n",
    "    return mean(pinball_loss(τ, y, quantile(prediction, τ)) for τ in levels)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "Moreover, we estimate the squared kernel calibration error (SKCE)\n",
    "and the p-value of the null hypothesis that the model is calibrated.\n",
    "for calibration on the training and test data set for every model and\n",
    "every iteration."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌ Info: evaluating training statistics: run 1\n",
      "└ @ Main.##281 string:66\n",
      "┌ Info: evaluating test statistics: run 1\n",
      "└ @ Main.##281 string:73\n",
      "┌ Info: evaluating training statistics: run 2\n",
      "└ @ Main.##281 string:66\n",
      "┌ Info: evaluating test statistics: run 2\n",
      "└ @ Main.##281 string:73\n",
      "┌ Info: evaluating training statistics: run 3\n",
      "└ @ Main.##281 string:66\n",
      "┌ Info: evaluating test statistics: run 3\n",
      "└ @ Main.##281 string:73\n",
      "┌ Info: evaluating training statistics: run 4\n",
      "└ @ Main.##281 string:66\n",
      "┌ Info: evaluating test statistics: run 4\n",
      "└ @ Main.##281 string:73\n",
      "┌ Info: evaluating training statistics: run 5\n",
      "└ @ Main.##281 string:66\n",
      "┌ Info: evaluating test statistics: run 5\n",
      "└ @ Main.##281 string:73\n",
      "┌ Info: evaluating training statistics: run 6\n",
      "└ @ Main.##281 string:66\n",
      "┌ Info: evaluating test statistics: run 6\n",
      "└ @ Main.##281 string:73\n",
      "┌ Info: evaluating training statistics: run 7\n",
      "└ @ Main.##281 string:66\n",
      "┌ Info: evaluating test statistics: run 7\n",
      "└ @ Main.##281 string:73\n",
      "┌ Info: evaluating training statistics: run 8\n",
      "└ @ Main.##281 string:66\n",
      "┌ Info: evaluating test statistics: run 8\n",
      "└ @ Main.##281 string:73\n",
      "┌ Info: evaluating training statistics: run 9\n",
      "└ @ Main.##281 string:66\n",
      "┌ Info: evaluating test statistics: run 9\n",
      "└ @ Main.##281 string:73\n",
      "┌ Info: evaluating training statistics: run 10\n",
      "└ @ Main.##281 string:66\n",
      "┌ Info: evaluating test statistics: run 10\n",
      "└ @ Main.##281 string:73\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "function evaluate_models(dataset, id, ys)\n",
    "    # output file\n",
    "    out = joinpath(\"data\", \"friedman\", \"statistics_id=$(id)_dataset=$(dataset).csv\")\n",
    "    isfile(out) && return nothing\n",
    "\n",
    "    # load data\n",
    "    filename = joinpath(\"data\", \"friedman\", \"predictions_id=$(id).arrow\")\n",
    "    isfile(filename) || error(\"predictions for run \", id, \" not found\")\n",
    "    tbl = Arrow.Table(filename)\n",
    "    σs = tbl.σ\n",
    "    μss = getproperty(tbl, Symbol(dataset, :_μs))\n",
    "    predictionss = map(μss, σs) do μs, σ\n",
    "        return map(μs) do μ\n",
    "            return Normal(μ, σ)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # define kernel\n",
    "    kernel = WassersteinExponentialKernel() ⊗ SqExponentialKernel()\n",
    "\n",
    "    return evaluate_stats(out, predictionss, ys, kernel)\n",
    "end\n",
    "\n",
    "function evaluate_stats(file, predictionss, ys, kernel)\n",
    "    mkpath(dirname(file))\n",
    "    open(file, \"w\") do f\n",
    "        # print headers\n",
    "        println(f, \"iteration,statistic,estimate\")\n",
    "\n",
    "        @progress name = \"iterations\" for (i, predictions) in enumerate(predictionss)\n",
    "            # average NLL\n",
    "            nll = -mean(map(logpdf, predictions, ys))\n",
    "            println(f, i - 1, \",NLL,\", nll)\n",
    "\n",
    "            # mean squared error\n",
    "            mse = Flux.Losses.mse(map(mean, predictions), ys)\n",
    "            println(f, i - 1, \",MSE,\", mse)\n",
    "\n",
    "            # pinball loss\n",
    "            pinball = mean(map(mean_pinball_loss, predictions, ys))\n",
    "            println(f, i - 1, \",pinball,\", pinball)\n",
    "\n",
    "            # unbiased estimator of SKCE\n",
    "            unbiased_estimator = UnbiasedSKCE(kernel)\n",
    "            skce = calibrationerror(unbiased_estimator, predictions, ys)\n",
    "            println(f, i - 1, \",SKCE (unbiased),\", skce)\n",
    "\n",
    "            # biased estimator of SKCE\n",
    "            biased_estimator = BiasedSKCE(kernel)\n",
    "            skce = calibrationerror(biased_estimator, predictions, ys)\n",
    "            println(f, i - 1, \",SKCE (biased),\", skce)\n",
    "\n",
    "            # p-value\n",
    "            test = AsymptoticSKCETest(kernel, predictions, ys)\n",
    "            p = pvalue(test; bootstrap_iters=1_000)\n",
    "            println(f, i - 1, \",p-value,\", p)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "Random.seed!(300)\n",
    "for (id, seed) in enumerate(rand(UInt, 10))\n",
    "    # evaluate models on training data set\n",
    "    @info \"evaluating training statistics: run $id\"\n",
    "    Random.seed!(seed)\n",
    "    with_logger(PROGRESSLOGGER) do\n",
    "        evaluate_models(\"train\", id, train_data[2])\n",
    "    end\n",
    "\n",
    "    # evaluate models on test data set\n",
    "    @info \"evaluating test statistics: run $id\"\n",
    "    Random.seed!(seed)\n",
    "    with_logger(PROGRESSLOGGER) do\n",
    "        evaluate_models(\"test\", id, test_data[2])\n",
    "    end\n",
    "end"
   ],
   "metadata": {},
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ensembles\n",
    "\n",
    "We create an ensemble of models from the trained models by combining models at each\n",
    "training iteration.\n",
    "\n",
    "We evaluate the same statistics as for the individual ensemble members above. Since the\n",
    "quantiles of a Gaussian mixture model are not available in a closed-form expression, we\n",
    "use a bisection algorithm."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌ Info: evaluating training statistics: ensembles of runs 1:10\n",
      "└ @ Main.##281 string:43\n",
      "┌ Info: evaluating test statistics: ensembles of runs 1:10\n",
      "└ @ Main.##281 string:50\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "# see https://github.com/JuliaStats/Distributions.jl/pull/1195\n",
    "function Distributions.quantile(dist::UnivariateMixture{Continuous}, level::Real)\n",
    "    ps = probs(dist)\n",
    "    bracket = extrema(\n",
    "        quantile(component(dist, i), level) for (i, pi) in enumerate(ps) if pi > 0\n",
    "    )\n",
    "    return find_zero(bracket) do x\n",
    "        cdf(dist, x) - level\n",
    "    end\n",
    "end\n",
    "\n",
    "function evaluate_ensembles(dataset, ys)\n",
    "    # output file\n",
    "    out = joinpath(\"data\", \"friedman\", \"statistics_ensembles_dataset=$(dataset).csv\")\n",
    "    isfile(out) && return nothing\n",
    "\n",
    "    # load data\n",
    "    tables = map(1:10) do id\n",
    "        file = joinpath(\"data\", \"friedman\", \"predictions_id=$(id).arrow\")\n",
    "        isfile(file) || error(\"predictions for run \", id, \" not found\")\n",
    "        return Arrow.Table(file)\n",
    "    end\n",
    "    nmodels = length(first(tables).σ)\n",
    "    nsamples = length(ys)\n",
    "    μs = Symbol(dataset, :_μs)\n",
    "    predictionss = map(1:nmodels) do i\n",
    "        map(1:nsamples) do j\n",
    "            MixtureModel(\n",
    "                map(tables) do table\n",
    "                    Normal(getproperty(table, μs)[i][j], table.σ[i])\n",
    "                end,\n",
    "            )\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # kernel\n",
    "    kernel = MixtureWassersteinExponentialKernel() ⊗ SqExponentialKernel()\n",
    "\n",
    "    return evaluate_stats(out, predictionss, ys, kernel)\n",
    "end\n",
    "\n",
    "# evaluate ensembles on training data set\n",
    "@info \"evaluating training statistics: ensembles of runs 1:10\"\n",
    "Random.seed!(400)\n",
    "with_logger(PROGRESSLOGGER) do\n",
    "    evaluate_ensembles(\"train\", train_data[2])\n",
    "end\n",
    "\n",
    "# evaluate ensembles on test data set\n",
    "@info \"evaluating test statistics: ensembles of runs 1:10\"\n",
    "Random.seed!(500)\n",
    "with_logger(PROGRESSLOGGER) do\n",
    "    evaluate_ensembles(\"test\", test_data[2])\n",
    "end"
   ],
   "metadata": {},
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualization\n",
    "\n",
    "We visualize the resulting estimates by overlaying them with the estimates of the\n",
    "individual models."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function logtickformat(base::Int)\n",
    "    function format(values)\n",
    "        return map(Base.Fix2(logformat, base), showoff(values))\n",
    "    end\n",
    "    return format\n",
    "end\n",
    "\n",
    "function logformat(digits::String, base::Int)\n",
    "    buf = IOBuffer()\n",
    "    print(buf, base)\n",
    "    for c in digits\n",
    "        if '0' ≤ c ≤ '9'\n",
    "            print(buf, Showoff.superscript_numerals[c - '0' + 1])\n",
    "        elseif c == '-'\n",
    "            print(buf, '⁻')\n",
    "        elseif c == '.'\n",
    "            print(buf, '·')\n",
    "        end\n",
    "    end\n",
    "    return String(take!(buf))\n",
    "end\n",
    "\n",
    "function asinhtickformat(factor)\n",
    "    function format(values)\n",
    "        return \"sinh(\" .* showoff(values ./ factor) .* \")\"\n",
    "    end\n",
    "    return format\n",
    "end\n",
    "\n",
    "function plot_statistic!(ax::Axis, iterations, statistic; transform=identity)\n",
    "    # load and filter statistics\n",
    "    models_df = mapreduce(vcat, Iterators.product(1:10, (\"train\", \"test\"))) do (id, dataset)\n",
    "        file = joinpath(\"data\", \"friedman\", \"statistics_id=$(id)_dataset=$(dataset).csv\")\n",
    "        df = @from i in DataFrame(CSV.File(file)) begin\n",
    "            @where i.statistic == statistic && i.iteration in iterations\n",
    "            @select i\n",
    "            @collect DataFrame\n",
    "        end\n",
    "        df[!, :dataset] .= dataset\n",
    "        return df\n",
    "    end\n",
    "    sort!(models_df, :iteration)\n",
    "\n",
    "    ensembles_df = mapreduce(vcat, (\"train\", \"test\")) do dataset\n",
    "        file = joinpath(\"data\", \"friedman\", \"statistics_ensembles_dataset=$(dataset).csv\")\n",
    "        df = @from i in DataFrame(CSV.File(file)) begin\n",
    "            @where i.statistic == statistic && i.iteration in iterations\n",
    "            @select i\n",
    "            @collect DataFrame\n",
    "        end\n",
    "        df[!, :dataset] .= dataset\n",
    "        return df\n",
    "    end\n",
    "    sort!(ensembles_df, :iteration)\n",
    "\n",
    "    # plot evaluations\n",
    "    for (j, dataset) in enumerate((\"train\", \"test\"))\n",
    "        # models\n",
    "        if !isempty(models_df)\n",
    "            models = @from i in models_df begin\n",
    "                @where i.dataset == dataset\n",
    "                @orderby i.iteration\n",
    "                @group i by i.iteration into g\n",
    "                @select {\n",
    "                    iteration = key(g),\n",
    "                    mean = transform(mean(g.estimate)),\n",
    "                    min = transform(minimum(g.estimate)),\n",
    "                    max = transform(maximum(g.estimate)),\n",
    "                }\n",
    "                @collect DataFrame\n",
    "            end\n",
    "            color = Dark2_8[2 * j - 1]\n",
    "            band!(ax, models.iteration, models.min, models.max; color=(color, 0.2))\n",
    "            lines!(ax, models.iteration, models.mean; color=color)\n",
    "        end\n",
    "\n",
    "        # ensembles\n",
    "        if !isempty(ensembles_df)\n",
    "            ensembles = @from i in ensembles_df begin\n",
    "                @where i.dataset == dataset\n",
    "                @orderby i.iteration\n",
    "                @select {i.iteration, estimate = transform(i.estimate)}\n",
    "                @collect DataFrame\n",
    "            end\n",
    "            color = Dark2_8[2 * j]\n",
    "            lines!(ax, ensembles.iteration, ensembles.estimate; color=color)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return ax\n",
    "end\n",
    "\n",
    "function statsplot(iterations, statistics::Matrix; kwargs...)\n",
    "    # create scene\n",
    "    nrows, ncols = size(statistics)\n",
    "    fig = Figure(; resolution=(500 * ncols, 200 * nrows))\n",
    "\n",
    "    # for all statistics\n",
    "    transforms = Dict(\n",
    "        \"MSE\" => log10,\n",
    "        \"NLL\" => asinh,\n",
    "        \"pinball\" => log10,\n",
    "        \"SKCE (unbiased)\" => x -> asinh(1000 * x),\n",
    "        \"SKCE (biased)\" => log10,\n",
    "        \"p-value\" => identity,\n",
    "    )\n",
    "    ytickformats = Dict(\n",
    "        \"MSE\" => logtickformat(10),\n",
    "        \"NLL\" => asinhtickformat(1),\n",
    "        \"pinball\" => logtickformat(10),\n",
    "        \"SKCE (unbiased)\" => asinhtickformat(1000),\n",
    "        \"SKCE (biased)\" => logtickformat(10),\n",
    "        \"p-value\" => AbstractPlotting.automatic,\n",
    "    )\n",
    "    for j in 1:ncols, i in 1:nrows\n",
    "        statistic = statistics[i, j]\n",
    "\n",
    "        # add axis\n",
    "        ax = Axis(\n",
    "            fig[i, j];\n",
    "            title=statistic,\n",
    "            ytickformat=ytickformats[statistic],\n",
    "            xticklabelsize=12.0f0,\n",
    "            yticklabelsize=12.0f0,\n",
    "        )\n",
    "\n",
    "        # plot results\n",
    "        plot_statistic!(ax, iterations, statistic; transform=transforms[statistic])\n",
    "\n",
    "        # hide x ticks in every column apart from bottom row\n",
    "        if i < nrows\n",
    "            hidexdecorations!(ax)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # link all x axes\n",
    "    axes = contents(fig[1:nrows, 1:ncols])\n",
    "    linkxaxes!(axes...)\n",
    "\n",
    "    # add labels\n",
    "    Label(fig[end + 1, 1:ncols], \"iteration\")\n",
    "    Label(fig[1:nrows, 0], \"estimate\"; rotation=π / 2, tellheight=false)\n",
    "\n",
    "    # tighten limits\n",
    "    for ax in axes\n",
    "        tightlimits!(ax, Left(), Right())\n",
    "    end\n",
    "\n",
    "    # create legend on right\n",
    "    elems = map(1:2:3) do i\n",
    "        model_color = Dark2_8[i]\n",
    "        ensemble_color = Dark2_8[i + 1]\n",
    "        return [\n",
    "            [\n",
    "                PolyElement(;\n",
    "                    color=(model_color, 0.2),\n",
    "                    strokewidth=0,\n",
    "                    strokecolor=(model_color, 0.2),\n",
    "                ),\n",
    "                LineElement(; color=model_color, linestyle=nothing),\n",
    "            ],\n",
    "            LineElement(; color=ensemble_color, linestyle=nothing),\n",
    "        ]\n",
    "    end\n",
    "    Legend(\n",
    "        fig[1:nrows, end + 1],\n",
    "        elems,\n",
    "        [[\"models\", \"ensemble\"], [\"models\", \"ensemble\"]],\n",
    "        [\"training\", \"test\"];\n",
    "        titlefont=\"Dejavu Sans Bold\",\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "end\n",
    "\n",
    "statsplot(0:5000, [\"MSE\" \"SKCE (unbiased)\"; \"NLL\" \"SKCE (biased)\"; \"pinball\" \"p-value\"])\n",
    "wsavefig(\"figures/friedman/statsplot.svg\");"
   ],
   "metadata": {},
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](figures/friedman/statsplot.svg)"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "statsplot(10:1500, [\"MSE\" \"NLL\"; \"SKCE (biased)\" \"p-value\"])\n",
    "wsavefig(\"figures/friedman/statsplot_zoom.svg\");"
   ],
   "metadata": {},
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](figures/friedman/statsplot_zoom.svg)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  },
  "kernelspec": {
   "name": "julia-1.5",
   "display_name": "Julia 1.5.3",
   "language": "julia"
  }
 },
 "nbformat": 4
}
