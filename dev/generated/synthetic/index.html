<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Synthetic models · Calibration tests beyond classification</title><link rel="canonical" href="https://devmotion.github.io/Calibration_ICLR2021/generated/synthetic/"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">Calibration tests beyond classification</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../software/">Software</a></li><li><span class="tocitem">Experiments</span><ul><li><a class="tocitem" href="../ols/">Ordinary least squares</a></li><li class="is-active"><a class="tocitem" href>Synthetic models</a><ul class="internal"><li><a class="tocitem" href="#Packages"><span>Packages</span></a></li><li><a class="tocitem" href="#Synthetic-models-2"><span>Synthetic models</span></a></li><li><a class="tocitem" href="#Convergence-and-computation-time-of-estimators"><span>Convergence and computation time of estimators</span></a></li><li><a class="tocitem" href="#Test-errors-and-computation-time-of-calibration-tests"><span>Test errors and computation time of calibration tests</span></a></li></ul></li><li><a class="tocitem" href="../friedman/">Friedman regression problem</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Experiments</a></li><li class="is-active"><a href>Synthetic models</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Synthetic models</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/devmotion/Calibration_ICLR2021/blob/main/experiments/src/synthetic/script.jl" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Synthetic-models"><a class="docs-heading-anchor" href="#Synthetic-models">Synthetic models</a><a id="Synthetic-models-1"></a><a class="docs-heading-anchor-permalink" href="#Synthetic-models" title="Permalink"></a></h1><p><a href="https://nbviewer.jupyter.org/github/devmotion/Calibration_ICLR2021/blob/gh-pages/dev/generated/synthetic.ipynb"><img src="https://img.shields.io/badge/show-nbviewer-579ACA.svg" alt/></a></p><p>You are seeing the HTML output generated by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a> from the <a href="https://github.com/devmotion/Calibration_ICLR2021/blob/main/experiments/src/synthetic/script.jl">Julia source file</a>. The corresponding notebook can be viewed in <a href="https://nbviewer.jupyter.org/github/devmotion/Calibration_ICLR2021/blob/gh-pages/dev/generated/synthetic.ipynb">nbviewer</a>, and the plain script output can be found <a href="../synthetic.jl">here</a>.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>If you want to run the experiments, make sure you have an identical environment. Please use Julia 1.5.3 and activate and instantiate the environment using <a href="https://github.com/devmotion/Calibration_ICLR2021/blob/main/experiments/src/synthetic/Project.toml">this Project.toml file</a> and <a href="https://github.com/devmotion/Calibration_ICLR2021/blob/main/experiments/src/synthetic/Manifest.toml">this Manifest.toml file</a>.</p><p><a href="https://github.com/devmotion/Calibration_ICLR2021/">The Github repository</a> contains <a href="https://github.com/devmotion/Calibration_ICLR2021/blob/main/experiments/README.md">more detailed instructions</a> and a <code>nix</code> project environment with a pinned Julia binary for improved reproducibility.</p></div></div><h2 id="Packages"><a class="docs-heading-anchor" href="#Packages">Packages</a><a id="Packages-1"></a><a class="docs-heading-anchor-permalink" href="#Packages" title="Permalink"></a></h2><pre><code class="language-julia">using CSV
using CairoMakie
using CalibrationErrors
using CalibrationErrorsDistributions
using CalibrationTests
using DataFrames
using Distributions
using FillArrays
using ProgressLogging
using Query
using Showoff
using StatsBase

using LinearAlgebra
using Printf
using Random

using CairoMakie.AbstractPlotting.ColorSchemes: Dark2_8

using Logging: with_logger
using TerminalLoggers: TerminalLogger

# set random seed
Random.seed!(1234)

# create path before saving
function wsavefig(file, fig=current_figure())
    mkpath(dirname(file))
    return save(file, fig)
end

# define progress logging frontend
const PROGRESSLOGGER = TerminalLogger()

# define non-intrusive plotting style
set_theme!(
    Theme(;
        Axis=(
            rightspinevisible=false,
            topspinevisible=false,
            xgridvisible=false,
            ygridvisible=false,
        ),
        Legend=(framevisible=false,),
    ),
)</code></pre><h2 id="Synthetic-models-2"><a class="docs-heading-anchor" href="#Synthetic-models-2">Synthetic models</a><a class="docs-heading-anchor-permalink" href="#Synthetic-models-2" title="Permalink"></a></h2><p>We study two setups with <span>$d$</span>-dimensional targets <span>$Y$</span> and normal distributions <span>$P_X$</span> of the form <span>$\mathcal{N}(c \mathbf{1}_d, 0.1^2 \mathbf{I}_d)$</span> as predictions, where <span>$c \sim \mathrm{U}(0, 1)$</span>. Since calibration analysis is only based on the targets and predicted distributions, we neglect features <span>$X$</span> in these experiments and specify only the distributions of <span>$Y$</span> and <span>$P_X$</span>.</p><h3 id="Calibrated-setup"><a class="docs-heading-anchor" href="#Calibrated-setup">Calibrated setup</a><a id="Calibrated-setup-1"></a><a class="docs-heading-anchor-permalink" href="#Calibrated-setup" title="Permalink"></a></h3><p>In the first setup we simulate a calibrated model. We achieve this by sampling targets from the predicted distributions, i.e., by defining the conditional distribution of <span>$Y$</span> given <span>$P_X$</span> as</p><p class="math-container">\[Y \,|\, P_X = \mathcal{N}(\mu, \Sigma) \sim \mathcal{N}(\mu, \Sigma).\]</p><pre><code class="language-julia">function calibrated_model(dim::Int, nsamples::Int)
    # sample predictions
    predictions = [MvNormal(Fill(rand(), dim), 0.1) for _ in 1:nsamples]

    # sample targets
    targets = map(rand, predictions)

    return predictions, targets
end</code></pre><pre class="documenter-example-output">calibrated_model (generic function with 1 method)</pre><h3 id="Uncalibrated-setup"><a class="docs-heading-anchor" href="#Uncalibrated-setup">Uncalibrated setup</a><a id="Uncalibrated-setup-1"></a><a class="docs-heading-anchor-permalink" href="#Uncalibrated-setup" title="Permalink"></a></h3><p>In the second setup we simulate an uncalibrated model of the form</p><p class="math-container">\[Y \,|\, P_X = \mathcal{N}(\mu, \Sigma) \sim \mathcal{N}([0.1, \mu_2, \ldots, \mu_d], \Sigma).\]</p><pre><code class="language-julia">function uncalibrated_model(dim::Int, nsamples::Int)
    # sample predictions
    predictions = [MvNormal(Fill(rand(), dim), 0.1) for _ in 1:nsamples]

    # sample targets
    targets = map(rand, predictions)
    altdist = Normal(0.1, 0.1)
    for t in targets
        t[1] = rand(altdist)
    end

    return predictions, targets
end</code></pre><pre class="documenter-example-output">uncalibrated_model (generic function with 1 method)</pre><h2 id="Convergence-and-computation-time-of-estimators"><a class="docs-heading-anchor" href="#Convergence-and-computation-time-of-estimators">Convergence and computation time of estimators</a><a id="Convergence-and-computation-time-of-estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Convergence-and-computation-time-of-estimators" title="Permalink"></a></h2><p>We perform an evaluation of the convergence and computation time of the biased estimator <span>$\widehat{\mathrm{SKCE}}_k$</span>, the unbiased estimator <span>$\widehat{\mathrm{SKCE}}_{k,B}$</span> with blocks of size <span>$B \in \{2, \sqrt{n}, n\}$</span>. We use the tensor product kernel</p><p class="math-container">\[\begin{aligned}
k\big((p, y), (p&#39;, y&#39;)\big) &amp;= \exp{\big(- W_2(p, p&#39;)\big)} \exp{\big(-(y - y&#39;)^2/2\big)} \\
&amp;= \exp{\big(-\sqrt{(m_p - m_{p&#39;})^2 + (\sigma_p - \sigma_{p&#39;})^2}\big)} \exp{\big( - (y - y&#39;)^2/2\big)},
\end{aligned}\]</p><p>where <span>$W_2$</span> is the 2-Wasserstein distance and <span>$m_p, m_{p&#39;}$</span> and <span>$\sigma_p, \sigma_{p&#39;}$</span> denote the mean and the standard deviation of the normal distributions <span>$p$</span> and <span>$p&#39;$</span>.</p><h3 id="Ground-truth"><a class="docs-heading-anchor" href="#Ground-truth">Ground truth</a><a id="Ground-truth-1"></a><a class="docs-heading-anchor-permalink" href="#Ground-truth" title="Permalink"></a></h3><p>For both models, we have to &quot;evaluate&quot; the true calibration error. Generally, the error depends on the model (and hence also dimension <span>$d$</span>) and the kernel. If the model is calibrated, we know that the calibration error is zero. For the uncalibrated model, we estimate the ground truth with the minimum-variance unbiased estimator as the mean of SKCE estimates for 1000 randomly sampled datasets with 1000 data points.</p><pre><code class="language-julia">true_SKCE(::typeof(calibrated_model), kernel; dim::Int) = 0.0
function true_SKCE(model::typeof(uncalibrated_model), kernel; dim::Int)
    estimator = UnbiasedSKCE(kernel)
    return mean(calibrationerror(estimator, model(dim, 1_000)...) for _ in 1:1_000)
end</code></pre><pre class="documenter-example-output">true_SKCE (generic function with 2 methods)</pre><h3 id="Benchmarking"><a class="docs-heading-anchor" href="#Benchmarking">Benchmarking</a><a id="Benchmarking-1"></a><a class="docs-heading-anchor-permalink" href="#Benchmarking" title="Permalink"></a></h3><p>The following two functions implement the benchmarking. We sample 500 datasets of 4, 16, 64, 256, and 1024 data points each for the models of dimensions <span>$d=1$</span> and <span>$d=10$</span>. For each of the datasets, we evaluate the different SKCE estimators. We compute the mean absolute error, the variance, and the minimum computation time for the estimates, grouped by the dimension of the model and the number of samples in the dataset.</p><pre><code class="language-julia">function benchmark_estimator(estimator, model; dim::Int, nsamples::Int, groundtruth)
    # compute the estimator (potentially depending on number of samples)
    _estimator = estimator(nsamples)

    # cache for calibration error estimates
    estimates = Vector{Float64}(undef, 500)

    mintime = Inf

    name = @sprintf(&quot;benchmarking (dim = %2d, nsamples = %4d)&quot;, dim, nsamples)
    @progress name = name for i in eachindex(estimates)
        # sample predictions and targets
        predictions, targets = model(dim, nsamples)

        # define benchmark function
        benchmark_f =
            let estimator = _estimator, predictions = predictions, targets = targets
                () -&gt; @timed calibrationerror(estimator, predictions, targets)
            end

        # precompile function
        benchmark_f()

        # compute calibration error and obtain elapsed time
        val, t = benchmark_f()

        # only keep minimum execution time
        mintime = min(mintime, t)

        # save error estimate
        estimates[i] = val
    end

    # save the mean absolute deviation and the variance of the estimates
    meanerror = mean(abs(x - groundtruth) for x in estimates)
    variance = var(estimates)

    return (; dim, nsamples, meanerror, variance, mintime)
end

function benchmark_estimators(model)
    # output file
    filename = joinpath(&quot;data&quot;, &quot;synthetic&quot;, &quot;errors_$(model).csv&quot;)

    # check if results exist
    isfile(filename) &amp;&amp; return DataFrame(CSV.File(filename))

    # define kernel
    kernel = WassersteinExponentialKernel() ⊗ SqExponentialKernel()

    # define estimators
    estimators = (
        &quot;SKCE&quot; =&gt; _ -&gt; BiasedSKCE(kernel),
        &quot;SKCE (B = 2)&quot; =&gt; _ -&gt; BlockUnbiasedSKCE(kernel, 2),
        &quot;SKCE (B = √n)&quot; =&gt; n -&gt; BlockUnbiasedSKCE(kernel, max(2, Int(floor(sqrt(n))))),
        &quot;SKCE (B = n)&quot; =&gt; _ -&gt; UnbiasedSKCE(kernel),
    )

    # define number of samples
    nsamples = 2 .^ (2:2:10)

    # ensure that output directory exists and open file for writing
    mkpath(dirname(filename))
    open(filename, &quot;w&quot;) do file
        # write headers
        println(file, &quot;estimator,dim,nsamples,meanerror,variance,mintime&quot;)

        # for dimensions ``d=1`` and ``d=10``
        for d in (1, 10)
            # compute/estimate ground truth
            groundtruth = true_SKCE(model, kernel; dim=d)

            for (i, (name, estimator)) in enumerate(estimators)
                # benchmark estimator
                @info &quot;benchmarking estimator: $(name)&quot;

                for n in nsamples
                    stats = benchmark_estimator(
                        estimator, model; dim=d, nsamples=n, groundtruth=groundtruth
                    )

                    # save statistics
                    print(file, name, &quot;,&quot;)
                    join(file, stats, &quot;,&quot;)
                    println(file)
                end
            end
        end
    end

    # load results
    return DataFrame(CSV.File(filename))
end</code></pre><pre class="documenter-example-output">benchmark_estimators (generic function with 1 method)</pre><p>We benchmark the estimators with the calibrated model.</p><pre><code class="language-julia">Random.seed!(100)
with_logger(PROGRESSLOGGER) do
    benchmark_estimators(calibrated_model)
end</code></pre><table class="data-frame"><thead><tr><th></th><th>estimator</th><th>dim</th><th>nsamples</th><th>meanerror</th><th>variance</th><th>mintime</th></tr><tr><th></th><th>String</th><th>Int64</th><th>Int64</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>40 rows × 6 columns</p><tr><th>1</th><td>SKCE</td><td>1</td><td>4</td><td>0.00232646</td><td>5.40611e-6</td><td>8.39e-7</td></tr><tr><th>2</th><td>SKCE</td><td>1</td><td>16</td><td>0.000612749</td><td>3.25968e-7</td><td>1.1546e-5</td></tr><tr><th>3</th><td>SKCE</td><td>1</td><td>64</td><td>0.000160806</td><td>2.0837e-8</td><td>0.000182119</td></tr><tr><th>4</th><td>SKCE</td><td>1</td><td>256</td><td>3.87249e-5</td><td>1.37054e-9</td><td>0.00293836</td></tr><tr><th>5</th><td>SKCE</td><td>1</td><td>1024</td><td>9.62075e-6</td><td>7.10919e-11</td><td>0.0473988</td></tr><tr><th>6</th><td>SKCE (B = 2)</td><td>1</td><td>4</td><td>0.00322484</td><td>2.14091e-5</td><td>2.48e-7</td></tr><tr><th>7</th><td>SKCE (B = 2)</td><td>1</td><td>16</td><td>0.00178976</td><td>5.51153e-6</td><td>1.318e-6</td></tr><tr><th>8</th><td>SKCE (B = 2)</td><td>1</td><td>64</td><td>0.000823384</td><td>1.11883e-6</td><td>3.404e-6</td></tr><tr><th>9</th><td>SKCE (B = 2)</td><td>1</td><td>256</td><td>0.000480282</td><td>3.55902e-7</td><td>1.3808e-5</td></tr><tr><th>10</th><td>SKCE (B = 2)</td><td>1</td><td>1024</td><td>0.000235386</td><td>8.60559e-8</td><td>5.5695e-5</td></tr><tr><th>11</th><td>SKCE (B = √n)</td><td>1</td><td>4</td><td>0.00329201</td><td>2.34735e-5</td><td>2.34e-7</td></tr><tr><th>12</th><td>SKCE (B = √n)</td><td>1</td><td>16</td><td>0.00105996</td><td>1.96389e-6</td><td>2.186e-6</td></tr><tr><th>13</th><td>SKCE (B = √n)</td><td>1</td><td>64</td><td>0.000342314</td><td>1.86192e-7</td><td>2.014e-5</td></tr><tr><th>14</th><td>SKCE (B = √n)</td><td>1</td><td>256</td><td>0.000121204</td><td>2.31226e-8</td><td>0.000173367</td></tr><tr><th>15</th><td>SKCE (B = √n)</td><td>1</td><td>1024</td><td>4.00685e-5</td><td>2.66636e-9</td><td>0.00143577</td></tr><tr><th>16</th><td>SKCE (B = n)</td><td>1</td><td>4</td><td>0.00183983</td><td>8.07962e-6</td><td>5.37e-7</td></tr><tr><th>17</th><td>SKCE (B = n)</td><td>1</td><td>16</td><td>0.000417513</td><td>3.18398e-7</td><td>1.0872e-5</td></tr><tr><th>18</th><td>SKCE (B = n)</td><td>1</td><td>64</td><td>9.96033e-5</td><td>1.92571e-8</td><td>0.000178632</td></tr><tr><th>19</th><td>SKCE (B = n)</td><td>1</td><td>256</td><td>2.48102e-5</td><td>1.25886e-9</td><td>0.00291616</td></tr><tr><th>20</th><td>SKCE (B = n)</td><td>1</td><td>1024</td><td>6.07345e-6</td><td>7.73514e-11</td><td>0.0471831</td></tr><tr><th>21</th><td>SKCE</td><td>10</td><td>4</td><td>0.0236122</td><td>3.94317e-5</td><td>1.104e-6</td></tr><tr><th>22</th><td>SKCE</td><td>10</td><td>16</td><td>0.00590577</td><td>1.67794e-6</td><td>1.6572e-5</td></tr><tr><th>23</th><td>SKCE</td><td>10</td><td>64</td><td>0.00147388</td><td>9.22523e-8</td><td>0.000266736</td></tr><tr><th>24</th><td>SKCE</td><td>10</td><td>256</td><td>0.000368578</td><td>5.66692e-9</td><td>0.00428609</td></tr><tr><th>25</th><td>SKCE</td><td>10</td><td>1024</td><td>9.10077e-5</td><td>3.19037e-10</td><td>0.0685317</td></tr><tr><th>26</th><td>SKCE (B = 2)</td><td>10</td><td>4</td><td>0.00615017</td><td>8.17789e-5</td><td>2.96e-7</td></tr><tr><th>27</th><td>SKCE (B = 2)</td><td>10</td><td>16</td><td>0.00327516</td><td>1.83537e-5</td><td>1.162e-6</td></tr><tr><th>28</th><td>SKCE (B = 2)</td><td>10</td><td>64</td><td>0.00181648</td><td>5.34779e-6</td><td>4.901e-6</td></tr><tr><th>29</th><td>SKCE (B = 2)</td><td>10</td><td>256</td><td>0.000962758</td><td>1.48284e-6</td><td>2.0093e-5</td></tr><tr><th>30</th><td>SKCE (B = 2)</td><td>10</td><td>1024</td><td>0.000462092</td><td>3.33079e-7</td><td>8.167e-5</td></tr><tr><th>31</th><td>SKCE (B = √n)</td><td>10</td><td>4</td><td>0.00593596</td><td>8.45258e-5</td><td>3.07e-7</td></tr><tr><th>32</th><td>SKCE (B = √n)</td><td>10</td><td>16</td><td>0.00216761</td><td>7.10482e-6</td><td>3.296e-6</td></tr><tr><th>33</th><td>SKCE (B = √n)</td><td>10</td><td>64</td><td>0.000666048</td><td>7.00752e-7</td><td>3.0625e-5</td></tr><tr><th>34</th><td>SKCE (B = √n)</td><td>10</td><td>256</td><td>0.000246347</td><td>9.09186e-8</td><td>0.000265018</td></tr><tr><th>35</th><td>SKCE (B = √n)</td><td>10</td><td>1024</td><td>8.47757e-5</td><td>1.10709e-8</td><td>0.00208533</td></tr><tr><th>36</th><td>SKCE (B = n)</td><td>10</td><td>4</td><td>0.00410328</td><td>2.93761e-5</td><td>7.2e-7</td></tr><tr><th>37</th><td>SKCE (B = n)</td><td>10</td><td>16</td><td>0.000955665</td><td>1.41091e-6</td><td>1.5352e-5</td></tr><tr><th>38</th><td>SKCE (B = n)</td><td>10</td><td>64</td><td>0.000231735</td><td>8.77091e-8</td><td>0.000260414</td></tr><tr><th>39</th><td>SKCE (B = n)</td><td>10</td><td>256</td><td>5.8408e-5</td><td>5.31428e-9</td><td>0.00424718</td></tr><tr><th>40</th><td>SKCE (B = n)</td><td>10</td><td>1024</td><td>1.4255e-5</td><td>3.31928e-10</td><td>0.0682267</td></tr></tbody></table><p>We repeat the benchmark with the uncalibrated model.</p><pre><code class="language-julia">Random.seed!(100)
with_logger(PROGRESSLOGGER) do
    benchmark_estimators(uncalibrated_model)
end</code></pre><table class="data-frame"><thead><tr><th></th><th>estimator</th><th>dim</th><th>nsamples</th><th>meanerror</th><th>variance</th><th>mintime</th></tr><tr><th></th><th>String</th><th>Int64</th><th>Int64</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>40 rows × 6 columns</p><tr><th>1</th><td>SKCE</td><td>1</td><td>4</td><td>0.0751342</td><td>0.00996145</td><td>8.4e-7</td></tr><tr><th>2</th><td>SKCE</td><td>1</td><td>16</td><td>0.0314564</td><td>0.00169231</td><td>1.1676e-5</td></tr><tr><th>3</th><td>SKCE</td><td>1</td><td>64</td><td>0.0159183</td><td>0.000394578</td><td>0.000184502</td></tr><tr><th>4</th><td>SKCE</td><td>1</td><td>256</td><td>0.00802689</td><td>0.000102252</td><td>0.00295501</td></tr><tr><th>5</th><td>SKCE</td><td>1</td><td>1024</td><td>0.00423135</td><td>2.77658e-5</td><td>0.047851</td></tr><tr><th>6</th><td>SKCE (B = 2)</td><td>1</td><td>4</td><td>0.078777</td><td>0.00982905</td><td>3.54e-7</td></tr><tr><th>7</th><td>SKCE (B = 2)</td><td>1</td><td>16</td><td>0.0406132</td><td>0.00246662</td><td>8.83e-7</td></tr><tr><th>8</th><td>SKCE (B = 2)</td><td>1</td><td>64</td><td>0.0194201</td><td>0.000600167</td><td>3.699e-6</td></tr><tr><th>9</th><td>SKCE (B = 2)</td><td>1</td><td>256</td><td>0.00987522</td><td>0.00015909</td><td>1.4895e-5</td></tr><tr><th>10</th><td>SKCE (B = 2)</td><td>1</td><td>1024</td><td>0.00511378</td><td>4.13817e-5</td><td>6.0314e-5</td></tr><tr><th>11</th><td>SKCE (B = √n)</td><td>1</td><td>4</td><td>0.0811177</td><td>0.0102605</td><td>2.35e-7</td></tr><tr><th>12</th><td>SKCE (B = √n)</td><td>1</td><td>16</td><td>0.0342851</td><td>0.00194094</td><td>2.25e-6</td></tr><tr><th>13</th><td>SKCE (B = √n)</td><td>1</td><td>64</td><td>0.0174102</td><td>0.000456507</td><td>2.0407e-5</td></tr><tr><th>14</th><td>SKCE (B = √n)</td><td>1</td><td>256</td><td>0.00834639</td><td>0.000107721</td><td>0.000175991</td></tr><tr><th>15</th><td>SKCE (B = √n)</td><td>1</td><td>1024</td><td>0.00414902</td><td>2.71046e-5</td><td>0.00146066</td></tr><tr><th>16</th><td>SKCE (B = n)</td><td>1</td><td>4</td><td>0.0714118</td><td>0.00821249</td><td>5.37e-7</td></tr><tr><th>17</th><td>SKCE (B = n)</td><td>1</td><td>16</td><td>0.0342723</td><td>0.00187879</td><td>1.0352e-5</td></tr><tr><th>18</th><td>SKCE (B = n)</td><td>1</td><td>64</td><td>0.0169261</td><td>0.000437577</td><td>0.000178923</td></tr><tr><th>19</th><td>SKCE (B = n)</td><td>1</td><td>256</td><td>0.00838566</td><td>0.000106268</td><td>0.00295301</td></tr><tr><th>20</th><td>SKCE (B = n)</td><td>1</td><td>1024</td><td>0.00426478</td><td>2.90507e-5</td><td>0.047773</td></tr><tr><th>21</th><td>SKCE</td><td>10</td><td>4</td><td>0.0605164</td><td>0.00428938</td><td>1.113e-6</td></tr><tr><th>22</th><td>SKCE</td><td>10</td><td>16</td><td>0.0234032</td><td>0.000668417</td><td>1.6879e-5</td></tr><tr><th>23</th><td>SKCE</td><td>10</td><td>64</td><td>0.0103131</td><td>0.000154495</td><td>0.000271091</td></tr><tr><th>24</th><td>SKCE</td><td>10</td><td>256</td><td>0.00498841</td><td>3.99274e-5</td><td>0.00435522</td></tr><tr><th>25</th><td>SKCE</td><td>10</td><td>1024</td><td>0.00235985</td><td>9.21217e-6</td><td>0.0694433</td></tr><tr><th>26</th><td>SKCE (B = 2)</td><td>10</td><td>4</td><td>0.0544382</td><td>0.00497703</td><td>2.99e-7</td></tr><tr><th>27</th><td>SKCE (B = 2)</td><td>10</td><td>16</td><td>0.0283976</td><td>0.00121663</td><td>1.177e-6</td></tr><tr><th>28</th><td>SKCE (B = 2)</td><td>10</td><td>64</td><td>0.0133977</td><td>0.000290239</td><td>4.817e-6</td></tr><tr><th>29</th><td>SKCE (B = 2)</td><td>10</td><td>256</td><td>0.00687261</td><td>7.63347e-5</td><td>1.9724e-5</td></tr><tr><th>30</th><td>SKCE (B = 2)</td><td>10</td><td>1024</td><td>0.00359383</td><td>1.99858e-5</td><td>7.9659e-5</td></tr><tr><th>31</th><td>SKCE (B = √n)</td><td>10</td><td>4</td><td>0.0567095</td><td>0.00549465</td><td>2.99e-7</td></tr><tr><th>32</th><td>SKCE (B = √n)</td><td>10</td><td>16</td><td>0.0225549</td><td>0.000846545</td><td>3.13e-6</td></tr><tr><th>33</th><td>SKCE (B = √n)</td><td>10</td><td>64</td><td>0.0102516</td><td>0.000172733</td><td>2.9899e-5</td></tr><tr><th>34</th><td>SKCE (B = √n)</td><td>10</td><td>256</td><td>0.00560227</td><td>4.84788e-5</td><td>0.000258742</td></tr><tr><th>35</th><td>SKCE (B = √n)</td><td>10</td><td>1024</td><td>0.00230399</td><td>8.42511e-6</td><td>0.00214255</td></tr><tr><th>36</th><td>SKCE (B = n)</td><td>10</td><td>4</td><td>0.0434448</td><td>0.00335596</td><td>7.28e-7</td></tr><tr><th>37</th><td>SKCE (B = n)</td><td>10</td><td>16</td><td>0.0188946</td><td>0.000584298</td><td>1.5216e-5</td></tr><tr><th>38</th><td>SKCE (B = n)</td><td>10</td><td>64</td><td>0.00994934</td><td>0.000150885</td><td>0.000263802</td></tr><tr><th>39</th><td>SKCE (B = n)</td><td>10</td><td>256</td><td>0.0053346</td><td>4.57406e-5</td><td>0.00430886</td></tr><tr><th>40</th><td>SKCE (B = n)</td><td>10</td><td>1024</td><td>0.00253457</td><td>9.91621e-6</td><td>0.0692274</td></tr></tbody></table><h3 id="Visualization"><a class="docs-heading-anchor" href="#Visualization">Visualization</a><a id="Visualization-1"></a><a class="docs-heading-anchor-permalink" href="#Visualization" title="Permalink"></a></h3><p>We show a visualization of the results below.</p><pre><code class="language-julia">function logtickformat(base::Int)
    function format(values)
        return map(Base.Fix2(logformat, base), showoff(values))
    end
    return format
end

function logformat(digits::String, base::Int)
    buf = IOBuffer()
    print(buf, base)
    for c in digits
        if &#39;0&#39; ≤ c ≤ &#39;9&#39;
            print(buf, Showoff.superscript_numerals[c - &#39;0&#39; + 1])
        elseif c == &#39;-&#39;
            print(buf, &#39;⁻&#39;)
        elseif c == &#39;.&#39;
            print(buf, &#39;·&#39;)
        end
    end
    return String(take!(buf))
end

function plot_benchmark_estimators(model; dim::Int)
    # load and preprocess data
    filename = joinpath(&quot;data&quot;, &quot;synthetic&quot;, &quot;errors_$(model).csv&quot;)
    groups = @from i in DataFrame(CSV.File(filename)) begin
        @where i.dim == dim
        @orderby i.nsamples
        @select {
            i.estimator,
            log2_nsamples = log2(i.nsamples),
            log10_meanerror = log10(i.meanerror),
            log10_variance = log10(i.variance),
            log10_mintime = log10(i.mintime),
        }
        @collect DataFrame
    end

    # create figure
    fig = Figure(; resolution=(960, 800))

    # create axes to plot mean error and variance vs number of samples
    ax1 = Axis(
        fig[1, 1];
        xlabel=&quot;# samples&quot;,
        ylabel=&quot;mean error&quot;,
        xticks=2:2:10,
        xtickformat=logtickformat(2),
        ytickformat=logtickformat(10),
    )
    ax2 = Axis(
        fig[2, 1];
        xlabel=&quot;# samples&quot;,
        ylabel=&quot;variance&quot;,
        xticks=2:2:10,
        xtickformat=logtickformat(2),
        ytickformat=logtickformat(10),
    )

    # create axes to plot mean error and variance vs timings
    ax3 = Axis(
        fig[1, 2];
        xlabel=&quot;time [s]&quot;,
        ylabel=&quot;mean error&quot;,
        xtickformat=logtickformat(10),
        ytickformat=logtickformat(10),
    )
    ax4 = Axis(
        fig[2, 2];
        xlabel=&quot;time [s]&quot;,
        ylabel=&quot;variance&quot;,
        xtickformat=logtickformat(10),
        ytickformat=logtickformat(10),
    )

    # plot benchmark results
    estimators = [&quot;SKCE&quot;, &quot;SKCE (B = 2)&quot;, &quot;SKCE (B = √n)&quot;, &quot;SKCE (B = n)&quot;]
    markers = [&#39;●&#39;, &#39;■&#39;, &#39;▲&#39;, &#39;◆&#39;]
    for (i, (estimator, marker)) in enumerate(zip(estimators, markers))
        group = filter(:estimator =&gt; ==(estimator), groups)
        color = Dark2_8[i]

        # plot mean error vs samples
        scatterlines!(
            ax1,
            group.log2_nsamples,
            group.log10_meanerror;
            color=color,
            linewidth=2,
            marker=marker,
            markercolor=color,
        )

        # plot variance vs samples
        scatterlines!(
            ax2,
            group.log2_nsamples,
            group.log10_variance;
            color=color,
            linewidth=2,
            marker=marker,
            markercolor=color,
        )

        # plot mean error vs time
        scatterlines!(
            ax3,
            group.log10_mintime,
            group.log10_meanerror;
            color=color,
            linewidth=2,
            marker=marker,
            markercolor=color,
        )

        # plot variance vs time
        scatterlines!(
            ax4,
            group.log10_mintime,
            group.log10_variance;
            color=color,
            linewidth=2,
            marker=marker,
            markercolor=color,
        )
    end

    # link axes and hide decorations
    linkxaxes!(ax1, ax2)
    hidexdecorations!(ax1)
    linkxaxes!(ax3, ax4)
    hidexdecorations!(ax3)
    linkyaxes!(ax1, ax3)
    hideydecorations!(ax3)
    linkyaxes!(ax2, ax4)
    hideydecorations!(ax4)

    # add legend
    elems = map(1:length(estimators)) do i
        [
            LineElement(; color=Dark2_8[i], linestyle=nothing, linewidth=2),
            MarkerElement(; color=Dark2_8[i], marker=markers[i], strokecolor=:black),
        ]
    end
    Legend(fig[end + 1, :], elems, estimators; orientation=:horizontal, tellheight=true)

    return fig
end</code></pre><pre class="documenter-example-output">plot_benchmark_estimators (generic function with 1 method)</pre><p>We obtain the following plots:</p><pre><code class="language-julia">plot_benchmark_estimators(calibrated_model; dim=1)
wsavefig(&quot;figures/synthetic/estimators_calibrated_model_dim=1.svg&quot;);</code></pre><p><img src="../figures/synthetic/estimators_calibrated_model_dim=1.svg" alt/></p><pre><code class="language-julia">plot_benchmark_estimators(calibrated_model; dim=10)
wsavefig(&quot;figures/synthetic/estimators_calibrated_model_dim=10.svg&quot;);</code></pre><p><img src="../figures/synthetic/estimators_calibrated_model_dim=10.svg" alt/></p><pre><code class="language-julia">plot_benchmark_estimators(uncalibrated_model; dim=1)
wsavefig(&quot;figures/synthetic/estimators_uncalibrated_model_dim=1.svg&quot;);</code></pre><p><img src="../figures/synthetic/estimators_uncalibrated_model_dim=1.svg" alt/></p><pre><code class="language-julia">plot_benchmark_estimators(uncalibrated_model; dim=10)
wsavefig(&quot;figures/synthetic/estimators_uncalibrated_model_dim=10.svg&quot;);</code></pre><p><img src="../figures/synthetic/estimators_uncalibrated_model_dim=10.svg" alt/></p><h2 id="Test-errors-and-computation-time-of-calibration-tests"><a class="docs-heading-anchor" href="#Test-errors-and-computation-time-of-calibration-tests">Test errors and computation time of calibration tests</a><a id="Test-errors-and-computation-time-of-calibration-tests-1"></a><a class="docs-heading-anchor-permalink" href="#Test-errors-and-computation-time-of-calibration-tests" title="Permalink"></a></h2><p>We fix the significance level <span>$\alpha = 0.05$</span>. Test predictions are sampled from the same distribution as <span>$P_X$</span>, and test targets are sampled independently from <span>$\mathcal{N}(0, 0.1^2 \mathbf{I}_d)$</span>.</p><h3 id="Benchmarking-2"><a class="docs-heading-anchor" href="#Benchmarking-2">Benchmarking</a><a class="docs-heading-anchor-permalink" href="#Benchmarking-2" title="Permalink"></a></h3><pre><code class="language-julia">iscalibrated(::typeof(calibrated_model)) = true
iscalibrated(::typeof(uncalibrated_model)) = false

function benchmark_test(test, model; dim::Int, nsamples::Int)
    # number of simulations
    nrepeat = 500

    # initial values
    ntesterrors = 0
    mintime = Inf

    name = @sprintf(&quot;benchmarking (dim = %2d, nsamples = %4d)&quot;, dim, nsamples)
    @progress name = name for _ in 1:nrepeat
        # sample predictions and targets
        predictions, targets = model(dim, nsamples)

        # define benchmark function
        benchmark_f = let test = test, predictions = predictions, targets = targets
            () -&gt; @timed pvalue(test(predictions, targets))
        end

        # precompile function
        benchmark_f()

        # compute calibration error and obtain elapsed time
        val, t = benchmark_f()

        # only keep minimum execution time
        mintime = min(mintime, t)

        # update number of empirical test errors for
        # significance level ``\alpha = 0.05``
        ntesterrors += iscalibrated(model) ⊻ (val ≥ 0.05)
    end

    # compute empirical test error rate
    testerror = ntesterrors / nrepeat

    return (; dim, nsamples, testerror, mintime)
end

function benchmark_tests(model)
    # output file
    filename = joinpath(&quot;data&quot;, &quot;synthetic&quot;, &quot;tests_$(model).csv&quot;)

    # check if results exist
    isfile(filename) &amp;&amp; return DataFrame(CSV.File(filename))

    # define kernel
    kernel = WassersteinExponentialKernel() ⊗ SqExponentialKernel()

    # define number of samples
    nsamples = 2 .^ (2:2:10)

    # ensure that output directory exists and open file for writing
    mkpath(dirname(filename))
    open(filename, &quot;w&quot;) do file
        # write headers
        println(file, &quot;test,dim,nsamples,testerror,mintime&quot;)

        # for dimensions ``d=1`` and ``d=10``
        for d in (1, 10)
            # define tests
            testpredictions = [MvNormal(rand(d), 0.1) for _ in 1:10]
            testtargets = [rand(MvNormal(d, 0.1)) for _ in 1:10]
            tests = (
                &quot;SKCE (B = 2)&quot; =&gt;
                    (predictions, targets) -&gt; AsymptoticBlockSKCETest(
                        BlockUnbiasedSKCE(kernel, 2), predictions, targets
                    ),
                &quot;SKCE (B = √n)&quot; =&gt;
                    (predictions, targets) -&gt; AsymptoticBlockSKCETest(
                        BlockUnbiasedSKCE(kernel, Int(floor(sqrt(length(predictions))))),
                        predictions,
                        targets,
                    ),
                &quot;SKCE (B = n)&quot; =&gt;
                    (predictions, targets) -&gt;
                        AsymptoticSKCETest(kernel, predictions, targets),
                &quot;CME&quot; =&gt;
                    (predictions, targets) -&gt; AsymptoticCMETest(
                        UCME(kernel, testpredictions, testtargets), predictions, targets
                    ),
            )

            for (i, (name, test)) in enumerate(tests)
                # benchmark estimator
                @info &quot;benchmarking test: $(name)&quot;

                for n in nsamples
                    stats = benchmark_test(test, model; dim=d, nsamples=n)

                    # save statistics
                    print(file, name, &quot;,&quot;)
                    join(file, stats, &quot;,&quot;)
                    println(file)
                end
            end
        end
    end

    # load results
    return DataFrame(CSV.File(filename))
end</code></pre><pre class="documenter-example-output">benchmark_tests (generic function with 1 method)</pre><p>First we benchmark the calibrated model.</p><pre><code class="language-julia">Random.seed!(100)
with_logger(PROGRESSLOGGER) do
    benchmark_tests(calibrated_model)
end</code></pre><table class="data-frame"><thead><tr><th></th><th>test</th><th>dim</th><th>nsamples</th><th>testerror</th><th>mintime</th></tr><tr><th></th><th>String</th><th>Int64</th><th>Int64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>40 rows × 5 columns</p><tr><th>1</th><td>SKCE (B = 2)</td><td>1</td><td>4</td><td>0.128</td><td>1.296e-6</td></tr><tr><th>2</th><td>SKCE (B = 2)</td><td>1</td><td>16</td><td>0.05</td><td>4.202e-6</td></tr><tr><th>3</th><td>SKCE (B = 2)</td><td>1</td><td>64</td><td>0.054</td><td>1.7021e-5</td></tr><tr><th>4</th><td>SKCE (B = 2)</td><td>1</td><td>256</td><td>0.054</td><td>6.4322e-5</td></tr><tr><th>5</th><td>SKCE (B = 2)</td><td>1</td><td>1024</td><td>0.06</td><td>0.000133484</td></tr><tr><th>6</th><td>SKCE (B = √n)</td><td>1</td><td>4</td><td>0.134</td><td>1.304e-6</td></tr><tr><th>7</th><td>SKCE (B = √n)</td><td>1</td><td>16</td><td>0.038</td><td>5.944e-6</td></tr><tr><th>8</th><td>SKCE (B = √n)</td><td>1</td><td>64</td><td>0.03</td><td>4.2991e-5</td></tr><tr><th>9</th><td>SKCE (B = √n)</td><td>1</td><td>256</td><td>0.02</td><td>0.000180911</td></tr><tr><th>10</th><td>SKCE (B = √n)</td><td>1</td><td>1024</td><td>0.02</td><td>0.00145807</td></tr><tr><th>11</th><td>SKCE (B = n)</td><td>1</td><td>4</td><td>0.142</td><td>8.6026e-5</td></tr><tr><th>12</th><td>SKCE (B = n)</td><td>1</td><td>16</td><td>0.058</td><td>0.00045643</td></tr><tr><th>13</th><td>SKCE (B = n)</td><td>1</td><td>64</td><td>0.04</td><td>0.00403782</td></tr><tr><th>14</th><td>SKCE (B = n)</td><td>1</td><td>256</td><td>0.06</td><td>0.0504854</td></tr><tr><th>15</th><td>SKCE (B = n)</td><td>1</td><td>1024</td><td>0.056</td><td>1.04588</td></tr><tr><th>16</th><td>CME</td><td>1</td><td>4</td><td>0.53</td><td>1.4472e-5</td></tr><tr><th>17</th><td>CME</td><td>1</td><td>16</td><td>0.882</td><td>2.8276e-5</td></tr><tr><th>18</th><td>CME</td><td>1</td><td>64</td><td>0.356</td><td>6.2975e-5</td></tr><tr><th>19</th><td>CME</td><td>1</td><td>256</td><td>0.118</td><td>0.000190426</td></tr><tr><th>20</th><td>CME</td><td>1</td><td>1024</td><td>0.068</td><td>0.000668129</td></tr><tr><th>21</th><td>SKCE (B = 2)</td><td>10</td><td>4</td><td>0.102</td><td>8.0e-7</td></tr><tr><th>22</th><td>SKCE (B = 2)</td><td>10</td><td>16</td><td>0.052</td><td>2.559e-6</td></tr><tr><th>23</th><td>SKCE (B = 2)</td><td>10</td><td>64</td><td>0.064</td><td>1.002e-5</td></tr><tr><th>24</th><td>SKCE (B = 2)</td><td>10</td><td>256</td><td>0.048</td><td>4.0336e-5</td></tr><tr><th>25</th><td>SKCE (B = 2)</td><td>10</td><td>1024</td><td>0.048</td><td>0.000163456</td></tr><tr><th>26</th><td>SKCE (B = √n)</td><td>10</td><td>4</td><td>0.108</td><td>9.71e-7</td></tr><tr><th>27</th><td>SKCE (B = √n)</td><td>10</td><td>16</td><td>0.066</td><td>4.368e-6</td></tr><tr><th>28</th><td>SKCE (B = √n)</td><td>10</td><td>64</td><td>0.062</td><td>3.2725e-5</td></tr><tr><th>29</th><td>SKCE (B = √n)</td><td>10</td><td>256</td><td>0.05</td><td>0.000274685</td></tr><tr><th>30</th><td>SKCE (B = √n)</td><td>10</td><td>1024</td><td>0.042</td><td>0.00224606</td></tr><tr><th>31</th><td>SKCE (B = n)</td><td>10</td><td>4</td><td>0.004</td><td>8.8992e-5</td></tr><tr><th>32</th><td>SKCE (B = n)</td><td>10</td><td>16</td><td>0.0</td><td>0.000483322</td></tr><tr><th>33</th><td>SKCE (B = n)</td><td>10</td><td>64</td><td>0.01</td><td>0.00431849</td></tr><tr><th>34</th><td>SKCE (B = n)</td><td>10</td><td>256</td><td>0.042</td><td>0.0513581</td></tr><tr><th>35</th><td>SKCE (B = n)</td><td>10</td><td>1024</td><td>0.044</td><td>0.796199</td></tr><tr><th>36</th><td>CME</td><td>10</td><td>4</td><td>0.478</td><td>2.4278e-5</td></tr><tr><th>37</th><td>CME</td><td>10</td><td>16</td><td>0.76</td><td>4.922e-5</td></tr><tr><th>38</th><td>CME</td><td>10</td><td>64</td><td>0.152</td><td>7.793e-5</td></tr><tr><th>39</th><td>CME</td><td>10</td><td>256</td><td>0.058</td><td>0.000260368</td></tr><tr><th>40</th><td>CME</td><td>10</td><td>1024</td><td>0.074</td><td>0.000988811</td></tr></tbody></table><p>We repeat the analysis with the uncalibrated model.</p><pre><code class="language-julia">Random.seed!(100)
with_logger(PROGRESSLOGGER) do
    benchmark_tests(uncalibrated_model)
end</code></pre><table class="data-frame"><thead><tr><th></th><th>test</th><th>dim</th><th>nsamples</th><th>testerror</th><th>mintime</th></tr><tr><th></th><th>String</th><th>Int64</th><th>Int64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>40 rows × 5 columns</p><tr><th>1</th><td>SKCE (B = 2)</td><td>1</td><td>4</td><td>0.648</td><td>7.8e-7</td></tr><tr><th>2</th><td>SKCE (B = 2)</td><td>1</td><td>16</td><td>0.122</td><td>4.424e-6</td></tr><tr><th>3</th><td>SKCE (B = 2)</td><td>1</td><td>64</td><td>0.0</td><td>1.714e-5</td></tr><tr><th>4</th><td>SKCE (B = 2)</td><td>1</td><td>256</td><td>0.0</td><td>6.9183e-5</td></tr><tr><th>5</th><td>SKCE (B = 2)</td><td>1</td><td>1024</td><td>0.0</td><td>0.000138376</td></tr><tr><th>6</th><td>SKCE (B = √n)</td><td>1</td><td>4</td><td>0.654</td><td>1.364e-6</td></tr><tr><th>7</th><td>SKCE (B = √n)</td><td>1</td><td>16</td><td>0.046</td><td>3.122e-6</td></tr><tr><th>8</th><td>SKCE (B = √n)</td><td>1</td><td>64</td><td>0.0</td><td>2.3081e-5</td></tr><tr><th>9</th><td>SKCE (B = √n)</td><td>1</td><td>256</td><td>0.0</td><td>0.000190658</td></tr><tr><th>10</th><td>SKCE (B = √n)</td><td>1</td><td>1024</td><td>0.0</td><td>0.00156119</td></tr><tr><th>11</th><td>SKCE (B = n)</td><td>1</td><td>4</td><td>0.272</td><td>8.8509e-5</td></tr><tr><th>12</th><td>SKCE (B = n)</td><td>1</td><td>16</td><td>0.0</td><td>0.000477038</td></tr><tr><th>13</th><td>SKCE (B = n)</td><td>1</td><td>64</td><td>0.0</td><td>0.00401854</td></tr><tr><th>14</th><td>SKCE (B = n)</td><td>1</td><td>256</td><td>0.0</td><td>0.0500722</td></tr><tr><th>15</th><td>SKCE (B = n)</td><td>1</td><td>1024</td><td>0.0</td><td>1.10676</td></tr><tr><th>16</th><td>CME</td><td>1</td><td>4</td><td>0.478</td><td>1.5267e-5</td></tr><tr><th>17</th><td>CME</td><td>1</td><td>16</td><td>0.0</td><td>2.5804e-5</td></tr><tr><th>18</th><td>CME</td><td>1</td><td>64</td><td>0.0</td><td>5.7052e-5</td></tr><tr><th>19</th><td>CME</td><td>1</td><td>256</td><td>0.0</td><td>0.000181037</td></tr><tr><th>20</th><td>CME</td><td>1</td><td>1024</td><td>0.0</td><td>0.000668172</td></tr><tr><th>21</th><td>SKCE (B = 2)</td><td>10</td><td>4</td><td>0.748</td><td>7.82e-7</td></tr><tr><th>22</th><td>SKCE (B = 2)</td><td>10</td><td>16</td><td>0.308</td><td>2.543e-6</td></tr><tr><th>23</th><td>SKCE (B = 2)</td><td>10</td><td>64</td><td>0.0</td><td>9.69e-6</td></tr><tr><th>24</th><td>SKCE (B = 2)</td><td>10</td><td>256</td><td>0.0</td><td>7.4793e-5</td></tr><tr><th>25</th><td>SKCE (B = 2)</td><td>10</td><td>1024</td><td>0.0</td><td>0.000164</td></tr><tr><th>26</th><td>SKCE (B = √n)</td><td>10</td><td>4</td><td>0.734</td><td>9.5e-7</td></tr><tr><th>27</th><td>SKCE (B = √n)</td><td>10</td><td>16</td><td>0.148</td><td>4.326e-6</td></tr><tr><th>28</th><td>SKCE (B = √n)</td><td>10</td><td>64</td><td>0.0</td><td>5.529e-5</td></tr><tr><th>29</th><td>SKCE (B = √n)</td><td>10</td><td>256</td><td>0.0</td><td>0.000460137</td></tr><tr><th>30</th><td>SKCE (B = √n)</td><td>10</td><td>1024</td><td>0.0</td><td>0.00226211</td></tr><tr><th>31</th><td>SKCE (B = n)</td><td>10</td><td>4</td><td>0.712</td><td>8.8602e-5</td></tr><tr><th>32</th><td>SKCE (B = n)</td><td>10</td><td>16</td><td>0.002</td><td>0.000483276</td></tr><tr><th>33</th><td>SKCE (B = n)</td><td>10</td><td>64</td><td>0.0</td><td>0.00431563</td></tr><tr><th>34</th><td>SKCE (B = n)</td><td>10</td><td>256</td><td>0.0</td><td>0.0541269</td></tr><tr><th>35</th><td>SKCE (B = n)</td><td>10</td><td>1024</td><td>0.0</td><td>0.763799</td></tr><tr><th>36</th><td>CME</td><td>10</td><td>4</td><td>0.458</td><td>1.8213e-5</td></tr><tr><th>37</th><td>CME</td><td>10</td><td>16</td><td>0.004</td><td>3.1897e-5</td></tr><tr><th>38</th><td>CME</td><td>10</td><td>64</td><td>0.0</td><td>7.844e-5</td></tr><tr><th>39</th><td>CME</td><td>10</td><td>256</td><td>0.0</td><td>0.00026352</td></tr><tr><th>40</th><td>CME</td><td>10</td><td>1024</td><td>0.0</td><td>0.000995937</td></tr></tbody></table><h3 id="Visualization-2"><a class="docs-heading-anchor" href="#Visualization-2">Visualization</a><a class="docs-heading-anchor-permalink" href="#Visualization-2" title="Permalink"></a></h3><p>Again we visualize the results of our benchmarks. However, this time we compare the results for the calibrated and the uncalibrated model in the same plot.</p><pre><code class="language-julia">function plot_benchmark_tests(; dim::Int)
    # load and preprocess data
    df = mapreduce(vcat, (calibrated_model, uncalibrated_model)) do model
        filename = joinpath(&quot;data&quot;, &quot;synthetic&quot;, &quot;tests_$(model).csv&quot;)
        df = DataFrame(CSV.File(filename))
        df[!, :model] .= string(model)
        return df
    end
    groups = @from i in df begin
        @where i.dim == dim
        @orderby i.nsamples
        @select {
            i.test,
            i.model,
            log2_nsamples = log2(i.nsamples),
            i.testerror,
            log10_mintime = log10(i.mintime),
        }
        @collect DataFrame
    end

    # create figure
    fig = Figure(; resolution=(960, 400))

    # add labels
    Label(fig[1:2, 1], &quot;empirical test error&quot;; rotation=π / 2, tellheight=false)
    Label(fig[1, 2:3, Top()], &quot;calibrated model&quot;; padding=(0, 0, 10, 0))
    Label(fig[2, 2:3, Top()], &quot;uncalibrated model&quot;; padding=(0, 0, 10, 0))

    # create axes to plot test error vs number of samples
    ax1 = Axis(
        fig[1, 2];
        ylabel=&quot;type I error&quot;,
        xticks=2:2:10,
        xtickformat=logtickformat(2),
        xticklabelsize=12,
        yticklabelsize=12,
    )
    ax2 = Axis(
        fig[2, 2];
        xlabel=&quot;# samples&quot;,
        ylabel=&quot;type II error&quot;,
        xticks=2:2:10,
        xtickformat=logtickformat(2),
        xticklabelsize=12,
        yticklabelsize=12,
    )

    # create axes to plot test error vs timings
    ax3 = Axis(
        fig[1, 3]; xtickformat=logtickformat(10), xticklabelsize=12, yticklabelsize=12
    )
    ax4 = Axis(
        fig[2, 3];
        xlabel=&quot;time [s]&quot;,
        xtickformat=logtickformat(10),
        xticklabelsize=12,
        yticklabelsize=12,
    )

    # plot benchmark results
    tests = [&quot;SKCE (B = 2)&quot;, &quot;SKCE (B = √n)&quot;, &quot;SKCE (B = n)&quot;, &quot;CME&quot;]
    markers = [&#39;●&#39;, &#39;■&#39;, &#39;▲&#39;, &#39;◆&#39;]
    for (i, (test, marker)) in enumerate(zip(tests, markers))
        color = Dark2_8[i]

        # for both calibrated and uncalibrated model
        for (axes, model) in
            zip(((ax1, ax3), (ax2, ax4)), (calibrated_model, uncalibrated_model))
            group = filter(x -&gt; x.test == test &amp;&amp; x.model == string(model), groups)

            # plot test error vs samples
            scatterlines!(
                axes[1],
                group.log2_nsamples,
                group.testerror;
                color=color,
                linewidth=2,
                marker=marker,
                markercolor=color,
            )

            # plot test error vs timings
            scatterlines!(
                axes[2],
                group.log10_mintime,
                group.testerror;
                color=color,
                linewidth=2,
                marker=marker,
                markercolor=color,
            )
        end
    end

    # plot horizontal lines for significance level
    for axis in (ax1, ax3)
        hlines!(axis, 0.05; color=:black, linestyle=:dash, linewidth=2)
    end

    # link axes and hide decorations
    linkxaxes!(ax1, ax2)
    hidexdecorations!(ax1)
    linkxaxes!(ax3, ax4)
    hidexdecorations!(ax3)
    linkyaxes!(ax1, ax3)
    hideydecorations!(ax3)
    linkyaxes!(ax2, ax4)
    hideydecorations!(ax4)

    # add legend
    elems = map(1:length(tests)) do i
        [
            LineElement(; color=Dark2_8[i], linestyle=nothing, linewidth=2),
            MarkerElement(; color=Dark2_8[i], marker=markers[i], strokecolor=:black),
        ]
    end
    push!(elems, [LineElement(; color=:black, linestyle=:dash, linewidth=2)])
    Legend(
        fig[1:2, end + 1],
        elems,
        vcat(tests, &quot;significance level&quot;);
        tellwidth=true,
        gridshalign=:left,
    )

    return fig
end

plot_benchmark_tests(; dim=1)
wsavefig(&quot;figures/synthetic/tests_dim=1.svg&quot;);</code></pre><p><img src="../figures/synthetic/tests_dim=1.svg" alt/></p><pre><code class="language-julia">plot_benchmark_tests(; dim=10)
wsavefig(&quot;figures/synthetic/tests_dim=10.svg&quot;);</code></pre><p><img src="../figures/synthetic/tests_dim=10.svg" alt/></p><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../ols/">« Ordinary least squares</a><a class="docs-footer-nextpage" href="../friedman/">Friedman regression problem »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Tuesday 4 May 2021 00:38">Tuesday 4 May 2021</span>. Using Julia version 1.5.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
