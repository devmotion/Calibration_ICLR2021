<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Friedman regression problem · Calibration tests beyond classification</title><link rel="canonical" href="https://devmotion.github.io/Calibration_ICLR2021/generated/friedman/"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">Calibration tests beyond classification</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../software/">Software</a></li><li><span class="tocitem">Experiments</span><ul><li><a class="tocitem" href="../ols/">Ordinary least squares</a></li><li><a class="tocitem" href="../synthetic/">Synthetic models</a></li><li class="is-active"><a class="tocitem" href>Friedman regression problem</a><ul class="internal"><li><a class="tocitem" href="#Packages"><span>Packages</span></a></li><li><a class="tocitem" href="#Regression-problem"><span>Regression problem</span></a></li><li><a class="tocitem" href="#Model"><span>Model</span></a></li><li><a class="tocitem" href="#Training"><span>Training</span></a></li><li><a class="tocitem" href="#Evaluations"><span>Evaluations</span></a></li><li><a class="tocitem" href="#Ensembles"><span>Ensembles</span></a></li><li><a class="tocitem" href="#Visualization"><span>Visualization</span></a></li></ul></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Experiments</a></li><li class="is-active"><a href>Friedman regression problem</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Friedman regression problem</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/devmotion/Calibration_ICLR2021/blob/main/experiments/src/friedman/script.jl" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Friedman-regression-problem"><a class="docs-heading-anchor" href="#Friedman-regression-problem">Friedman regression problem</a><a id="Friedman-regression-problem-1"></a><a class="docs-heading-anchor-permalink" href="#Friedman-regression-problem" title="Permalink"></a></h1><p><a href="https://nbviewer.jupyter.org/github/devmotion/Calibration_ICLR2021/blob/gh-pages/dev/generated/friedman.ipynb"><img src="https://img.shields.io/badge/show-nbviewer-579ACA.svg" alt/></a></p><p>You are seeing the HTML output generated by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a> from the <a href="https://github.com/devmotion/Calibration_ICLR2021/blob/main/experiments/src/friedman/script.jl">Julia source file</a>. The corresponding notebook can be viewed in <a href="https://nbviewer.jupyter.org/github/devmotion/Calibration_ICLR2021/blob/gh-pages/dev/generated/friedman.ipynb">nbviewer</a>, and the plain script output can be found <a href="../friedman.jl">here</a>.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>If you want to run the experiments, make sure you have an identical environment. Please use Julia 1.5.3 and activate and instantiate the environment using <a href="https://github.com/devmotion/Calibration_ICLR2021/blob/main/experiments/src/friedman/Project.toml">this Project.toml file</a> and <a href="https://github.com/devmotion/Calibration_ICLR2021/blob/main/experiments/src/friedman/Manifest.toml">this Manifest.toml file</a>.</p><p><a href="https://github.com/devmotion/Calibration_ICLR2021/">The Github repository</a> contains <a href="https://github.com/devmotion/Calibration_ICLR2021/blob/main/experiments/README.md">more detailed instructions</a> and a <code>nix</code> project environment with a pinned Julia binary for improved reproducibility.</p></div></div><h2 id="Packages"><a class="docs-heading-anchor" href="#Packages">Packages</a><a id="Packages-1"></a><a class="docs-heading-anchor-permalink" href="#Packages" title="Permalink"></a></h2><pre><code class="language-julia">using Arrow
using CairoMakie
using CalibrationErrors
using CalibrationErrorsDistributions
using CalibrationTests
using CSV
using DataFrames
using Distributions
using Flux
using ProgressLogging
using Query
using Roots
using Showoff

using Random

using CairoMakie.AbstractPlotting.ColorSchemes: Dark2_8

using Logging: with_logger
using TerminalLoggers: TerminalLogger

# set random seed
Random.seed!(1234)

# create path before saving and
# use a different line join style to avoid visually irritating and incorrect values
function wsavefig(file, fig::Figure=current_figure())
    mkpath(dirname(file))
    scene = CairoMakie.AbstractPlotting.get_scene(fig)
    ext = lowercase(replace(splitext(file)[2], &quot;.&quot; =&gt; &quot;&quot;))
    screen = CairoMakie.CairoScreen(scene, file, Symbol(ext))
    CairoMakie.Cairo.set_line_join(screen.context, CairoMakie.Cairo.CAIRO_LINE_JOIN_BEVEL)
    CairoMakie.cairo_draw(screen, scene)
    CairoMakie.Cairo.finish(screen.surface)
    return screen
end

# define progress logging frontend
const PROGRESSLOGGER = TerminalLogger()

# define non-intrusive plotting style
set_theme!(
    Theme(;
        Axis=(
            rightspinevisible=false,
            topspinevisible=false,
            xgridvisible=false,
            ygridvisible=false,
        ),
        Legend=(framevisible=false,),
    ),
)</code></pre><h2 id="Regression-problem"><a class="docs-heading-anchor" href="#Regression-problem">Regression problem</a><a id="Regression-problem-1"></a><a class="docs-heading-anchor-permalink" href="#Regression-problem" title="Permalink"></a></h2><p>We study the so-called Friedman 1 regression problem, which was initially described for 200 inputs in the six-dimensional unit hypercube and later modified to 100 inputs in the 10-dimensional unit hypercube. In this regression problem real-valued target <span>$Y$</span> depends on input <span>$X$</span> via</p><p class="math-container">\[Y = 10 \sin{(\pi X_1 X_2)} + 20 (X_3 − 0.5)^2 + 10 X_4 + 5 X_5 + \epsilon,\]</p><p>where noise <span>$\epsilon$</span> is typically chosen to be independently standard normally distributed. We generate a training data set of 100 inputs distributed uniformly at random in the 10-dimensional unit hypercube and corresponding targets with identically and independently distributed noise following a standard normal distribution.</p><pre><code class="language-julia">friedman1(x) = 10 * sinpi(x[1] * x[2]) + 20 * (x[3] - 1//2)^2 + 10 * x[4] + 5 * x[5]

function sample_data(n::Int)
    # sample inputs
    xs = rand(10, n)

    # sample targets
    ys = map(eachcol(xs)) do x
        return friedman1(x) + randn()
    end

    return xs, ys
end

Random.seed!(100)
train_data = sample_data(100);</code></pre><p>For the evaluation of the models we use another data set of 50 samples that is sampled according to the same law.</p><pre><code class="language-julia">Random.seed!(200)
test_data = sample_data(50);</code></pre><h2 id="Model"><a class="docs-heading-anchor" href="#Model">Model</a><a id="Model-1"></a><a class="docs-heading-anchor-permalink" href="#Model" title="Permalink"></a></h2><p>We consider models <span>$P^{(\theta,\sigma^2)}$</span> of normal distributions with fixed variance</p><p class="math-container">\[P^{(\theta,\sigma^2)}(Y | X = x) = \mathcal{N}(f_{\theta}(x), \sigma^2),\]</p><p>where <span>$f_{\theta}(x)$</span>, the model of the mean of the distribution <span>$\mathbb{P}(Y|X = x)$</span>, is given by a fully connected neural network with two hidden layers with 200 and 50 hidden units and ReLU activation functions. The parameters of the neural network are denoted by <span>$\theta$</span>.</p><pre><code class="language-julia"># `Float64` version of `Flux.glorot_uniform`
function glorot_uniform(nout::Int, nin::Int)
    return (rand(nout, nin) .- 0.5) .* sqrt(24 / (nout + nin))
end

# neural network model
function nn_model()
    # initial parameters
    f = Chain(
        Dense(10, 200, relu; initW=glorot_uniform, initb=zeros),
        Dense(200, 50, relu; initW=glorot_uniform, initb=zeros),
        Dense(50, 1; initW=glorot_uniform, initb=zeros),
        vec,
    )
    σ = Random.randexp()

    return f, σ
end</code></pre><pre class="documenter-example-output">nn_model (generic function with 1 method)</pre><h2 id="Training"><a class="docs-heading-anchor" href="#Training">Training</a><a id="Training-1"></a><a class="docs-heading-anchor-permalink" href="#Training" title="Permalink"></a></h2><p>We use a maximum likelihood approach and train the parameters <span>$\theta$</span> of the model for 5000 iterations by minimizing the mean squared error on the training data set using ADAM. In each iteration, the variance <span>$\sigma^2$</span> is set to the maximizer of the likelihood of the training data set.</p><p>We train 10 models for each combination of regression problem and model category, and compute the predicted distributions on the training and test data sets in each iteration step.</p><p>The initial values of the weight matrices of the neural networks are sampled from the <a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">uniform Glorot initialization</a> and the offset vectors are initialized with zeros. The model parameters are learnt by iteratively minimizing the negative log-likelihood on the training data set. The parameters of the neural networks are trained by gradient descent with the <a href="https://arxiv.org/pdf/1412.6980.pdf">Adam optimization algorithm</a> (default settings in <a href="https://github.com/FluxML/Flux.jl">Flux.jl</a>), and in each iteration step the variance parameter of the predicted Gaussian distributions is set to the optimal value with respect to the negative log-likelihood on the training data set.</p><pre><code class="language-julia">function train(id, (train_xs, train_ys), (test_xs, _))
    # check if file exists
    filename = joinpath(&quot;data&quot;, &quot;friedman&quot;, &quot;predictions_id=$(id).arrow&quot;)
    isfile(filename) &amp;&amp; return nothing

    # compute the predictions of the initial neural network
    f, σ = nn_model()
    train_μs = f(train_xs)
    test_μs = f(test_xs)

    # save the initial model and its predictions
    niters = 5000
    train_μss = Vector{typeof(train_μs)}(undef, niters + 1)
    test_μss = Vector{typeof(test_μs)}(undef, niters + 1)
    σs = Vector{typeof(σ)}(undef, niters + 1)
    train_μss[1] = train_μs
    test_μss[1] = test_μs
    σs[1] = σ

    # train with ADAM
    params = Flux.Params(Flux.params(f))
    opt = ADAM()
    @progress name = &quot;training (id = $id)&quot; for i in 2:(niters + 1)
        # compute gradients
        gradients = gradient(params) do
            return Flux.Losses.mse(f(train_xs), train_ys)
        end

        # update the parameters
        Flux.Optimise.update!(opt, params, gradients)

        # update the variance
        yhats = f(train_xs)
        σ = sqrt(Flux.Losses.mse(yhats, train_ys))

        # save the model and its predictions
        train_μss[i] = yhats
        test_μss[i] = f(test_xs)
        σs[i] = σ
    end

    # save the predictions
    mkpath(dirname(filename))
    Arrow.write(filename, (train_μs=train_μss, test_μs=test_μss, σ=σs))

    return nothing
end

Random.seed!(100)
for (id, seed) in enumerate(rand(UInt, 10))
    @info &quot;training NN model: run $id&quot;
    Random.seed!(seed)
    with_logger(PROGRESSLOGGER) do
        train(id, train_data, test_data)
    end
end</code></pre><pre class="documenter-example-output">[ Info: training NN model: run 1
[ Info: training NN model: run 2
[ Info: training NN model: run 3
[ Info: training NN model: run 4
[ Info: training NN model: run 5
[ Info: training NN model: run 6
[ Info: training NN model: run 7
[ Info: training NN model: run 8
[ Info: training NN model: run 9
[ Info: training NN model: run 10</pre><h2 id="Evaluations"><a class="docs-heading-anchor" href="#Evaluations">Evaluations</a><a id="Evaluations-1"></a><a class="docs-heading-anchor-permalink" href="#Evaluations" title="Permalink"></a></h2><p>We estimate the average negative log-likelihood (NLL) and the mean squared error (MSE). Additionally, we estimate the average pinball loss</p><p class="math-container">\[\frac{1}{n_\tau} \sum_{i=1}^{n_\tau} \mathbb{E}_{X,Y} L_{\tau_i}\big(Y, \mathrm{quantile}(P_X,\tau_i)\big)\]</p><p>for quantile levels <span>$\tau_i = 0.05i$</span> (<span>$n_\tau = 19$</span>), where <span>$L_\tau(y, \tilde{y}) = (1 - \tau) (\tilde{y} - y)_{+} + \tau (y - \tilde{y})_{+}$</span> for observation <span>$y$</span> and prediction <span>$\tilde{y}$</span>, and <span>$\mathrm{quantile}(P_x, \tau) = \inf_y \{P_x(Y \leq y) \geq \tau\}$</span> for quantile level <span>$\tau \in [0, 1]$</span>.</p><pre><code class="language-julia">function pinball_loss(level::Real, y::Real, quantile::Real)
    δ = quantile - y
    return ((δ &gt; 0) - level) * δ
end

function mean_pinball_loss(prediction, y)
    levels = 0.05:0.05:0.95
    return mean(pinball_loss(τ, y, quantile(prediction, τ)) for τ in levels)
end</code></pre><pre class="documenter-example-output">mean_pinball_loss (generic function with 1 method)</pre><p>Moreover, we estimate the squared kernel calibration error (SKCE) and the p-value of the null hypothesis that the model is calibrated. for calibration on the training and test data set for every model and every iteration.</p><pre><code class="language-julia">function evaluate_models(dataset, id, ys)
    # output file
    out = joinpath(&quot;data&quot;, &quot;friedman&quot;, &quot;statistics_id=$(id)_dataset=$(dataset).csv&quot;)
    isfile(out) &amp;&amp; return nothing

    # load data
    filename = joinpath(&quot;data&quot;, &quot;friedman&quot;, &quot;predictions_id=$(id).arrow&quot;)
    isfile(filename) || error(&quot;predictions for run &quot;, id, &quot; not found&quot;)
    tbl = Arrow.Table(filename)
    σs = tbl.σ
    μss = getproperty(tbl, Symbol(dataset, :_μs))
    predictionss = map(μss, σs) do μs, σ
        return map(μs) do μ
            return Normal(μ, σ)
        end
    end

    # define kernel
    kernel = WassersteinExponentialKernel() ⊗ SqExponentialKernel()

    return evaluate_stats(out, predictionss, ys, kernel)
end

function evaluate_stats(file, predictionss, ys, kernel)
    mkpath(dirname(file))
    open(file, &quot;w&quot;) do f
        # print headers
        println(f, &quot;iteration,statistic,estimate&quot;)

        @progress name = &quot;iterations&quot; for (i, predictions) in enumerate(predictionss)
            # average NLL
            nll = -mean(map(logpdf, predictions, ys))
            println(f, i - 1, &quot;,NLL,&quot;, nll)

            # mean squared error
            mse = Flux.Losses.mse(map(mean, predictions), ys)
            println(f, i - 1, &quot;,MSE,&quot;, mse)

            # pinball loss
            pinball = mean(map(mean_pinball_loss, predictions, ys))
            println(f, i - 1, &quot;,pinball,&quot;, pinball)

            # unbiased estimator of SKCE
            unbiased_estimator = UnbiasedSKCE(kernel)
            skce = calibrationerror(unbiased_estimator, predictions, ys)
            println(f, i - 1, &quot;,SKCE (unbiased),&quot;, skce)

            # biased estimator of SKCE
            biased_estimator = BiasedSKCE(kernel)
            skce = calibrationerror(biased_estimator, predictions, ys)
            println(f, i - 1, &quot;,SKCE (biased),&quot;, skce)

            # p-value
            test = AsymptoticSKCETest(kernel, predictions, ys)
            p = pvalue(test; bootstrap_iters=1_000)
            println(f, i - 1, &quot;,p-value,&quot;, p)
        end
    end

    return nothing
end

Random.seed!(300)
for (id, seed) in enumerate(rand(UInt, 10))
    # evaluate models on training data set
    @info &quot;evaluating training statistics: run $id&quot;
    Random.seed!(seed)
    with_logger(PROGRESSLOGGER) do
        evaluate_models(&quot;train&quot;, id, train_data[2])
    end

    # evaluate models on test data set
    @info &quot;evaluating test statistics: run $id&quot;
    Random.seed!(seed)
    with_logger(PROGRESSLOGGER) do
        evaluate_models(&quot;test&quot;, id, test_data[2])
    end
end</code></pre><pre class="documenter-example-output">[ Info: evaluating training statistics: run 1
[ Info: evaluating test statistics: run 1
[ Info: evaluating training statistics: run 2
[ Info: evaluating test statistics: run 2
[ Info: evaluating training statistics: run 3
[ Info: evaluating test statistics: run 3
[ Info: evaluating training statistics: run 4
[ Info: evaluating test statistics: run 4
[ Info: evaluating training statistics: run 5
[ Info: evaluating test statistics: run 5
[ Info: evaluating training statistics: run 6
[ Info: evaluating test statistics: run 6
[ Info: evaluating training statistics: run 7
[ Info: evaluating test statistics: run 7
[ Info: evaluating training statistics: run 8
[ Info: evaluating test statistics: run 8
[ Info: evaluating training statistics: run 9
[ Info: evaluating test statistics: run 9
[ Info: evaluating training statistics: run 10
[ Info: evaluating test statistics: run 10</pre><h2 id="Ensembles"><a class="docs-heading-anchor" href="#Ensembles">Ensembles</a><a id="Ensembles-1"></a><a class="docs-heading-anchor-permalink" href="#Ensembles" title="Permalink"></a></h2><p>We create an ensemble of models from the trained models by combining models at each training iteration.</p><p>We evaluate the same statistics as for the individual ensemble members above. Since the quantiles of a Gaussian mixture model are not available in a closed-form expression, we use a bisection algorithm.</p><pre><code class="language-julia"># see https://github.com/JuliaStats/Distributions.jl/pull/1195
function Distributions.quantile(dist::UnivariateMixture{Continuous}, level::Real)
    ps = probs(dist)
    bracket = extrema(
        quantile(component(dist, i), level) for (i, pi) in enumerate(ps) if pi &gt; 0
    )
    return find_zero(bracket) do x
        cdf(dist, x) - level
    end
end

function evaluate_ensembles(dataset, ys)
    # output file
    out = joinpath(&quot;data&quot;, &quot;friedman&quot;, &quot;statistics_ensembles_dataset=$(dataset).csv&quot;)
    isfile(out) &amp;&amp; return nothing

    # load data
    tables = map(1:10) do id
        file = joinpath(&quot;data&quot;, &quot;friedman&quot;, &quot;predictions_id=$(id).arrow&quot;)
        isfile(file) || error(&quot;predictions for run &quot;, id, &quot; not found&quot;)
        return Arrow.Table(file)
    end
    nmodels = length(first(tables).σ)
    nsamples = length(ys)
    μs = Symbol(dataset, :_μs)
    predictionss = map(1:nmodels) do i
        map(1:nsamples) do j
            MixtureModel(
                map(tables) do table
                    Normal(getproperty(table, μs)[i][j], table.σ[i])
                end,
            )
        end
    end

    # kernel
    kernel = MixtureWassersteinExponentialKernel() ⊗ SqExponentialKernel()

    return evaluate_stats(out, predictionss, ys, kernel)
end

# evaluate ensembles on training data set
@info &quot;evaluating training statistics: ensembles of runs 1:10&quot;
Random.seed!(400)
with_logger(PROGRESSLOGGER) do
    evaluate_ensembles(&quot;train&quot;, train_data[2])
end

# evaluate ensembles on test data set
@info &quot;evaluating test statistics: ensembles of runs 1:10&quot;
Random.seed!(500)
with_logger(PROGRESSLOGGER) do
    evaluate_ensembles(&quot;test&quot;, test_data[2])
end</code></pre><pre class="documenter-example-output">[ Info: evaluating training statistics: ensembles of runs 1:10
[ Info: evaluating test statistics: ensembles of runs 1:10</pre><h2 id="Visualization"><a class="docs-heading-anchor" href="#Visualization">Visualization</a><a id="Visualization-1"></a><a class="docs-heading-anchor-permalink" href="#Visualization" title="Permalink"></a></h2><p>We visualize the resulting estimates by overlaying them with the estimates of the individual models.</p><pre><code class="language-julia">function logtickformat(base::Int)
    function format(values)
        return map(Base.Fix2(logformat, base), showoff(values))
    end
    return format
end

function logformat(digits::String, base::Int)
    buf = IOBuffer()
    print(buf, base)
    for c in digits
        if &#39;0&#39; ≤ c ≤ &#39;9&#39;
            print(buf, Showoff.superscript_numerals[c - &#39;0&#39; + 1])
        elseif c == &#39;-&#39;
            print(buf, &#39;⁻&#39;)
        elseif c == &#39;.&#39;
            print(buf, &#39;·&#39;)
        end
    end
    return String(take!(buf))
end

function asinhtickformat(factor)
    function format(values)
        return &quot;sinh(&quot; .* showoff(values ./ factor) .* &quot;)&quot;
    end
    return format
end

function plot_statistic!(ax::Axis, iterations, statistic; transform=identity)
    # load and filter statistics
    models_df = mapreduce(vcat, Iterators.product(1:10, (&quot;train&quot;, &quot;test&quot;))) do (id, dataset)
        file = joinpath(&quot;data&quot;, &quot;friedman&quot;, &quot;statistics_id=$(id)_dataset=$(dataset).csv&quot;)
        df = @from i in DataFrame(CSV.File(file)) begin
            @where i.statistic == statistic &amp;&amp; i.iteration in iterations
            @select i
            @collect DataFrame
        end
        df[!, :dataset] .= dataset
        return df
    end
    sort!(models_df, :iteration)

    ensembles_df = mapreduce(vcat, (&quot;train&quot;, &quot;test&quot;)) do dataset
        file = joinpath(&quot;data&quot;, &quot;friedman&quot;, &quot;statistics_ensembles_dataset=$(dataset).csv&quot;)
        df = @from i in DataFrame(CSV.File(file)) begin
            @where i.statistic == statistic &amp;&amp; i.iteration in iterations
            @select i
            @collect DataFrame
        end
        df[!, :dataset] .= dataset
        return df
    end
    sort!(ensembles_df, :iteration)

    # plot evaluations
    for (j, dataset) in enumerate((&quot;train&quot;, &quot;test&quot;))
        # models
        if !isempty(models_df)
            models = @from i in models_df begin
                @where i.dataset == dataset
                @orderby i.iteration
                @group i by i.iteration into g
                @select {
                    iteration = key(g),
                    mean = transform(mean(g.estimate)),
                    min = transform(minimum(g.estimate)),
                    max = transform(maximum(g.estimate)),
                }
                @collect DataFrame
            end
            color = Dark2_8[2 * j - 1]
            band!(ax, models.iteration, models.min, models.max; color=(color, 0.2))
            lines!(ax, models.iteration, models.mean; color=color)
        end

        # ensembles
        if !isempty(ensembles_df)
            ensembles = @from i in ensembles_df begin
                @where i.dataset == dataset
                @orderby i.iteration
                @select {i.iteration, estimate = transform(i.estimate)}
                @collect DataFrame
            end
            color = Dark2_8[2 * j]
            lines!(ax, ensembles.iteration, ensembles.estimate; color=color)
        end
    end

    return ax
end

function statsplot(iterations, statistics::Matrix; kwargs...)
    # create scene
    nrows, ncols = size(statistics)
    fig = Figure(; resolution=(500 * ncols, 200 * nrows))

    # for all statistics
    transforms = Dict(
        &quot;MSE&quot; =&gt; log10,
        &quot;NLL&quot; =&gt; asinh,
        &quot;pinball&quot; =&gt; log10,
        &quot;SKCE (unbiased)&quot; =&gt; x -&gt; asinh(1000 * x),
        &quot;SKCE (biased)&quot; =&gt; log10,
        &quot;p-value&quot; =&gt; identity,
    )
    ytickformats = Dict(
        &quot;MSE&quot; =&gt; logtickformat(10),
        &quot;NLL&quot; =&gt; asinhtickformat(1),
        &quot;pinball&quot; =&gt; logtickformat(10),
        &quot;SKCE (unbiased)&quot; =&gt; asinhtickformat(1000),
        &quot;SKCE (biased)&quot; =&gt; logtickformat(10),
        &quot;p-value&quot; =&gt; AbstractPlotting.automatic,
    )
    for j in 1:ncols, i in 1:nrows
        statistic = statistics[i, j]

        # add axis
        ax = Axis(
            fig[i, j];
            title=statistic,
            ytickformat=ytickformats[statistic],
            xticklabelsize=12.0f0,
            yticklabelsize=12.0f0,
        )

        # plot results
        plot_statistic!(ax, iterations, statistic; transform=transforms[statistic])

        # hide x ticks in every column apart from bottom row
        if i &lt; nrows
            hidexdecorations!(ax)
        end
    end

    # link all x axes
    axes = contents(fig[1:nrows, 1:ncols])
    linkxaxes!(axes...)

    # add labels
    Label(fig[end + 1, 1:ncols], &quot;iteration&quot;)
    Label(fig[1:nrows, 0], &quot;estimate&quot;; rotation=π / 2, tellheight=false)

    # tighten limits
    for ax in axes
        tightlimits!(ax, Left(), Right())
    end

    # create legend on right
    elems = map(1:2:3) do i
        model_color = Dark2_8[i]
        ensemble_color = Dark2_8[i + 1]
        return [
            [
                PolyElement(;
                    color=(model_color, 0.2),
                    strokewidth=0,
                    strokecolor=(model_color, 0.2),
                ),
                LineElement(; color=model_color, linestyle=nothing),
            ],
            LineElement(; color=ensemble_color, linestyle=nothing),
        ]
    end
    Legend(
        fig[1:nrows, end + 1],
        elems,
        [[&quot;models&quot;, &quot;ensemble&quot;], [&quot;models&quot;, &quot;ensemble&quot;]],
        [&quot;training&quot;, &quot;test&quot;];
        titlefont=&quot;Dejavu Sans Bold&quot;,
    )

    return fig
end

statsplot(0:5000, [&quot;MSE&quot; &quot;SKCE (unbiased)&quot;; &quot;NLL&quot; &quot;SKCE (biased)&quot;; &quot;pinball&quot; &quot;p-value&quot;])
wsavefig(&quot;figures/friedman/statsplot.svg&quot;);</code></pre><p><img src="../figures/friedman/statsplot.svg" alt/></p><pre><code class="language-julia">statsplot(10:1500, [&quot;MSE&quot; &quot;NLL&quot;; &quot;SKCE (biased)&quot; &quot;p-value&quot;])
wsavefig(&quot;figures/friedman/statsplot_zoom.svg&quot;);</code></pre><p><img src="../figures/friedman/statsplot_zoom.svg" alt/></p><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../synthetic/">« Synthetic models</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Friday 21 October 2022 10:06">Friday 21 October 2022</span>. Using Julia version 1.5.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
