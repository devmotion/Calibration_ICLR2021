<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Ordinary least squares · Calibration tests beyond classification</title><link rel="canonical" href="https://devmotion.github.io/Calibration_ICLR2021/generated/ols/"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">Calibration tests beyond classification</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../software/">Software</a></li><li><span class="tocitem">Experiments</span><ul><li class="is-active"><a class="tocitem" href>Ordinary least squares</a><ul class="internal"><li><a class="tocitem" href="#Packages"><span>Packages</span></a></li><li><a class="tocitem" href="#Regression-problem"><span>Regression problem</span></a></li><li><a class="tocitem" href="#Ordinary-least-squares-regression"><span>Ordinary least squares regression</span></a></li><li><a class="tocitem" href="#Validation"><span>Validation</span></a></li><li><a class="tocitem" href="#Quantile-calibration"><span>Quantile calibration</span></a></li><li><a class="tocitem" href="#Calibration-test"><span>Calibration test</span></a></li></ul></li><li><a class="tocitem" href="../synthetic/">Synthetic models</a></li><li><a class="tocitem" href="../friedman/">Friedman regression problem</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Experiments</a></li><li class="is-active"><a href>Ordinary least squares</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Ordinary least squares</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/devmotion/Calibration_ICLR2021/blob/main/experiments/src/ols/script.jl" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Ordinary-least-squares"><a class="docs-heading-anchor" href="#Ordinary-least-squares">Ordinary least squares</a><a id="Ordinary-least-squares-1"></a><a class="docs-heading-anchor-permalink" href="#Ordinary-least-squares" title="Permalink"></a></h1><p><a href="https://nbviewer.jupyter.org/github/devmotion/Calibration_ICLR2021/blob/gh-pages/dev/generated/ols.ipynb"><img src="https://img.shields.io/badge/show-nbviewer-579ACA.svg" alt/></a></p><p>You are seeing the HTML output generated by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a> from the <a href="https://github.com/devmotion/Calibration_ICLR2021/blob/main/experiments/src/ols/script.jl">Julia source file</a>. The corresponding notebook can be viewed in <a href="https://nbviewer.jupyter.org/github/devmotion/Calibration_ICLR2021/blob/gh-pages/dev/generated/ols.ipynb">nbviewer</a>, and the plain script output can be found <a href="../ols.jl">here</a>.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>If you want to run the experiments, make sure you have an identical environment. Please use Julia 1.5.3 and activate and instantiate the environment using <a href="https://github.com/devmotion/Calibration_ICLR2021/blob/main/experiments/src/ols/Project.toml">this Project.toml file</a> and <a href="https://github.com/devmotion/Calibration_ICLR2021/blob/main/experiments/src/ols/Manifest.toml">this Manifest.toml file</a>.</p><p><a href="https://github.com/devmotion/Calibration_ICLR2021/">The Github repository</a> contains <a href="https://github.com/devmotion/Calibration_ICLR2021/blob/main/experiments/README.md">more detailed instructions</a> and a <code>nix</code> project environment with a pinned Julia binary for improved reproducibility.</p></div></div><h2 id="Packages"><a class="docs-heading-anchor" href="#Packages">Packages</a><a id="Packages-1"></a><a class="docs-heading-anchor-permalink" href="#Packages" title="Permalink"></a></h2><pre><code class="language-julia">using CairoMakie
using CalibrationErrors
using CalibrationErrorsDistributions
using CalibrationTests
using Distributions
using StatsBase

using Random

using CairoMakie.AbstractPlotting.ColorSchemes: Dark2_8

# set random seed
Random.seed!(1234)

# create path before saving
function wsavefig(file, fig=current_figure())
    mkpath(dirname(file))
    return save(file, fig)
end</code></pre><pre class="documenter-example-output">wsavefig (generic function with 2 methods)</pre><h2 id="Regression-problem"><a class="docs-heading-anchor" href="#Regression-problem">Regression problem</a><a id="Regression-problem-1"></a><a class="docs-heading-anchor-permalink" href="#Regression-problem" title="Permalink"></a></h2><p>We consider a regression problem with scalar feature <span>$X$</span> and scalar target <span>$Y$</span> with input-dependent Gaussian noise that is inspired by a problem by <a href="https://arxiv.org/abs/1906.01620">Gustafsson, Danelljan, and Schön</a>. Feature <span>$X$</span> is distributed uniformly at random in <span>$[-1, 1]$</span>, and target <span>$Y$</span> is distributed according to</p><p class="math-container">\[Y \sim \sin(\pi X) + | 1 + X | \epsilon,\]</p><p>where <span>$\epsilon \sim \mathcal{N}(0, 0.15^2)$</span>.</p><p>We start by generating a data set consisting of 100 i.i.d. pairs of feature <span>$X$</span> and target <span>$Y$</span>:</p><pre><code class="language-julia">xs = rand(Uniform(-1, 1), 100)
ys = rand.(Normal.(sinpi.(xs), 0.15 .* abs.(1 .+ xs)))</code></pre><pre class="documenter-example-output">100-element Array{Float64,1}:
  0.5729105366916976
  0.7008258975704353
  0.579462922689501
 -0.3738848521513358
  1.1417358680464722
  0.4616589802994891
 -0.9707788440783894
 -0.8366526373482899
 -0.8925127554986921
  0.615269681318631
  ⋮
 -0.7679599180187714
 -0.657807297873009
  0.827303587029641
  0.5040791127460987
 -0.529848796212195
 -0.7957679298117362
 -0.777286429640957
  1.095834107039475
 -1.0344250288630767</pre><h2 id="Ordinary-least-squares-regression"><a class="docs-heading-anchor" href="#Ordinary-least-squares-regression">Ordinary least squares regression</a><a id="Ordinary-least-squares-regression-1"></a><a class="docs-heading-anchor-permalink" href="#Ordinary-least-squares-regression" title="Permalink"></a></h2><p>We perform ordinary least squares regression for this nonlinear heteroscedastic regression problem, and train a model <span>$P$</span> with homoscedastic variance. The fitted parameters of the model are</p><pre><code class="language-julia">bs = hcat(ones(length(xs)), xs) \ ys</code></pre><pre class="documenter-example-output">2-element Array{Float64,1}:
 0.03205598523016466
 1.005988112134222</pre><p>and the standard deviation of the model is given by</p><pre><code class="language-julia">stddev = std(bs[1] .+ bs[2] .* xs .- ys)</code></pre><pre class="documenter-example-output">0.44716982188151255</pre><p>The following plot visualizes the training data set and model <span>$P$</span>, together with the function <span>$f(x) = \mathbb{E}[Y | X = x] = \sin(\pi x)$</span>.</p><pre><code class="language-julia">fig = Figure(; resolution=(960, 450))

# plot the data generating distribution
ax1 = Axis(fig[1, 1]; title=&quot;ℙ(Y|X)&quot;, xlabel=&quot;X&quot;, ylabel=&quot;Y&quot;)
heatmap!(
    -1:0.01:1,
    -2:0.01:2,
    (x, y) -&gt; pdf(Normal(sinpi(x), 0.15 * abs(1 + x)), y);
    colorrange=(0, 1),
)
scatter!(xs, ys; color=Dark2_8[2])
tightlimits!(ax1)

# plot the predictions of the model
ax2 = Axis(fig[1, 2]; title=&quot;P(Y|X)&quot;, xlabel=&quot;X&quot;, ylabel=&quot;Y&quot;)
heatmap!(
    -1:0.01:1,
    -2:0.01:2,
    let offset = bs[1], slope = bs[2], stddev = stddev
        (x, y) -&gt; pdf(Normal(offset + slope * x, stddev), y)
    end;
    colorrange=(0, 1),
)
scatter!(xs, ys; color=Dark2_8[2])
tightlimits!(ax2)

# link axes and hide y labels and ticks of the second plot
linkaxes!(ax1, ax2)
hideydecorations!(ax2; grid=false)

# add a colorbar
Colorbar(fig[1, 3]; label=&quot;density&quot;, width=30)

# adjust space
colgap!(fig.layout, 50)

wsavefig(&quot;figures/ols/heatmap.svg&quot;);</code></pre><p><img src="../figures/ols/heatmap.svg" alt/></p><h2 id="Validation"><a class="docs-heading-anchor" href="#Validation">Validation</a><a id="Validation-1"></a><a class="docs-heading-anchor-permalink" href="#Validation" title="Permalink"></a></h2><p>We evaluate calibration of the model with a validation data set of <span>$n = 50$</span> i.i.d. pairs of samples <span>$(X_1, Y_1), \ldots, (X_n, Y_n)$</span> of <span>$(X, Y)$</span>.</p><pre><code class="language-julia">valxs = rand(Uniform(-1, 1), 50)
valys = rand.(Normal.(sinpi.(valxs), 0.15 .* abs.(1 .+ valxs)))</code></pre><pre class="documenter-example-output">50-element Array{Float64,1}:
  0.8326520514581572
  0.14294419777673867
  0.5259941064739304
 -0.7588180975334454
 -0.22997692304435916
 -0.16904762050501862
 -0.34910087885759844
  0.46510023094823394
 -0.35216378065610593
 -0.9371350333199572
  ⋮
  0.7176168534475863
 -0.88489958005972
 -0.48045353020324627
 -0.7338495669243824
 -0.8484758728738399
 -0.8915442531193621
  0.5135713205439527
  0.8026195193453686
 -0.5179293540690129</pre><p>For these validation data points we compute the predicted distributions <span>$P(Y | X = X_i)$</span>.</p><pre><code class="language-julia">valps = Normal.(bs[1] .+ bs[2] .* valxs, stddev)</code></pre><pre class="documenter-example-output">50-element Array{Distributions.Normal{Float64},1}:
 Distributions.Normal{Float64}(μ=0.8970077906642928, σ=0.44716982188151255)
 Distributions.Normal{Float64}(μ=0.07369870816218799, σ=0.44716982188151255)
 Distributions.Normal{Float64}(μ=0.25940967206693516, σ=0.44716982188151255)
 Distributions.Normal{Float64}(μ=-0.6829736779116784, σ=0.44716982188151255)
 Distributions.Normal{Float64}(μ=-0.03794760257789277, σ=0.44716982188151255)
 Distributions.Normal{Float64}(μ=-0.9188737914330312, σ=0.44716982188151255)
 Distributions.Normal{Float64}(μ=-0.8593922040278451, σ=0.44716982188151255)
 Distributions.Normal{Float64}(μ=0.7180619651280059, σ=0.44716982188151255)
 Distributions.Normal{Float64}(μ=0.9771644795118668, σ=0.44716982188151255)
 Distributions.Normal{Float64}(μ=-0.4217415548264924, σ=0.44716982188151255)
 ⋮
 Distributions.Normal{Float64}(μ=0.2730567642719082, σ=0.44716982188151255)
 Distributions.Normal{Float64}(μ=-0.39171286084273294, σ=0.44716982188151255)
 Distributions.Normal{Float64}(μ=-0.8150460351758918, σ=0.44716982188151255)
 Distributions.Normal{Float64}(μ=-0.6936273214320248, σ=0.44716982188151255)
 Distributions.Normal{Float64}(μ=-0.351051448290986, σ=0.44716982188151255)
 Distributions.Normal{Float64}(μ=-0.3531169842483718, σ=0.44716982188151255)
 Distributions.Normal{Float64}(μ=0.9666583662589786, σ=0.44716982188151255)
 Distributions.Normal{Float64}(μ=0.6566165341916999, σ=0.44716982188151255)
 Distributions.Normal{Float64}(μ=-0.10723928713276361, σ=0.44716982188151255)</pre><h2 id="Quantile-calibration"><a class="docs-heading-anchor" href="#Quantile-calibration">Quantile calibration</a><a id="Quantile-calibration-1"></a><a class="docs-heading-anchor-permalink" href="#Quantile-calibration" title="Permalink"></a></h2><p>We evaluate the predicted cumulative probability <span>$\tau_i = P(Y \leq Y_i | X = X_i)$</span> for each validation data point.</p><pre><code class="language-julia">τs = cdf.(valps, valys)</code></pre><pre class="documenter-example-output">50-element Array{Float64,1}:
 0.44278265255012883
 0.5615313093194866
 0.7244655837714757
 0.4326584761922252
 0.3338041995994174
 0.9532117849671311
 0.8730978667242094
 0.2858005660265308
 0.0014756788522630802
 0.12454394031495247
 ⋮
 0.8399284573060224
 0.1350337806791715
 0.772843750864971
 0.4641640781508847
 0.132986603236461
 0.11427974892548726
 0.1554745338732305
 0.6279786794373055
 0.1791993018454301</pre><p>The following plot visualizes the empirical cumulative distribution function of the predicted quantiles.</p><pre><code class="language-julia">fig = Figure(; resolution=(600, 450))

ax = Axis(
    fig[1, 1];
    xlabel=&quot;quantile level&quot;,
    ylabel=&quot;cumulative probability&quot;,
    xticks=0:0.25:1,
    yticks=0:0.25:1,
    autolimitaspect=1,
    rightspinevisible=false,
    topspinevisible=false,
    xgridvisible=false,
    ygridvisible=false,
)

# plot the ideal
lines!([0, 1], [0, 1]; label=&quot;ideal&quot;, linewidth=2, color=Dark2_8[1])

# plot the empirical cdf
sort!(τs)
ecdf_xs = vcat(0, repeat(τs; inner=2), 1)
ecdf_ys = repeat(range(0, 1; length=length(τs) + 1); inner=2)
lines!(ecdf_xs, ecdf_ys; label=&quot;data&quot;, linewidth=2, color=Dark2_8[2])

# add legend
Legend(fig[1, 2], ax; valign=:top, framevisible=false)

# set limits and aspect ratio
colsize!(fig.layout, 1, Aspect(1, 1))
tightlimits!(ax)

wsavefig(&quot;figures/ols/quantiles.svg&quot;);</code></pre><p><img src="../figures/ols/quantiles.svg" alt/></p><h2 id="Calibration-test"><a class="docs-heading-anchor" href="#Calibration-test">Calibration test</a><a id="Calibration-test-1"></a><a class="docs-heading-anchor-permalink" href="#Calibration-test" title="Permalink"></a></h2><p>We compute a <span>$p$</span>-value estimate of the null hypothesis that model <span>$P$</span> is calibrated using an estimation of the quantile of the asymptotic distribution of <span>$n \widehat{\mathrm{SKCE}}_{k,n}$</span> with 100000 bootstrap samples on the validation data set. Kernel <span>$k$</span> is chosen as the tensor product kernel</p><p class="math-container">\[\begin{aligned}
k\big((p, y), (p&#39;, y&#39;)\big) &amp;= \exp{\big(- W_2(p, p&#39;)\big)} \exp{\big(-(y - y&#39;)^2/2\big)} \\
&amp;= \exp{\big(-\sqrt{(m_p - m_{p&#39;})^2 + (\sigma_p - \sigma_{p&#39;})^2}\big)} \exp{\big( - (y - y&#39;)^2/2\big)},
\end{aligned}\]</p><p>where <span>$W_2$</span> is the 2-Wasserstein distance and <span>$m_p, m_{p&#39;}$</span> and <span>$\sigma_p, \sigma_{p&#39;}$</span> denote the mean and the standard deviation of the normal distributions <span>$p$</span> and <span>$p&#39;$</span>.</p><pre><code class="language-julia"># define kernel
kernel = WassersteinExponentialKernel() ⊗ SqExponentialKernel()

# compute p-value estimate using bootstrapping
pvalue(AsymptoticSKCETest(kernel, valps, valys); bootstrap_iters=100_000)</code></pre><pre class="documenter-example-output">0.04625</pre><p>We obtain <span>$p &lt; 0.05$</span> in our experiment, and hence the calibration test rejects <span>$H_0$</span> at the significance level <span>$\alpha = 0.05$</span>.</p><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../software/">« Software</a><a class="docs-footer-nextpage" href="../synthetic/">Synthetic models »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Friday 21 October 2022 10:06">Friday 21 October 2022</span>. Using Julia version 1.5.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
