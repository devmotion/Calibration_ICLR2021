{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Ordinary least squares\n",
    "\n",
    "\n",
    "You are seeing the\n",
    "notebook output generated by\n",
    "[Literate.jl](https://github.com/fredrikekre/Literate.jl) from the\n",
    "[Julia source file](https://github.com/devmotion/Calibration_ICLR2021/blob/main/experiments/src/ols/script.jl).\n",
    "The corresponding\n",
    "HTML output can be viewed [here](https://devmotion.github.io/Calibration_ICLR2021/dev/generated/ols/),\n",
    "and the plain script output can be found [here](./ols.jl).\n",
    "\n",
    "> **Note**\n",
    "> If you want to run the experiments, make sure you have an identical environment.\n",
    "> Please use Julia 1.5.3 and activate and instantiate the environment using\n",
    "> [this Project.toml file](https://github.com/devmotion/Calibration_ICLR2021/blob/main/experiments/src/ols/Project.toml)\n",
    "> and [this Manifest.toml file](https://github.com/devmotion/Calibration_ICLR2021/blob/main/experiments/src/ols/Manifest.toml).\n",
    ">\n",
    "> [The Github repository](https://github.com/devmotion/Calibration_ICLR2021/) contains\n",
    "> [more detailed instructions](https://github.com/devmotion/Calibration_ICLR2021/blob/main/experiments/README.md) and a\n",
    "> `nix` project environment with a pinned Julia binary for improved reproducibility."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Packages"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: SqExponentialKernel changed convention in version 0.8.0.\n",
      "This kernel now divides the squared distance by 2 to align with standard practice.\n",
      "This warning will be removed in 0.9.0.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "wsavefig (generic function with 2 methods)"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "cell_type": "code",
   "source": [
    "using CairoMakie\n",
    "using CalibrationErrors\n",
    "using CalibrationErrorsDistributions\n",
    "using CalibrationTests\n",
    "using Distributions\n",
    "using StatsBase\n",
    "\n",
    "using Random\n",
    "\n",
    "using CairoMakie.AbstractPlotting.ColorSchemes: Dark2_8\n",
    "\n",
    "# set random seed\n",
    "Random.seed!(1234)\n",
    "\n",
    "# create path before saving\n",
    "function wsavefig(file, fig=current_figure())\n",
    "    mkpath(dirname(file))\n",
    "    return save(file, fig)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Regression problem\n",
    "\n",
    "We consider a regression problem with scalar feature $X$ and scalar\n",
    "target $Y$ with input-dependent Gaussian noise that is inspired by a\n",
    "problem by [Gustafsson, Danelljan, and Schön](https://arxiv.org/abs/1906.01620).\n",
    "Feature $X$ is distributed uniformly at random in $[-1, 1]$,\n",
    "and target $Y$ is distributed according to\n",
    "$$\n",
    "Y \\sim \\sin(\\pi X) + | 1 + X | \\epsilon,\n",
    "$$\n",
    "where $\\epsilon \\sim \\mathcal{N}(0, 0.15^2)$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We start by generating a data set consisting of 100 i.i.d. pairs of\n",
    "feature $X$ and target $Y$:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "100-element Array{Float64,1}:\n  0.5729105366916976\n  0.7008258975704353\n  0.579462922689501\n -0.3738848521513358\n  1.1417358680464722\n  0.4616589802994891\n -0.9707788440783894\n -0.8366526373482899\n -0.8925127554986921\n  0.615269681318631\n  ⋮\n -0.7679599180187714\n -0.657807297873009\n  0.827303587029641\n  0.5040791127460987\n -0.529848796212195\n -0.7957679298117362\n -0.777286429640957\n  1.095834107039475\n -1.0344250288630767"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "cell_type": "code",
   "source": [
    "xs = rand(Uniform(-1, 1), 100)\n",
    "ys = rand.(Normal.(sinpi.(xs), 0.15 .* abs.(1 .+ xs)))"
   ],
   "metadata": {},
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ordinary least squares regression\n",
    "\n",
    "We perform ordinary least squares regression for this nonlinear heteroscedastic\n",
    "regression problem, and train a model $P$ with homoscedastic variance. The fitted\n",
    "parameters of the model are"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "2-element Array{Float64,1}:\n 0.03205598523016466\n 1.005988112134222"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "cell_type": "code",
   "source": [
    "bs = hcat(ones(length(xs)), xs) \\ ys"
   ],
   "metadata": {},
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "and the standard deviation of the model is given by"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.44716982188151255"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "cell_type": "code",
   "source": [
    "stddev = std(bs[1] .+ bs[2] .* xs .- ys)"
   ],
   "metadata": {},
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following plot visualizes the training data set and model $P$, together\n",
    "with the function $f(x) = \\mathbb{E}[Y | X = x] = \\sin(\\pi x)$."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌ Info: Makie/AbstractPlotting is caching fonts, this may take a while. Needed only on first run!\n",
      "└ @ AbstractPlotting /home/runner/.julia/packages/AbstractPlotting/ek9LT/src/utilities/texture_atlas.jl:115\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "fig = Figure(; resolution=(960, 450))\n",
    "\n",
    "# plot the data generating distribution\n",
    "ax1 = Axis(fig[1, 1]; title=\"ℙ(Y|X)\", xlabel=\"X\", ylabel=\"Y\")\n",
    "heatmap!(\n",
    "    -1:0.01:1,\n",
    "    -2:0.01:2,\n",
    "    (x, y) -> pdf(Normal(sinpi(x), 0.15 * abs(1 + x)), y);\n",
    "    colorrange=(0, 1),\n",
    ")\n",
    "scatter!(xs, ys; color=Dark2_8[2])\n",
    "tightlimits!(ax1)\n",
    "\n",
    "# plot the predictions of the model\n",
    "ax2 = Axis(fig[1, 2]; title=\"P(Y|X)\", xlabel=\"X\", ylabel=\"Y\")\n",
    "heatmap!(\n",
    "    -1:0.01:1,\n",
    "    -2:0.01:2,\n",
    "    let offset = bs[1], slope = bs[2], stddev = stddev\n",
    "        (x, y) -> pdf(Normal(offset + slope * x, stddev), y)\n",
    "    end;\n",
    "    colorrange=(0, 1),\n",
    ")\n",
    "scatter!(xs, ys; color=Dark2_8[2])\n",
    "tightlimits!(ax2)\n",
    "\n",
    "# link axes and hide y labels and ticks of the second plot\n",
    "linkaxes!(ax1, ax2)\n",
    "hideydecorations!(ax2; grid=false)\n",
    "\n",
    "# add a colorbar\n",
    "Colorbar(fig[1, 3]; label=\"density\", width=30)\n",
    "\n",
    "# adjust space\n",
    "colgap!(fig.layout, 50)\n",
    "\n",
    "wsavefig(\"figures/ols/heatmap.svg\");"
   ],
   "metadata": {},
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](figures/ols/heatmap.svg)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Validation\n",
    "\n",
    "We evaluate calibration of the model with a validation data set of $n = 50$ i.i.d. pairs\n",
    "of samples $(X_1, Y_1), \\ldots, (X_n, Y_n)$ of $(X, Y)$."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "50-element Array{Float64,1}:\n  0.8326520514581572\n  0.14294419777673867\n  0.5259941064739304\n -0.7588180975334454\n -0.22997692304435916\n -0.16904762050501862\n -0.34910087885759844\n  0.46510023094823394\n -0.35216378065610593\n -0.9371350333199572\n  ⋮\n  0.7176168534475863\n -0.88489958005972\n -0.48045353020324627\n -0.7338495669243824\n -0.8484758728738399\n -0.8915442531193621\n  0.5135713205439527\n  0.8026195193453686\n -0.5179293540690129"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "cell_type": "code",
   "source": [
    "valxs = rand(Uniform(-1, 1), 50)\n",
    "valys = rand.(Normal.(sinpi.(valxs), 0.15 .* abs.(1 .+ valxs)))"
   ],
   "metadata": {},
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "For these validation data points we compute the predicted distributions $P(Y | X = X_i)$."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "50-element Array{Distributions.Normal{Float64},1}:\n Distributions.Normal{Float64}(μ=0.8970077906642928, σ=0.44716982188151255)\n Distributions.Normal{Float64}(μ=0.07369870816218799, σ=0.44716982188151255)\n Distributions.Normal{Float64}(μ=0.25940967206693516, σ=0.44716982188151255)\n Distributions.Normal{Float64}(μ=-0.6829736779116784, σ=0.44716982188151255)\n Distributions.Normal{Float64}(μ=-0.03794760257789277, σ=0.44716982188151255)\n Distributions.Normal{Float64}(μ=-0.9188737914330312, σ=0.44716982188151255)\n Distributions.Normal{Float64}(μ=-0.8593922040278451, σ=0.44716982188151255)\n Distributions.Normal{Float64}(μ=0.7180619651280059, σ=0.44716982188151255)\n Distributions.Normal{Float64}(μ=0.9771644795118668, σ=0.44716982188151255)\n Distributions.Normal{Float64}(μ=-0.4217415548264924, σ=0.44716982188151255)\n ⋮\n Distributions.Normal{Float64}(μ=0.2730567642719082, σ=0.44716982188151255)\n Distributions.Normal{Float64}(μ=-0.39171286084273294, σ=0.44716982188151255)\n Distributions.Normal{Float64}(μ=-0.8150460351758918, σ=0.44716982188151255)\n Distributions.Normal{Float64}(μ=-0.6936273214320248, σ=0.44716982188151255)\n Distributions.Normal{Float64}(μ=-0.351051448290986, σ=0.44716982188151255)\n Distributions.Normal{Float64}(μ=-0.3531169842483718, σ=0.44716982188151255)\n Distributions.Normal{Float64}(μ=0.9666583662589786, σ=0.44716982188151255)\n Distributions.Normal{Float64}(μ=0.6566165341916999, σ=0.44716982188151255)\n Distributions.Normal{Float64}(μ=-0.10723928713276361, σ=0.44716982188151255)"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "cell_type": "code",
   "source": [
    "valps = Normal.(bs[1] .+ bs[2] .* valxs, stddev)"
   ],
   "metadata": {},
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Quantile calibration\n",
    "\n",
    "We evaluate the predicted cumulative probability $\\tau_i = P(Y \\leq Y_i | X = X_i)$ for\n",
    "each validation data point."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "50-element Array{Float64,1}:\n 0.44278265255012883\n 0.5615313093194866\n 0.7244655837714757\n 0.4326584761922252\n 0.3338041995994174\n 0.9532117849671311\n 0.8730978667242094\n 0.2858005660265308\n 0.0014756788522630802\n 0.12454394031495247\n ⋮\n 0.8399284573060224\n 0.1350337806791715\n 0.772843750864971\n 0.4641640781508847\n 0.132986603236461\n 0.11427974892548726\n 0.1554745338732305\n 0.6279786794373055\n 0.1791993018454301"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "cell_type": "code",
   "source": [
    "τs = cdf.(valps, valys)"
   ],
   "metadata": {},
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following plot visualizes the empirical cumulative distribution function of the\n",
    "predicted quantiles."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "fig = Figure(; resolution=(600, 450))\n",
    "\n",
    "ax = Axis(\n",
    "    fig[1, 1];\n",
    "    xlabel=\"quantile level\",\n",
    "    ylabel=\"cumulative probability\",\n",
    "    xticks=0:0.25:1,\n",
    "    yticks=0:0.25:1,\n",
    "    autolimitaspect=1,\n",
    "    rightspinevisible=false,\n",
    "    topspinevisible=false,\n",
    "    xgridvisible=false,\n",
    "    ygridvisible=false,\n",
    ")\n",
    "\n",
    "# plot the ideal\n",
    "lines!([0, 1], [0, 1]; label=\"ideal\", linewidth=2, color=Dark2_8[1])\n",
    "\n",
    "# plot the empirical cdf\n",
    "sort!(τs)\n",
    "ecdf_xs = vcat(0, repeat(τs; inner=2), 1)\n",
    "ecdf_ys = repeat(range(0, 1; length=length(τs) + 1); inner=2)\n",
    "lines!(ecdf_xs, ecdf_ys; label=\"data\", linewidth=2, color=Dark2_8[2])\n",
    "\n",
    "# add legend\n",
    "Legend(fig[1, 2], ax; valign=:top, framevisible=false)\n",
    "\n",
    "# set limits and aspect ratio\n",
    "colsize!(fig.layout, 1, Aspect(1, 1))\n",
    "tightlimits!(ax)\n",
    "\n",
    "wsavefig(\"figures/ols/quantiles.svg\");"
   ],
   "metadata": {},
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](figures/ols/quantiles.svg)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Calibration test\n",
    "\n",
    "We compute a $p$-value estimate of the null hypothesis that model $P$ is calibrated using\n",
    "an estimation of the quantile of the asymptotic distribution of\n",
    "$n \\widehat{\\mathrm{SKCE}}_{k,n}$ with 100000 bootstrap samples on the validation data\n",
    "set. Kernel $k$ is chosen as the tensor product kernel\n",
    "$$\n",
    "\\begin{aligned}\n",
    "k\\big((p, y), (p', y')\\big) &= \\exp{\\big(- W_2(p, p')\\big)} \\exp{\\big(-(y - y')^2/2\\big)} \\\\\n",
    "&= \\exp{\\big(-\\sqrt{(m_p - m_{p'})^2 + (\\sigma_p - \\sigma_{p'})^2}\\big)} \\exp{\\big( - (y - y')^2/2\\big)},\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $W_2$ is the 2-Wasserstein distance and $m_p, m_{p'}$ and $\\sigma_p, \\sigma_{p'}$\n",
    "denote the mean and the standard deviation of the normal distributions $p$ and $p'$."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.04625"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "cell_type": "code",
   "source": [
    "# define kernel\n",
    "kernel = WassersteinExponentialKernel() ⊗ SqExponentialKernel()\n",
    "\n",
    "# compute p-value estimate using bootstrapping\n",
    "pvalue(AsymptoticSKCETest(kernel, valps, valys); bootstrap_iters=100_000)"
   ],
   "metadata": {},
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "We obtain $p < 0.05$ in our experiment, and hence the calibration test rejects $H_0$ at\n",
    "the significance level $\\alpha = 0.05$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  },
  "kernelspec": {
   "name": "julia-1.5",
   "display_name": "Julia 1.5.3",
   "language": "julia"
  }
 },
 "nbformat": 4
}
