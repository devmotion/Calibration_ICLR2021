var documenterSearchIndex = {"docs":
[{"location":"generated/friedman/","page":"Friedman regression problem","title":"Friedman regression problem","text":"EditURL = \"https://github.com/devmotion/Calibration_ICLR2021/blob/main/experiments/src/friedman/script.jl\"","category":"page"},{"location":"generated/friedman/#Friedman-regression-problem","page":"Friedman regression problem","title":"Friedman regression problem","text":"","category":"section"},{"location":"generated/friedman/","page":"Friedman regression problem","title":"Friedman regression problem","text":"(Image: )","category":"page"},{"location":"generated/friedman/","page":"Friedman regression problem","title":"Friedman regression problem","text":"You are seeing the HTML output generated by Documenter.jl and Literate.jl from the Julia source file. The corresponding notebook can be viewed in nbviewer, and the plain script output can be found here.","category":"page"},{"location":"generated/friedman/","page":"Friedman regression problem","title":"Friedman regression problem","text":"note: Note\nIf you want to run the experiments, make sure you have an identical environment. Please use Julia 1.5.3 and activate and instantiate the environment using this Project.toml file and this Manifest.toml file.The Github repository contains more detailed instructions and a nix project environment with a pinned Julia binary for improved reproducibility.","category":"page"},{"location":"generated/friedman/","page":"Friedman regression problem","title":"Friedman regression problem","text":"using Pkg: Pkg\nPkg.activate(\"/home/runner/work/Calibration_ICLR2021/Calibration_ICLR2021/site/../experiments/src/friedman\")\nPkg.instantiate()","category":"page"},{"location":"generated/friedman/#Packages","page":"Friedman regression problem","title":"Packages","text":"","category":"section"},{"location":"generated/friedman/","page":"Friedman regression problem","title":"Friedman regression problem","text":"using Arrow\nusing CairoMakie\nusing CalibrationErrors\nusing CalibrationErrorsDistributions\nusing CalibrationTests\nusing CSV\nusing DataFrames\nusing Distributions\nusing Flux\nusing ProgressLogging\nusing Query\nusing Roots\nusing Showoff\n\nusing Random\n\nusing CairoMakie.AbstractPlotting.ColorSchemes: Dark2_8\n\nusing Logging: with_logger\nusing TerminalLoggers: TerminalLogger\n\n# set random seed\nRandom.seed!(1234)\n\n# create path before saving and\n# use a different line join style to avoid visually irritating and incorrect values\nfunction wsavefig(file, fig::Figure=current_figure())\n    mkpath(dirname(file))\n    scene = CairoMakie.AbstractPlotting.get_scene(fig)\n    ext = lowercase(replace(splitext(file)[2], \".\" => \"\"))\n    screen = CairoMakie.CairoScreen(scene, file, Symbol(ext))\n    CairoMakie.Cairo.set_line_join(screen.context, CairoMakie.Cairo.CAIRO_LINE_JOIN_BEVEL)\n    CairoMakie.cairo_draw(screen, scene)\n    CairoMakie.Cairo.finish(screen.surface)\n    return screen\nend\n\n# define progress logging frontend\nconst PROGRESSLOGGER = TerminalLogger()\n\n# define non-intrusive plotting style\nset_theme!(\n    Theme(;\n        Axis=(\n            rightspinevisible=false,\n            topspinevisible=false,\n            xgridvisible=false,\n            ygridvisible=false,\n        ),\n        Legend=(framevisible=false,),\n    ),\n)","category":"page"},{"location":"generated/friedman/#Regression-problem","page":"Friedman regression problem","title":"Regression problem","text":"","category":"section"},{"location":"generated/friedman/","page":"Friedman regression problem","title":"Friedman regression problem","text":"We study the so-called Friedman 1 regression problem, which was initially described for 200 inputs in the six-dimensional unit hypercube and later modified to 100 inputs in the 10-dimensional unit hypercube. In this regression problem real-valued target Y depends on input X via","category":"page"},{"location":"generated/friedman/","page":"Friedman regression problem","title":"Friedman regression problem","text":"Y = 10 sin(pi X_1 X_2) + 20 (X_3  05)^2 + 10 X_4 + 5 X_5 + epsilon","category":"page"},{"location":"generated/friedman/","page":"Friedman regression problem","title":"Friedman regression problem","text":"where noise epsilon is typically chosen to be independently standard normally distributed. We generate a training data set of 100 inputs distributed uniformly at random in the 10-dimensional unit hypercube and corresponding targets with identically and independently distributed noise following a standard normal distribution.","category":"page"},{"location":"generated/friedman/","page":"Friedman regression problem","title":"Friedman regression problem","text":"friedman1(x) = 10 * sinpi(x[1] * x[2]) + 20 * (x[3] - 1//2)^2 + 10 * x[4] + 5 * x[5]\n\nfunction sample_data(n::Int)\n    # sample inputs\n    xs = rand(10, n)\n\n    # sample targets\n    ys = map(eachcol(xs)) do x\n        return friedman1(x) + randn()\n    end\n\n    return xs, ys\nend\n\nRandom.seed!(100)\ntrain_data = sample_data(100);\nnothing #hide","category":"page"},{"location":"generated/friedman/","page":"Friedman regression problem","title":"Friedman regression problem","text":"For the evaluation of the models we use another data set of 50 samples that is sampled according to the same law.","category":"page"},{"location":"generated/friedman/","page":"Friedman regression problem","title":"Friedman regression problem","text":"Random.seed!(200)\ntest_data = sample_data(50);\nnothing #hide","category":"page"},{"location":"generated/friedman/#Model","page":"Friedman regression problem","title":"Model","text":"","category":"section"},{"location":"generated/friedman/","page":"Friedman regression problem","title":"Friedman regression problem","text":"We consider models P^(thetasigma^2) of normal distributions with fixed variance","category":"page"},{"location":"generated/friedman/","page":"Friedman regression problem","title":"Friedman regression problem","text":"P^(thetasigma^2)(Y  X = x) = mathcalN(f_theta(x) sigma^2)","category":"page"},{"location":"generated/friedman/","page":"Friedman regression problem","title":"Friedman regression problem","text":"where f_theta(x), the model of the mean of the distribution mathbbP(YX = x), is given by a fully connected neural network with two hidden layers with 200 and 50 hidden units and ReLU activation functions. The parameters of the neural network are denoted by theta.","category":"page"},{"location":"generated/friedman/","page":"Friedman regression problem","title":"Friedman regression problem","text":"# `Float64` version of `Flux.glorot_uniform`\nfunction glorot_uniform(nout::Int, nin::Int)\n    return (rand(nout, nin) .- 0.5) .* sqrt(24 / (nout + nin))\nend\n\n# neural network model\nfunction nn_model()\n    # initial parameters\n    f = Chain(\n        Dense(10, 200, relu; initW=glorot_uniform, initb=zeros),\n        Dense(200, 50, relu; initW=glorot_uniform, initb=zeros),\n        Dense(50, 1; initW=glorot_uniform, initb=zeros),\n        vec,\n    )\n    σ = Random.randexp()\n\n    return f, σ\nend","category":"page"},{"location":"generated/friedman/#Training","page":"Friedman regression problem","title":"Training","text":"","category":"section"},{"location":"generated/friedman/","page":"Friedman regression problem","title":"Friedman regression problem","text":"We use a maximum likelihood approach and train the parameters theta of the model for 5000 iterations by minimizing the mean squared error on the training data set using ADAM. In each iteration, the variance sigma^2 is set to the maximizer of the likelihood of the training data set.","category":"page"},{"location":"generated/friedman/","page":"Friedman regression problem","title":"Friedman regression problem","text":"We train 10 models for each combination of regression problem and model category, and compute the predicted distributions on the training and test data sets in each iteration step.","category":"page"},{"location":"generated/friedman/","page":"Friedman regression problem","title":"Friedman regression problem","text":"The initial values of the weight matrices of the neural networks are sampled from the uniform Glorot initialization and the offset vectors are initialized with zeros. The model parameters are learnt by iteratively minimizing the negative log-likelihood on the training data set. The parameters of the neural networks are trained by gradient descent with the Adam optimization algorithm (default settings in Flux.jl), and in each iteration step the variance parameter of the predicted Gaussian distributions is set to the optimal value with respect to the negative log-likelihood on the training data set.","category":"page"},{"location":"generated/friedman/","page":"Friedman regression problem","title":"Friedman regression problem","text":"function train(id, (train_xs, train_ys), (test_xs, _))\n    # check if file exists\n    filename = joinpath(\"data\", \"friedman\", \"predictions_id=$(id).arrow\")\n    isfile(filename) && return nothing\n\n    # compute the predictions of the initial neural network\n    f, σ = nn_model()\n    train_μs = f(train_xs)\n    test_μs = f(test_xs)\n\n    # save the initial model and its predictions\n    niters = 5000\n    train_μss = Vector{typeof(train_μs)}(undef, niters + 1)\n    test_μss = Vector{typeof(test_μs)}(undef, niters + 1)\n    σs = Vector{typeof(σ)}(undef, niters + 1)\n    train_μss[1] = train_μs\n    test_μss[1] = test_μs\n    σs[1] = σ\n\n    # train with ADAM\n    params = Flux.Params(Flux.params(f))\n    opt = ADAM()\n    @progress name = \"training (id = $id)\" for i in 2:(niters + 1)\n        # compute gradients\n        gradients = gradient(params) do\n            return Flux.Losses.mse(f(train_xs), train_ys)\n        end\n\n        # update the parameters\n        Flux.Optimise.update!(opt, params, gradients)\n\n        # update the variance\n        yhats = f(train_xs)\n        σ = sqrt(Flux.Losses.mse(yhats, train_ys))\n\n        # save the model and its predictions\n        train_μss[i] = yhats\n        test_μss[i] = f(test_xs)\n        σs[i] = σ\n    end\n\n    # save the predictions\n    mkpath(dirname(filename))\n    Arrow.write(filename, (train_μs=train_μss, test_μs=test_μss, σ=σs))\n\n    return nothing\nend\n\nRandom.seed!(100)\nfor (id, seed) in enumerate(rand(UInt, 10))\n    @info \"training NN model: run $id\"\n    Random.seed!(seed)\n    with_logger(PROGRESSLOGGER) do\n        train(id, train_data, test_data)\n    end\nend","category":"page"},{"location":"generated/friedman/#Evaluations","page":"Friedman regression problem","title":"Evaluations","text":"","category":"section"},{"location":"generated/friedman/","page":"Friedman regression problem","title":"Friedman regression problem","text":"We estimate the average negative log-likelihood (NLL) and the mean squared error (MSE). Additionally, we estimate the average pinball loss","category":"page"},{"location":"generated/friedman/","page":"Friedman regression problem","title":"Friedman regression problem","text":"frac1n_tau sum_i=1^n_tau mathbbE_XY L_tau_ibig(Y mathrmquantile(P_Xtau_i)big)","category":"page"},{"location":"generated/friedman/","page":"Friedman regression problem","title":"Friedman regression problem","text":"for quantile levels tau_i = 005i (n_tau = 19), where L_tau(y tildey) = (1 - tau) (tildey - y)_+ + tau (y - tildey)_+ for observation y and prediction tildey, and mathrmquantile(P_x tau) = inf_y P_x(Y leq y) geq tau for quantile level tau in 0 1.","category":"page"},{"location":"generated/friedman/","page":"Friedman regression problem","title":"Friedman regression problem","text":"function pinball_loss(level::Real, y::Real, quantile::Real)\n    δ = quantile - y\n    return ((δ > 0) - level) * δ\nend\n\nfunction mean_pinball_loss(prediction, y)\n    levels = 0.05:0.05:0.95\n    return mean(pinball_loss(τ, y, quantile(prediction, τ)) for τ in levels)\nend","category":"page"},{"location":"generated/friedman/","page":"Friedman regression problem","title":"Friedman regression problem","text":"Moreover, we estimate the squared kernel calibration error (SKCE) and the p-value of the null hypothesis that the model is calibrated. for calibration on the training and test data set for every model and every iteration.","category":"page"},{"location":"generated/friedman/","page":"Friedman regression problem","title":"Friedman regression problem","text":"function evaluate_models(dataset, id, ys)\n    # output file\n    out = joinpath(\"data\", \"friedman\", \"statistics_id=$(id)_dataset=$(dataset).csv\")\n    isfile(out) && return nothing\n\n    # load data\n    filename = joinpath(\"data\", \"friedman\", \"predictions_id=$(id).arrow\")\n    isfile(filename) || error(\"predictions for run \", id, \" not found\")\n    tbl = Arrow.Table(filename)\n    σs = tbl.σ\n    μss = getproperty(tbl, Symbol(dataset, :_μs))\n    predictionss = map(μss, σs) do μs, σ\n        return map(μs) do μ\n            return Normal(μ, σ)\n        end\n    end\n\n    # define kernel\n    kernel = WassersteinExponentialKernel() ⊗ SqExponentialKernel()\n\n    return evaluate_stats(out, predictionss, ys, kernel)\nend\n\nfunction evaluate_stats(file, predictionss, ys, kernel)\n    mkpath(dirname(file))\n    open(file, \"w\") do f\n        # print headers\n        println(f, \"iteration,statistic,estimate\")\n\n        @progress name = \"iterations\" for (i, predictions) in enumerate(predictionss)\n            # average NLL\n            nll = -mean(map(logpdf, predictions, ys))\n            println(f, i - 1, \",NLL,\", nll)\n\n            # mean squared error\n            mse = Flux.Losses.mse(map(mean, predictions), ys)\n            println(f, i - 1, \",MSE,\", mse)\n\n            # pinball loss\n            pinball = mean(map(mean_pinball_loss, predictions, ys))\n            println(f, i - 1, \",pinball,\", pinball)\n\n            # unbiased estimator of SKCE\n            unbiased_estimator = UnbiasedSKCE(kernel)\n            skce = calibrationerror(unbiased_estimator, predictions, ys)\n            println(f, i - 1, \",SKCE (unbiased),\", skce)\n\n            # biased estimator of SKCE\n            biased_estimator = BiasedSKCE(kernel)\n            skce = calibrationerror(biased_estimator, predictions, ys)\n            println(f, i - 1, \",SKCE (biased),\", skce)\n\n            # p-value\n            test = AsymptoticSKCETest(kernel, predictions, ys)\n            p = pvalue(test; bootstrap_iters=1_000)\n            println(f, i - 1, \",p-value,\", p)\n        end\n    end\n\n    return nothing\nend\n\nRandom.seed!(300)\nfor (id, seed) in enumerate(rand(UInt, 10))\n    # evaluate models on training data set\n    @info \"evaluating training statistics: run $id\"\n    Random.seed!(seed)\n    with_logger(PROGRESSLOGGER) do\n        evaluate_models(\"train\", id, train_data[2])\n    end\n\n    # evaluate models on test data set\n    @info \"evaluating test statistics: run $id\"\n    Random.seed!(seed)\n    with_logger(PROGRESSLOGGER) do\n        evaluate_models(\"test\", id, test_data[2])\n    end\nend","category":"page"},{"location":"generated/friedman/#Ensembles","page":"Friedman regression problem","title":"Ensembles","text":"","category":"section"},{"location":"generated/friedman/","page":"Friedman regression problem","title":"Friedman regression problem","text":"We create an ensemble of models from the trained models by combining models at each training iteration.","category":"page"},{"location":"generated/friedman/","page":"Friedman regression problem","title":"Friedman regression problem","text":"We evaluate the same statistics as for the individual ensemble members above. Since the quantiles of a Gaussian mixture model are not available in a closed-form expression, we use a bisection algorithm.","category":"page"},{"location":"generated/friedman/","page":"Friedman regression problem","title":"Friedman regression problem","text":"# see https://github.com/JuliaStats/Distributions.jl/pull/1195\nfunction Distributions.quantile(dist::UnivariateMixture{Continuous}, level::Real)\n    ps = probs(dist)\n    bracket = extrema(\n        quantile(component(dist, i), level) for (i, pi) in enumerate(ps) if pi > 0\n    )\n    return find_zero(bracket) do x\n        cdf(dist, x) - level\n    end\nend\n\nfunction evaluate_ensembles(dataset, ys)\n    # output file\n    out = joinpath(\"data\", \"friedman\", \"statistics_ensembles_dataset=$(dataset).csv\")\n    isfile(out) && return nothing\n\n    # load data\n    tables = map(1:10) do id\n        file = joinpath(\"data\", \"friedman\", \"predictions_id=$(id).arrow\")\n        isfile(file) || error(\"predictions for run \", id, \" not found\")\n        return Arrow.Table(file)\n    end\n    nmodels = length(first(tables).σ)\n    nsamples = length(ys)\n    μs = Symbol(dataset, :_μs)\n    predictionss = map(1:nmodels) do i\n        map(1:nsamples) do j\n            MixtureModel(\n                map(tables) do table\n                    Normal(getproperty(table, μs)[i][j], table.σ[i])\n                end,\n            )\n        end\n    end\n\n    # kernel\n    kernel = MixtureWassersteinExponentialKernel() ⊗ SqExponentialKernel()\n\n    return evaluate_stats(out, predictionss, ys, kernel)\nend\n\n# evaluate ensembles on training data set\n@info \"evaluating training statistics: ensembles of runs 1:10\"\nRandom.seed!(400)\nwith_logger(PROGRESSLOGGER) do\n    evaluate_ensembles(\"train\", train_data[2])\nend\n\n# evaluate ensembles on test data set\n@info \"evaluating test statistics: ensembles of runs 1:10\"\nRandom.seed!(500)\nwith_logger(PROGRESSLOGGER) do\n    evaluate_ensembles(\"test\", test_data[2])\nend","category":"page"},{"location":"generated/friedman/#Visualization","page":"Friedman regression problem","title":"Visualization","text":"","category":"section"},{"location":"generated/friedman/","page":"Friedman regression problem","title":"Friedman regression problem","text":"We visualize the resulting estimates by overlaying them with the estimates of the individual models.","category":"page"},{"location":"generated/friedman/","page":"Friedman regression problem","title":"Friedman regression problem","text":"function logtickformat(base::Int)\n    function format(values)\n        return map(Base.Fix2(logformat, base), showoff(values))\n    end\n    return format\nend\n\nfunction logformat(digits::String, base::Int)\n    buf = IOBuffer()\n    print(buf, base)\n    for c in digits\n        if '0' ≤ c ≤ '9'\n            print(buf, Showoff.superscript_numerals[c - '0' + 1])\n        elseif c == '-'\n            print(buf, '⁻')\n        elseif c == '.'\n            print(buf, '·')\n        end\n    end\n    return String(take!(buf))\nend\n\nfunction asinhtickformat(factor)\n    function format(values)\n        return \"sinh(\" .* showoff(values ./ factor) .* \")\"\n    end\n    return format\nend\n\nfunction plot_statistic!(ax::Axis, iterations, statistic; transform=identity)\n    # load and filter statistics\n    models_df = mapreduce(vcat, Iterators.product(1:10, (\"train\", \"test\"))) do (id, dataset)\n        file = joinpath(\"data\", \"friedman\", \"statistics_id=$(id)_dataset=$(dataset).csv\")\n        df = @from i in DataFrame(CSV.File(file)) begin\n            @where i.statistic == statistic && i.iteration in iterations\n            @select i\n            @collect DataFrame\n        end\n        df[!, :dataset] .= dataset\n        return df\n    end\n    sort!(models_df, :iteration)\n\n    ensembles_df = mapreduce(vcat, (\"train\", \"test\")) do dataset\n        file = joinpath(\"data\", \"friedman\", \"statistics_ensembles_dataset=$(dataset).csv\")\n        df = @from i in DataFrame(CSV.File(file)) begin\n            @where i.statistic == statistic && i.iteration in iterations\n            @select i\n            @collect DataFrame\n        end\n        df[!, :dataset] .= dataset\n        return df\n    end\n    sort!(ensembles_df, :iteration)\n\n    # plot evaluations\n    for (j, dataset) in enumerate((\"train\", \"test\"))\n        # models\n        if !isempty(models_df)\n            models = @from i in models_df begin\n                @where i.dataset == dataset\n                @orderby i.iteration\n                @group i by i.iteration into g\n                @select {\n                    iteration = key(g),\n                    mean = transform(mean(g.estimate)),\n                    min = transform(minimum(g.estimate)),\n                    max = transform(maximum(g.estimate)),\n                }\n                @collect DataFrame\n            end\n            color = Dark2_8[2 * j - 1]\n            band!(ax, models.iteration, models.min, models.max; color=(color, 0.2))\n            lines!(ax, models.iteration, models.mean; color=color)\n        end\n\n        # ensembles\n        if !isempty(ensembles_df)\n            ensembles = @from i in ensembles_df begin\n                @where i.dataset == dataset\n                @orderby i.iteration\n                @select {i.iteration, estimate = transform(i.estimate)}\n                @collect DataFrame\n            end\n            color = Dark2_8[2 * j]\n            lines!(ax, ensembles.iteration, ensembles.estimate; color=color)\n        end\n    end\n\n    return ax\nend\n\nfunction statsplot(iterations, statistics::Matrix; kwargs...)\n    # create scene\n    nrows, ncols = size(statistics)\n    fig = Figure(; resolution=(500 * ncols, 200 * nrows))\n\n    # for all statistics\n    transforms = Dict(\n        \"MSE\" => log10,\n        \"NLL\" => asinh,\n        \"pinball\" => log10,\n        \"SKCE (unbiased)\" => x -> asinh(1000 * x),\n        \"SKCE (biased)\" => log10,\n        \"p-value\" => identity,\n    )\n    ytickformats = Dict(\n        \"MSE\" => logtickformat(10),\n        \"NLL\" => asinhtickformat(1),\n        \"pinball\" => logtickformat(10),\n        \"SKCE (unbiased)\" => asinhtickformat(1000),\n        \"SKCE (biased)\" => logtickformat(10),\n        \"p-value\" => AbstractPlotting.automatic,\n    )\n    for j in 1:ncols, i in 1:nrows\n        statistic = statistics[i, j]\n\n        # add axis\n        ax = Axis(\n            fig[i, j];\n            title=statistic,\n            ytickformat=ytickformats[statistic],\n            xticklabelsize=12.0f0,\n            yticklabelsize=12.0f0,\n        )\n\n        # plot results\n        plot_statistic!(ax, iterations, statistic; transform=transforms[statistic])\n\n        # hide x ticks in every column apart from bottom row\n        if i < nrows\n            hidexdecorations!(ax)\n        end\n    end\n\n    # link all x axes\n    axes = contents(fig[1:nrows, 1:ncols])\n    linkxaxes!(axes...)\n\n    # add labels\n    Label(fig[end + 1, 1:ncols], \"iteration\")\n    Label(fig[1:nrows, 0], \"estimate\"; rotation=π / 2, tellheight=false)\n\n    # tighten limits\n    for ax in axes\n        tightlimits!(ax, Left(), Right())\n    end\n\n    # create legend on right\n    elems = map(1:2:3) do i\n        model_color = Dark2_8[i]\n        ensemble_color = Dark2_8[i + 1]\n        return [\n            [\n                PolyElement(;\n                    color=(model_color, 0.2),\n                    strokewidth=0,\n                    strokecolor=(model_color, 0.2),\n                ),\n                LineElement(; color=model_color, linestyle=nothing),\n            ],\n            LineElement(; color=ensemble_color, linestyle=nothing),\n        ]\n    end\n    Legend(\n        fig[1:nrows, end + 1],\n        elems,\n        [[\"models\", \"ensemble\"], [\"models\", \"ensemble\"]],\n        [\"training\", \"test\"];\n        titlefont=\"Dejavu Sans Bold\",\n    )\n\n    return fig\nend\n\nstatsplot(0:5000, [\"MSE\" \"SKCE (unbiased)\"; \"NLL\" \"SKCE (biased)\"; \"pinball\" \"p-value\"])\nwsavefig(\"figures/friedman/statsplot.svg\");\nnothing #hide","category":"page"},{"location":"generated/friedman/","page":"Friedman regression problem","title":"Friedman regression problem","text":"(Image: )","category":"page"},{"location":"generated/friedman/","page":"Friedman regression problem","title":"Friedman regression problem","text":"statsplot(10:1500, [\"MSE\" \"NLL\"; \"SKCE (biased)\" \"p-value\"])\nwsavefig(\"figures/friedman/statsplot_zoom.svg\");\nnothing #hide","category":"page"},{"location":"generated/friedman/","page":"Friedman regression problem","title":"Friedman regression problem","text":"(Image: )","category":"page"},{"location":"generated/friedman/","page":"Friedman regression problem","title":"Friedman regression problem","text":"","category":"page"},{"location":"generated/friedman/","page":"Friedman regression problem","title":"Friedman regression problem","text":"This page was generated using Literate.jl.","category":"page"},{"location":"generated/ols/","page":"Ordinary least squares","title":"Ordinary least squares","text":"EditURL = \"https://github.com/devmotion/Calibration_ICLR2021/blob/main/experiments/src/ols/script.jl\"","category":"page"},{"location":"generated/ols/#Ordinary-least-squares","page":"Ordinary least squares","title":"Ordinary least squares","text":"","category":"section"},{"location":"generated/ols/","page":"Ordinary least squares","title":"Ordinary least squares","text":"(Image: )","category":"page"},{"location":"generated/ols/","page":"Ordinary least squares","title":"Ordinary least squares","text":"You are seeing the HTML output generated by Documenter.jl and Literate.jl from the Julia source file. The corresponding notebook can be viewed in nbviewer, and the plain script output can be found here.","category":"page"},{"location":"generated/ols/","page":"Ordinary least squares","title":"Ordinary least squares","text":"note: Note\nIf you want to run the experiments, make sure you have an identical environment. Please use Julia 1.5.3 and activate and instantiate the environment using this Project.toml file and this Manifest.toml file.The Github repository contains more detailed instructions and a nix project environment with a pinned Julia binary for improved reproducibility.","category":"page"},{"location":"generated/ols/","page":"Ordinary least squares","title":"Ordinary least squares","text":"using Pkg: Pkg\nPkg.activate(\"/home/runner/work/Calibration_ICLR2021/Calibration_ICLR2021/site/../experiments/src/ols\")\nPkg.instantiate()","category":"page"},{"location":"generated/ols/#Packages","page":"Ordinary least squares","title":"Packages","text":"","category":"section"},{"location":"generated/ols/","page":"Ordinary least squares","title":"Ordinary least squares","text":"using CairoMakie\nusing CalibrationErrors\nusing CalibrationErrorsDistributions\nusing CalibrationTests\nusing Distributions\nusing StatsBase\n\nusing Random\n\nusing CairoMakie.AbstractPlotting.ColorSchemes: Dark2_8\n\n# set random seed\nRandom.seed!(1234)\n\n# create path before saving\nfunction wsavefig(file, fig=current_figure())\n    mkpath(dirname(file))\n    return save(file, fig)\nend","category":"page"},{"location":"generated/ols/#Regression-problem","page":"Ordinary least squares","title":"Regression problem","text":"","category":"section"},{"location":"generated/ols/","page":"Ordinary least squares","title":"Ordinary least squares","text":"We consider a regression problem with scalar feature X and scalar target Y with input-dependent Gaussian noise that is inspired by a problem by Gustafsson, Danelljan, and Schön. Feature X is distributed uniformly at random in -1 1, and target Y is distributed according to","category":"page"},{"location":"generated/ols/","page":"Ordinary least squares","title":"Ordinary least squares","text":"Y sim sin(pi X) +  1 + X  epsilon","category":"page"},{"location":"generated/ols/","page":"Ordinary least squares","title":"Ordinary least squares","text":"where epsilon sim mathcalN(0 015^2).","category":"page"},{"location":"generated/ols/","page":"Ordinary least squares","title":"Ordinary least squares","text":"We start by generating a data set consisting of 100 i.i.d. pairs of feature X and target Y:","category":"page"},{"location":"generated/ols/","page":"Ordinary least squares","title":"Ordinary least squares","text":"xs = rand(Uniform(-1, 1), 100)\nys = rand.(Normal.(sinpi.(xs), 0.15 .* abs.(1 .+ xs)))","category":"page"},{"location":"generated/ols/#Ordinary-least-squares-regression","page":"Ordinary least squares","title":"Ordinary least squares regression","text":"","category":"section"},{"location":"generated/ols/","page":"Ordinary least squares","title":"Ordinary least squares","text":"We perform ordinary least squares regression for this nonlinear heteroscedastic regression problem, and train a model P with homoscedastic variance. The fitted parameters of the model are","category":"page"},{"location":"generated/ols/","page":"Ordinary least squares","title":"Ordinary least squares","text":"bs = hcat(ones(length(xs)), xs) \\ ys","category":"page"},{"location":"generated/ols/","page":"Ordinary least squares","title":"Ordinary least squares","text":"and the standard deviation of the model is given by","category":"page"},{"location":"generated/ols/","page":"Ordinary least squares","title":"Ordinary least squares","text":"stddev = std(bs[1] .+ bs[2] .* xs .- ys)","category":"page"},{"location":"generated/ols/","page":"Ordinary least squares","title":"Ordinary least squares","text":"The following plot visualizes the training data set and model P, together with the function f(x) = mathbbEY  X = x = sin(pi x).","category":"page"},{"location":"generated/ols/","page":"Ordinary least squares","title":"Ordinary least squares","text":"fig = Figure(; resolution=(960, 450))\n\n# plot the data generating distribution\nax1 = Axis(fig[1, 1]; title=\"ℙ(Y|X)\", xlabel=\"X\", ylabel=\"Y\")\nheatmap!(\n    -1:0.01:1,\n    -2:0.01:2,\n    (x, y) -> pdf(Normal(sinpi(x), 0.15 * abs(1 + x)), y);\n    colorrange=(0, 1),\n)\nscatter!(xs, ys; color=Dark2_8[2])\ntightlimits!(ax1)\n\n# plot the predictions of the model\nax2 = Axis(fig[1, 2]; title=\"P(Y|X)\", xlabel=\"X\", ylabel=\"Y\")\nheatmap!(\n    -1:0.01:1,\n    -2:0.01:2,\n    let offset = bs[1], slope = bs[2], stddev = stddev\n        (x, y) -> pdf(Normal(offset + slope * x, stddev), y)\n    end;\n    colorrange=(0, 1),\n)\nscatter!(xs, ys; color=Dark2_8[2])\ntightlimits!(ax2)\n\n# link axes and hide y labels and ticks of the second plot\nlinkaxes!(ax1, ax2)\nhideydecorations!(ax2; grid=false)\n\n# add a colorbar\nColorbar(fig[1, 3]; label=\"density\", width=30)\n\n# adjust space\ncolgap!(fig.layout, 50)\n\nwsavefig(\"figures/ols/heatmap.svg\");\nnothing #hide","category":"page"},{"location":"generated/ols/","page":"Ordinary least squares","title":"Ordinary least squares","text":"(Image: )","category":"page"},{"location":"generated/ols/#Validation","page":"Ordinary least squares","title":"Validation","text":"","category":"section"},{"location":"generated/ols/","page":"Ordinary least squares","title":"Ordinary least squares","text":"We evaluate calibration of the model with a validation data set of n = 50 i.i.d. pairs of samples (X_1 Y_1) ldots (X_n Y_n) of (X Y).","category":"page"},{"location":"generated/ols/","page":"Ordinary least squares","title":"Ordinary least squares","text":"valxs = rand(Uniform(-1, 1), 50)\nvalys = rand.(Normal.(sinpi.(valxs), 0.15 .* abs.(1 .+ valxs)))","category":"page"},{"location":"generated/ols/","page":"Ordinary least squares","title":"Ordinary least squares","text":"For these validation data points we compute the predicted distributions P(Y  X = X_i).","category":"page"},{"location":"generated/ols/","page":"Ordinary least squares","title":"Ordinary least squares","text":"valps = Normal.(bs[1] .+ bs[2] .* valxs, stddev)","category":"page"},{"location":"generated/ols/#Quantile-calibration","page":"Ordinary least squares","title":"Quantile calibration","text":"","category":"section"},{"location":"generated/ols/","page":"Ordinary least squares","title":"Ordinary least squares","text":"We evaluate the predicted cumulative probability tau_i = P(Y leq Y_i  X = X_i) for each validation data point.","category":"page"},{"location":"generated/ols/","page":"Ordinary least squares","title":"Ordinary least squares","text":"τs = cdf.(valps, valys)","category":"page"},{"location":"generated/ols/","page":"Ordinary least squares","title":"Ordinary least squares","text":"The following plot visualizes the empirical cumulative distribution function of the predicted quantiles.","category":"page"},{"location":"generated/ols/","page":"Ordinary least squares","title":"Ordinary least squares","text":"fig = Figure(; resolution=(600, 450))\n\nax = Axis(\n    fig[1, 1];\n    xlabel=\"quantile level\",\n    ylabel=\"cumulative probability\",\n    xticks=0:0.25:1,\n    yticks=0:0.25:1,\n    autolimitaspect=1,\n    rightspinevisible=false,\n    topspinevisible=false,\n    xgridvisible=false,\n    ygridvisible=false,\n)\n\n# plot the ideal\nlines!([0, 1], [0, 1]; label=\"ideal\", linewidth=2, color=Dark2_8[1])\n\n# plot the empirical cdf\nsort!(τs)\necdf_xs = vcat(0, repeat(τs; inner=2), 1)\necdf_ys = repeat(range(0, 1; length=length(τs) + 1); inner=2)\nlines!(ecdf_xs, ecdf_ys; label=\"data\", linewidth=2, color=Dark2_8[2])\n\n# add legend\nLegend(fig[1, 2], ax; valign=:top, framevisible=false)\n\n# set limits and aspect ratio\ncolsize!(fig.layout, 1, Aspect(1, 1))\ntightlimits!(ax)\n\nwsavefig(\"figures/ols/quantiles.svg\");\nnothing #hide","category":"page"},{"location":"generated/ols/","page":"Ordinary least squares","title":"Ordinary least squares","text":"(Image: )","category":"page"},{"location":"generated/ols/#Calibration-test","page":"Ordinary least squares","title":"Calibration test","text":"","category":"section"},{"location":"generated/ols/","page":"Ordinary least squares","title":"Ordinary least squares","text":"We compute a p-value estimate of the null hypothesis that model P is calibrated using an estimation of the quantile of the asymptotic distribution of n widehatmathrmSKCE_kn with 100000 bootstrap samples on the validation data set. Kernel k is chosen as the tensor product kernel","category":"page"},{"location":"generated/ols/","page":"Ordinary least squares","title":"Ordinary least squares","text":"beginaligned\nkbig((p y) (p y)big) = expbig(- W_2(p p)big) expbig(-(y - y)^22big) \n= expbig(-sqrt(m_p - m_p)^2 + (sigma_p - sigma_p)^2big) expbig( - (y - y)^22big)\nendaligned","category":"page"},{"location":"generated/ols/","page":"Ordinary least squares","title":"Ordinary least squares","text":"where W_2 is the 2-Wasserstein distance and m_p m_p and sigma_p sigma_p denote the mean and the standard deviation of the normal distributions p and p.","category":"page"},{"location":"generated/ols/","page":"Ordinary least squares","title":"Ordinary least squares","text":"# define kernel\nkernel = WassersteinExponentialKernel() ⊗ SqExponentialKernel()\n\n# compute p-value estimate using bootstrapping\npvalue(AsymptoticSKCETest(kernel, valps, valys); bootstrap_iters=100_000)","category":"page"},{"location":"generated/ols/","page":"Ordinary least squares","title":"Ordinary least squares","text":"We obtain p  005 in our experiment, and hence the calibration test rejects H_0 at the significance level alpha = 005.","category":"page"},{"location":"generated/ols/","page":"Ordinary least squares","title":"Ordinary least squares","text":"","category":"page"},{"location":"generated/ols/","page":"Ordinary least squares","title":"Ordinary least squares","text":"This page was generated using Literate.jl.","category":"page"},{"location":"#Home","page":"Home","title":"Home","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This webpage accompanies the paper \"Calibration tests beyond classification\" by David Widmann, Fredrik Lindsten, and Dave Zachariah, which will be presented at ICLR 2021.","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: )","category":"page"},{"location":"","page":"Home","title":"Home","text":"The source code for the paper, the experiments therein, and also this webpage are available on Github.","category":"page"},{"location":"#Abstract","page":"Home","title":"Abstract","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Most supervised machine learning tasks are subject to irreducible prediction errors. Probabilistic predictive models address this limitation by providing probability distributions that represent a belief over plausible targets, rather than point estimates. Such models can be a valuable tool in decision-making under uncertainty, provided that the model output is meaningful and interpretable. Calibrated models guarantee that the probabilistic predictions are neither over- nor under-confident. In the machine learning literature, different measures and statistical tests have been proposed and studied for evaluating the calibration of classification models. For regression problems, however, research has been focused on a weaker condition of calibration based on predicted quantiles for real-valued targets. In this paper, we propose the first framework that unifies calibration evaluation and tests for general probabilistic predictive models. It applies to any such model, including classification and regression models of arbitrary dimension. Furthermore, the framework generalizes existing measures and provides a more intuitive reformulation of a recently proposed framework for calibration in multi-class classification. In particular, we reformulate and generalize the kernel calibration error, its estimators, and hypothesis tests using scalar-valued kernels, and evaluate the calibration of real-valued regression problems.","category":"page"},{"location":"software/#Software","page":"Software","title":"Software","text":"","category":"section"},{"location":"software/","page":"Software","title":"Software","text":"The empirical evaluations in the paper are performed with the Julia programming language.[Julia2017]","category":"page"},{"location":"software/","page":"Software","title":"Software","text":"[Julia2017]: Bezanson, J., Edelman, A., Karpinski, S., & Shah, V. (2017). Julia: A fresh approach to numerical computing. SIAM Review, 59(1), 65–98.","category":"page"},{"location":"software/#Calibration-analysis","page":"Software","title":"Calibration analysis","text":"","category":"section"},{"location":"software/#Julia-packages","page":"Software","title":"Julia packages","text":"","category":"section"},{"location":"software/","page":"Software","title":"Software","text":"CalibrationErrors.jl: This package implements different estimators of the expected calibration error (ECE), the squared kernel calibration (SKCE), and the unnormalized calibration mean embedding (UCME) in the Julia language.\nCalibrationErrorsDistributions.jl: This package extends calibration error estimation for classification models in the package CalibrationErrors.jl to more general probabilistic predictive models that output arbitrary probability distributions, as proposed in our paper.\nCalibrationTests.jl: This package contains statistical hypothesis tests of calibration.","category":"page"},{"location":"software/#Python-interface","page":"Software","title":"Python interface","text":"","category":"section"},{"location":"software/","page":"Software","title":"Software","text":"The Python package pycalibration is a wrapper of the Julia packages CalibrationErrors.jl, CalibrationErrorsDistributions.jl, and CalibrationTests.jl and exposes all their functionality to Python users with PyJulia.","category":"page"},{"location":"software/#R-interface","page":"Software","title":"R interface","text":"","category":"section"},{"location":"software/","page":"Software","title":"Software","text":"Similarly, the R package rcalibration is an interface of CalibrationErrors.jl, CalibrationErrorsDistributions.jl, and CalibrationTests.jl for R. It is based on JuliaCall.","category":"page"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"EditURL = \"https://github.com/devmotion/Calibration_ICLR2021/blob/main/experiments/src/synthetic/script.jl\"","category":"page"},{"location":"generated/synthetic/#Synthetic-models","page":"Synthetic models","title":"Synthetic models","text":"","category":"section"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"(Image: )","category":"page"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"You are seeing the HTML output generated by Documenter.jl and Literate.jl from the Julia source file. The corresponding notebook can be viewed in nbviewer, and the plain script output can be found here.","category":"page"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"note: Note\nIf you want to run the experiments, make sure you have an identical environment. Please use Julia 1.5.3 and activate and instantiate the environment using this Project.toml file and this Manifest.toml file.The Github repository contains more detailed instructions and a nix project environment with a pinned Julia binary for improved reproducibility.","category":"page"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"using Pkg: Pkg\nPkg.activate(\"/home/runner/work/Calibration_ICLR2021/Calibration_ICLR2021/site/../experiments/src/synthetic\")\nPkg.instantiate()","category":"page"},{"location":"generated/synthetic/#Packages","page":"Synthetic models","title":"Packages","text":"","category":"section"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"using CSV\nusing CairoMakie\nusing CalibrationErrors\nusing CalibrationErrorsDistributions\nusing CalibrationTests\nusing DataFrames\nusing Distributions\nusing FillArrays\nusing ProgressLogging\nusing Query\nusing Showoff\nusing StatsBase\n\nusing LinearAlgebra\nusing Printf\nusing Random\n\nusing CairoMakie.AbstractPlotting.ColorSchemes: Dark2_8\n\nusing Logging: with_logger\nusing TerminalLoggers: TerminalLogger\n\n# set random seed\nRandom.seed!(1234)\n\n# create path before saving\nfunction wsavefig(file, fig=current_figure())\n    mkpath(dirname(file))\n    return save(file, fig)\nend\n\n# define progress logging frontend\nconst PROGRESSLOGGER = TerminalLogger()\n\n# define non-intrusive plotting style\nset_theme!(\n    Theme(;\n        Axis=(\n            rightspinevisible=false,\n            topspinevisible=false,\n            xgridvisible=false,\n            ygridvisible=false,\n        ),\n        Legend=(framevisible=false,),\n    ),\n)","category":"page"},{"location":"generated/synthetic/#Synthetic-models-2","page":"Synthetic models","title":"Synthetic models","text":"","category":"section"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"We study two setups with d-dimensional targets Y and normal distributions P_X of the form mathcalN(c mathbf1_d 01^2 mathbfI_d) as predictions, where c sim mathrmU(0 1). Since calibration analysis is only based on the targets and predicted distributions, we neglect features X in these experiments and specify only the distributions of Y and P_X.","category":"page"},{"location":"generated/synthetic/#Calibrated-setup","page":"Synthetic models","title":"Calibrated setup","text":"","category":"section"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"In the first setup we simulate a calibrated model. We achieve this by sampling targets from the predicted distributions, i.e., by defining the conditional distribution of Y given P_X as","category":"page"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"Y  P_X = mathcalN(mu Sigma) sim mathcalN(mu Sigma)","category":"page"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"function calibrated_model(dim::Int, nsamples::Int)\n    # sample predictions\n    predictions = [MvNormal(Fill(rand(), dim), 0.1) for _ in 1:nsamples]\n\n    # sample targets\n    targets = map(rand, predictions)\n\n    return predictions, targets\nend","category":"page"},{"location":"generated/synthetic/#Uncalibrated-setup","page":"Synthetic models","title":"Uncalibrated setup","text":"","category":"section"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"In the second setup we simulate an uncalibrated model of the form","category":"page"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"Y  P_X = mathcalN(mu Sigma) sim mathcalN(01 mu_2 ldots mu_d Sigma)","category":"page"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"function uncalibrated_model(dim::Int, nsamples::Int)\n    # sample predictions\n    predictions = [MvNormal(Fill(rand(), dim), 0.1) for _ in 1:nsamples]\n\n    # sample targets\n    targets = map(rand, predictions)\n    altdist = Normal(0.1, 0.1)\n    for t in targets\n        t[1] = rand(altdist)\n    end\n\n    return predictions, targets\nend","category":"page"},{"location":"generated/synthetic/#Convergence-and-computation-time-of-estimators","page":"Synthetic models","title":"Convergence and computation time of estimators","text":"","category":"section"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"We perform an evaluation of the convergence and computation time of the biased estimator widehatmathrmSKCE_k, the unbiased estimator widehatmathrmSKCE_kB with blocks of size B in 2 sqrtn n. We use the tensor product kernel","category":"page"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"beginaligned\nkbig((p y) (p y)big) = expbig(- W_2(p p)big) expbig(-(y - y)^22big) \n= expbig(-sqrt(m_p - m_p)^2 + (sigma_p - sigma_p)^2big) expbig( - (y - y)^22big)\nendaligned","category":"page"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"where W_2 is the 2-Wasserstein distance and m_p m_p and sigma_p sigma_p denote the mean and the standard deviation of the normal distributions p and p.","category":"page"},{"location":"generated/synthetic/#Ground-truth","page":"Synthetic models","title":"Ground truth","text":"","category":"section"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"For both models, we have to \"evaluate\" the true calibration error. Generally, the error depends on the model (and hence also dimension d) and the kernel. If the model is calibrated, we know that the calibration error is zero. For the uncalibrated model, we estimate the ground truth with the minimum-variance unbiased estimator as the mean of SKCE estimates for 1000 randomly sampled datasets with 1000 data points.","category":"page"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"true_SKCE(::typeof(calibrated_model), kernel; dim::Int) = 0.0\nfunction true_SKCE(model::typeof(uncalibrated_model), kernel; dim::Int)\n    estimator = UnbiasedSKCE(kernel)\n    return mean(calibrationerror(estimator, model(dim, 1_000)...) for _ in 1:1_000)\nend","category":"page"},{"location":"generated/synthetic/#Benchmarking","page":"Synthetic models","title":"Benchmarking","text":"","category":"section"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"The following two functions implement the benchmarking. We sample 500 datasets of 4, 16, 64, 256, and 1024 data points each for the models of dimensions d=1 and d=10. For each of the datasets, we evaluate the different SKCE estimators. We compute the mean absolute error, the variance, and the minimum computation time for the estimates, grouped by the dimension of the model and the number of samples in the dataset.","category":"page"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"function benchmark_estimator(estimator, model; dim::Int, nsamples::Int, groundtruth)\n    # compute the estimator (potentially depending on number of samples)\n    _estimator = estimator(nsamples)\n\n    # cache for calibration error estimates\n    estimates = Vector{Float64}(undef, 500)\n\n    mintime = Inf\n\n    name = @sprintf(\"benchmarking (dim = %2d, nsamples = %4d)\", dim, nsamples)\n    @progress name = name for i in eachindex(estimates)\n        # sample predictions and targets\n        predictions, targets = model(dim, nsamples)\n\n        # define benchmark function\n        benchmark_f =\n            let estimator = _estimator, predictions = predictions, targets = targets\n                () -> @timed calibrationerror(estimator, predictions, targets)\n            end\n\n        # precompile function\n        benchmark_f()\n\n        # compute calibration error and obtain elapsed time\n        val, t = benchmark_f()\n\n        # only keep minimum execution time\n        mintime = min(mintime, t)\n\n        # save error estimate\n        estimates[i] = val\n    end\n\n    # save the mean absolute deviation and the variance of the estimates\n    meanerror = mean(abs(x - groundtruth) for x in estimates)\n    variance = var(estimates)\n\n    return (; dim, nsamples, meanerror, variance, mintime)\nend\n\nfunction benchmark_estimators(model)\n    # output file\n    filename = joinpath(\"data\", \"synthetic\", \"errors_$(model).csv\")\n\n    # check if results exist\n    isfile(filename) && return DataFrame(CSV.File(filename))\n\n    # define kernel\n    kernel = WassersteinExponentialKernel() ⊗ SqExponentialKernel()\n\n    # define estimators\n    estimators = (\n        \"SKCE\" => _ -> BiasedSKCE(kernel),\n        \"SKCE (B = 2)\" => _ -> BlockUnbiasedSKCE(kernel, 2),\n        \"SKCE (B = √n)\" => n -> BlockUnbiasedSKCE(kernel, max(2, Int(floor(sqrt(n))))),\n        \"SKCE (B = n)\" => _ -> UnbiasedSKCE(kernel),\n    )\n\n    # define number of samples\n    nsamples = 2 .^ (2:2:10)\n\n    # ensure that output directory exists and open file for writing\n    mkpath(dirname(filename))\n    open(filename, \"w\") do file\n        # write headers\n        println(file, \"estimator,dim,nsamples,meanerror,variance,mintime\")\n\n        # for dimensions ``d=1`` and ``d=10``\n        for d in (1, 10)\n            # compute/estimate ground truth\n            groundtruth = true_SKCE(model, kernel; dim=d)\n\n            for (i, (name, estimator)) in enumerate(estimators)\n                # benchmark estimator\n                @info \"benchmarking estimator: $(name)\"\n\n                for n in nsamples\n                    stats = benchmark_estimator(\n                        estimator, model; dim=d, nsamples=n, groundtruth=groundtruth\n                    )\n\n                    # save statistics\n                    print(file, name, \",\")\n                    join(file, stats, \",\")\n                    println(file)\n                end\n            end\n        end\n    end\n\n    # load results\n    return DataFrame(CSV.File(filename))\nend","category":"page"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"We benchmark the estimators with the calibrated model.","category":"page"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"Random.seed!(100)\nwith_logger(PROGRESSLOGGER) do\n    benchmark_estimators(calibrated_model)\nend","category":"page"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"We repeat the benchmark with the uncalibrated model.","category":"page"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"Random.seed!(100)\nwith_logger(PROGRESSLOGGER) do\n    benchmark_estimators(uncalibrated_model)\nend","category":"page"},{"location":"generated/synthetic/#Visualization","page":"Synthetic models","title":"Visualization","text":"","category":"section"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"We show a visualization of the results below.","category":"page"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"function logtickformat(base::Int)\n    function format(values)\n        return map(Base.Fix2(logformat, base), showoff(values))\n    end\n    return format\nend\n\nfunction logformat(digits::String, base::Int)\n    buf = IOBuffer()\n    print(buf, base)\n    for c in digits\n        if '0' ≤ c ≤ '9'\n            print(buf, Showoff.superscript_numerals[c - '0' + 1])\n        elseif c == '-'\n            print(buf, '⁻')\n        elseif c == '.'\n            print(buf, '·')\n        end\n    end\n    return String(take!(buf))\nend\n\nfunction plot_benchmark_estimators(model; dim::Int)\n    # load and preprocess data\n    filename = joinpath(\"data\", \"synthetic\", \"errors_$(model).csv\")\n    groups = @from i in DataFrame(CSV.File(filename)) begin\n        @where i.dim == dim\n        @orderby i.nsamples\n        @select {\n            i.estimator,\n            log2_nsamples = log2(i.nsamples),\n            log10_meanerror = log10(i.meanerror),\n            log10_variance = log10(i.variance),\n            log10_mintime = log10(i.mintime),\n        }\n        @collect DataFrame\n    end\n\n    # create figure\n    fig = Figure(; resolution=(960, 800))\n\n    # create axes to plot mean error and variance vs number of samples\n    ax1 = Axis(\n        fig[1, 1];\n        xlabel=\"# samples\",\n        ylabel=\"mean error\",\n        xticks=2:2:10,\n        xtickformat=logtickformat(2),\n        ytickformat=logtickformat(10),\n    )\n    ax2 = Axis(\n        fig[2, 1];\n        xlabel=\"# samples\",\n        ylabel=\"variance\",\n        xticks=2:2:10,\n        xtickformat=logtickformat(2),\n        ytickformat=logtickformat(10),\n    )\n\n    # create axes to plot mean error and variance vs timings\n    ax3 = Axis(\n        fig[1, 2];\n        xlabel=\"time [s]\",\n        ylabel=\"mean error\",\n        xtickformat=logtickformat(10),\n        ytickformat=logtickformat(10),\n    )\n    ax4 = Axis(\n        fig[2, 2];\n        xlabel=\"time [s]\",\n        ylabel=\"variance\",\n        xtickformat=logtickformat(10),\n        ytickformat=logtickformat(10),\n    )\n\n    # plot benchmark results\n    estimators = [\"SKCE\", \"SKCE (B = 2)\", \"SKCE (B = √n)\", \"SKCE (B = n)\"]\n    markers = ['●', '■', '▲', '◆']\n    for (i, (estimator, marker)) in enumerate(zip(estimators, markers))\n        group = filter(:estimator => ==(estimator), groups)\n        color = Dark2_8[i]\n\n        # plot mean error vs samples\n        scatterlines!(\n            ax1,\n            group.log2_nsamples,\n            group.log10_meanerror;\n            color=color,\n            linewidth=2,\n            marker=marker,\n            markercolor=color,\n        )\n\n        # plot variance vs samples\n        scatterlines!(\n            ax2,\n            group.log2_nsamples,\n            group.log10_variance;\n            color=color,\n            linewidth=2,\n            marker=marker,\n            markercolor=color,\n        )\n\n        # plot mean error vs time\n        scatterlines!(\n            ax3,\n            group.log10_mintime,\n            group.log10_meanerror;\n            color=color,\n            linewidth=2,\n            marker=marker,\n            markercolor=color,\n        )\n\n        # plot variance vs time\n        scatterlines!(\n            ax4,\n            group.log10_mintime,\n            group.log10_variance;\n            color=color,\n            linewidth=2,\n            marker=marker,\n            markercolor=color,\n        )\n    end\n\n    # link axes and hide decorations\n    linkxaxes!(ax1, ax2)\n    hidexdecorations!(ax1)\n    linkxaxes!(ax3, ax4)\n    hidexdecorations!(ax3)\n    linkyaxes!(ax1, ax3)\n    hideydecorations!(ax3)\n    linkyaxes!(ax2, ax4)\n    hideydecorations!(ax4)\n\n    # add legend\n    elems = map(1:length(estimators)) do i\n        [\n            LineElement(; color=Dark2_8[i], linestyle=nothing, linewidth=2),\n            MarkerElement(; color=Dark2_8[i], marker=markers[i], strokecolor=:black),\n        ]\n    end\n    Legend(fig[end + 1, :], elems, estimators; orientation=:horizontal, tellheight=true)\n\n    return fig\nend","category":"page"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"We obtain the following plots:","category":"page"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"plot_benchmark_estimators(calibrated_model; dim=1)\nwsavefig(\"figures/synthetic/estimators_calibrated_model_dim=1.svg\");\nnothing #hide","category":"page"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"(Image: )","category":"page"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"plot_benchmark_estimators(calibrated_model; dim=10)\nwsavefig(\"figures/synthetic/estimators_calibrated_model_dim=10.svg\");\nnothing #hide","category":"page"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"(Image: )","category":"page"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"plot_benchmark_estimators(uncalibrated_model; dim=1)\nwsavefig(\"figures/synthetic/estimators_uncalibrated_model_dim=1.svg\");\nnothing #hide","category":"page"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"(Image: )","category":"page"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"plot_benchmark_estimators(uncalibrated_model; dim=10)\nwsavefig(\"figures/synthetic/estimators_uncalibrated_model_dim=10.svg\");\nnothing #hide","category":"page"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"(Image: )","category":"page"},{"location":"generated/synthetic/#Test-errors-and-computation-time-of-calibration-tests","page":"Synthetic models","title":"Test errors and computation time of calibration tests","text":"","category":"section"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"We fix the significance level alpha = 005. Test predictions are sampled from the same distribution as P_X, and test targets are sampled independently from mathcalN(0 01^2 mathbfI_d).","category":"page"},{"location":"generated/synthetic/#Benchmarking-2","page":"Synthetic models","title":"Benchmarking","text":"","category":"section"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"iscalibrated(::typeof(calibrated_model)) = true\niscalibrated(::typeof(uncalibrated_model)) = false\n\nfunction benchmark_test(test, model; dim::Int, nsamples::Int)\n    # number of simulations\n    nrepeat = 500\n\n    # initial values\n    ntesterrors = 0\n    mintime = Inf\n\n    name = @sprintf(\"benchmarking (dim = %2d, nsamples = %4d)\", dim, nsamples)\n    @progress name = name for _ in 1:nrepeat\n        # sample predictions and targets\n        predictions, targets = model(dim, nsamples)\n\n        # define benchmark function\n        benchmark_f = let test = test, predictions = predictions, targets = targets\n            () -> @timed pvalue(test(predictions, targets))\n        end\n\n        # precompile function\n        benchmark_f()\n\n        # compute calibration error and obtain elapsed time\n        val, t = benchmark_f()\n\n        # only keep minimum execution time\n        mintime = min(mintime, t)\n\n        # update number of empirical test errors for\n        # significance level ``\\alpha = 0.05``\n        ntesterrors += iscalibrated(model) ⊻ (val ≥ 0.05)\n    end\n\n    # compute empirical test error rate\n    testerror = ntesterrors / nrepeat\n\n    return (; dim, nsamples, testerror, mintime)\nend\n\nfunction benchmark_tests(model)\n    # output file\n    filename = joinpath(\"data\", \"synthetic\", \"tests_$(model).csv\")\n\n    # check if results exist\n    isfile(filename) && return DataFrame(CSV.File(filename))\n\n    # define kernel\n    kernel = WassersteinExponentialKernel() ⊗ SqExponentialKernel()\n\n    # define number of samples\n    nsamples = 2 .^ (2:2:10)\n\n    # ensure that output directory exists and open file for writing\n    mkpath(dirname(filename))\n    open(filename, \"w\") do file\n        # write headers\n        println(file, \"test,dim,nsamples,testerror,mintime\")\n\n        # for dimensions ``d=1`` and ``d=10``\n        for d in (1, 10)\n            # define tests\n            testpredictions = [MvNormal(rand(d), 0.1) for _ in 1:10]\n            testtargets = [rand(MvNormal(d, 0.1)) for _ in 1:10]\n            tests = (\n                \"SKCE (B = 2)\" =>\n                    (predictions, targets) -> AsymptoticBlockSKCETest(\n                        BlockUnbiasedSKCE(kernel, 2), predictions, targets\n                    ),\n                \"SKCE (B = √n)\" =>\n                    (predictions, targets) -> AsymptoticBlockSKCETest(\n                        BlockUnbiasedSKCE(kernel, Int(floor(sqrt(length(predictions))))),\n                        predictions,\n                        targets,\n                    ),\n                \"SKCE (B = n)\" =>\n                    (predictions, targets) ->\n                        AsymptoticSKCETest(kernel, predictions, targets),\n                \"CME\" =>\n                    (predictions, targets) -> AsymptoticCMETest(\n                        UCME(kernel, testpredictions, testtargets), predictions, targets\n                    ),\n            )\n\n            for (i, (name, test)) in enumerate(tests)\n                # benchmark estimator\n                @info \"benchmarking test: $(name)\"\n\n                for n in nsamples\n                    stats = benchmark_test(test, model; dim=d, nsamples=n)\n\n                    # save statistics\n                    print(file, name, \",\")\n                    join(file, stats, \",\")\n                    println(file)\n                end\n            end\n        end\n    end\n\n    # load results\n    return DataFrame(CSV.File(filename))\nend","category":"page"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"First we benchmark the calibrated model.","category":"page"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"Random.seed!(100)\nwith_logger(PROGRESSLOGGER) do\n    benchmark_tests(calibrated_model)\nend","category":"page"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"We repeat the analysis with the uncalibrated model.","category":"page"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"Random.seed!(100)\nwith_logger(PROGRESSLOGGER) do\n    benchmark_tests(uncalibrated_model)\nend","category":"page"},{"location":"generated/synthetic/#Visualization-2","page":"Synthetic models","title":"Visualization","text":"","category":"section"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"Again we visualize the results of our benchmarks. However, this time we compare the results for the calibrated and the uncalibrated model in the same plot.","category":"page"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"function plot_benchmark_tests(; dim::Int)\n    # load and preprocess data\n    df = mapreduce(vcat, (calibrated_model, uncalibrated_model)) do model\n        filename = joinpath(\"data\", \"synthetic\", \"tests_$(model).csv\")\n        df = DataFrame(CSV.File(filename))\n        df[!, :model] .= string(model)\n        return df\n    end\n    groups = @from i in df begin\n        @where i.dim == dim\n        @orderby i.nsamples\n        @select {\n            i.test,\n            i.model,\n            log2_nsamples = log2(i.nsamples),\n            i.testerror,\n            log10_mintime = log10(i.mintime),\n        }\n        @collect DataFrame\n    end\n\n    # create figure\n    fig = Figure(; resolution=(960, 400))\n\n    # add labels\n    Label(fig[1:2, 1], \"empirical test error\"; rotation=π / 2, tellheight=false)\n    Label(fig[1, 2:3, Top()], \"calibrated model\"; padding=(0, 0, 10, 0))\n    Label(fig[2, 2:3, Top()], \"uncalibrated model\"; padding=(0, 0, 10, 0))\n\n    # create axes to plot test error vs number of samples\n    ax1 = Axis(\n        fig[1, 2];\n        ylabel=\"type I error\",\n        xticks=2:2:10,\n        xtickformat=logtickformat(2),\n        xticklabelsize=12,\n        yticklabelsize=12,\n    )\n    ax2 = Axis(\n        fig[2, 2];\n        xlabel=\"# samples\",\n        ylabel=\"type II error\",\n        xticks=2:2:10,\n        xtickformat=logtickformat(2),\n        xticklabelsize=12,\n        yticklabelsize=12,\n    )\n\n    # create axes to plot test error vs timings\n    ax3 = Axis(\n        fig[1, 3]; xtickformat=logtickformat(10), xticklabelsize=12, yticklabelsize=12\n    )\n    ax4 = Axis(\n        fig[2, 3];\n        xlabel=\"time [s]\",\n        xtickformat=logtickformat(10),\n        xticklabelsize=12,\n        yticklabelsize=12,\n    )\n\n    # plot benchmark results\n    tests = [\"SKCE (B = 2)\", \"SKCE (B = √n)\", \"SKCE (B = n)\", \"CME\"]\n    markers = ['●', '■', '▲', '◆']\n    for (i, (test, marker)) in enumerate(zip(tests, markers))\n        color = Dark2_8[i]\n\n        # for both calibrated and uncalibrated model\n        for (axes, model) in\n            zip(((ax1, ax3), (ax2, ax4)), (calibrated_model, uncalibrated_model))\n            group = filter(x -> x.test == test && x.model == string(model), groups)\n\n            # plot test error vs samples\n            scatterlines!(\n                axes[1],\n                group.log2_nsamples,\n                group.testerror;\n                color=color,\n                linewidth=2,\n                marker=marker,\n                markercolor=color,\n            )\n\n            # plot test error vs timings\n            scatterlines!(\n                axes[2],\n                group.log10_mintime,\n                group.testerror;\n                color=color,\n                linewidth=2,\n                marker=marker,\n                markercolor=color,\n            )\n        end\n    end\n\n    # plot horizontal lines for significance level\n    for axis in (ax1, ax3)\n        hlines!(axis, 0.05; color=:black, linestyle=:dash, linewidth=2)\n    end\n\n    # link axes and hide decorations\n    linkxaxes!(ax1, ax2)\n    hidexdecorations!(ax1)\n    linkxaxes!(ax3, ax4)\n    hidexdecorations!(ax3)\n    linkyaxes!(ax1, ax3)\n    hideydecorations!(ax3)\n    linkyaxes!(ax2, ax4)\n    hideydecorations!(ax4)\n\n    # add legend\n    elems = map(1:length(tests)) do i\n        [\n            LineElement(; color=Dark2_8[i], linestyle=nothing, linewidth=2),\n            MarkerElement(; color=Dark2_8[i], marker=markers[i], strokecolor=:black),\n        ]\n    end\n    push!(elems, [LineElement(; color=:black, linestyle=:dash, linewidth=2)])\n    Legend(\n        fig[1:2, end + 1],\n        elems,\n        vcat(tests, \"significance level\");\n        tellwidth=true,\n        gridshalign=:left,\n    )\n\n    return fig\nend\n\nplot_benchmark_tests(; dim=1)\nwsavefig(\"figures/synthetic/tests_dim=1.svg\");\nnothing #hide","category":"page"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"(Image: )","category":"page"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"plot_benchmark_tests(; dim=10)\nwsavefig(\"figures/synthetic/tests_dim=10.svg\");\nnothing #hide","category":"page"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"(Image: )","category":"page"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"","category":"page"},{"location":"generated/synthetic/","page":"Synthetic models","title":"Synthetic models","text":"This page was generated using Literate.jl.","category":"page"}]
}
